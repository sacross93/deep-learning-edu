"""
ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 8: Transformer NLP

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Multi30k ë²ˆì—­ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 
Transformer ëª¨ë¸ì˜ ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
1. Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ê°œë…
2. Self-Attentionê³¼ Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜
3. ìœ„ì¹˜ ì¸ì½”ë”©(Positional Encoding)ì˜ ì—­í• 
4. ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì™€ ë§ˆìŠ¤í‚¹
5. ê¸°ê³„ ë²ˆì—­ êµ¬í˜„ ë° BLEU ì ìˆ˜ í‰ê°€
6. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”ì™€ í•´ì„

ë°ì´í„°ì…‹ ì„ íƒ ì´ìœ  - Multi30k (ì˜ì–´-ë…ì¼ì–´ ë²ˆì—­):
- 29,000ê°œì˜ ì˜ì–´-ë…ì¼ì–´ ë¬¸ì¥ ìŒ
- ì ë‹¹í•œ í¬ê¸°ë¡œ êµìœ¡ì— ì í•©
- ê¸°ê³„ ë²ˆì—­ì˜ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬
- ë‹¤ì–‘í•œ ì¼ìƒ í‘œí˜„ê³¼ ì–´íœ˜ í¬í•¨
- Transformer íš¨ê³¼ë¥¼ ëª…í™•íˆ í™•ì¸ ê°€ëŠ¥
- ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”ì— ì í•©

ì™œ Transformerë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?
1. ë³‘ë ¬ ì²˜ë¦¬: RNNê³¼ ë‹¬ë¦¬ ìˆœì°¨ ì²˜ë¦¬ ë¶ˆí•„ìš”
2. ì¥ê±°ë¦¬ ì˜ì¡´ì„±: ì–´í…ì…˜ìœ¼ë¡œ ì§ì ‘ ì—°ê²°
3. í™•ì¥ì„±: ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ë°ì´í„°ì— ì í•©
4. ë²”ìš©ì„±: NLP ì „ ë¶„ì•¼ì—ì„œ SOTA ë‹¬ì„±
5. í•´ì„ ê°€ëŠ¥ì„±: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ë™ì‘ ì´í•´

RNN/LSTM vs Transformer:
- RNN: ìˆœì°¨ ì²˜ë¦¬, ì¥ê±°ë¦¬ ì˜ì¡´ì„± ì œí•œ, ëŠë¦° í›ˆë ¨
- Transformer: ë³‘ë ¬ ì²˜ë¦¬, ì§ì ‘ì  ì˜ì¡´ì„±, ë¹ ë¥¸ í›ˆë ¨
- ì„±ëŠ¥: Transformerê°€ ëŒ€ë¶€ë¶„ ì‘ì—…ì—ì„œ ìš°ìˆ˜
- ë©”ëª¨ë¦¬: Transformerê°€ ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence
import math
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time
import copy
import re
import warnings
from collections import Counter
import pickle
import os
warnings.filterwarnings('ignore')

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.visualization import plot_training_curves
from utils.model_utils import count_parameters, save_checkpoint

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 8: Transformer NLP")
print("=" * 60)

# ============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# =====================================================
import pickle
import os
from collections import Counter
import requests
import zipfile
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu
import warnings
warnings.filterwarnings('ignore')

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.visualization import plot_training_curves
from utils.model_utils import count_parameters, save_checkpoint

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 8: Transformer NLP")
print("=" * 60)# ==
==========================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}")

# Transformerë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
BATCH_SIZE = 32        # TransformerëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©
LEARNING_RATE = 0.0001 # TransformerëŠ” ë‚®ì€ í•™ìŠµë¥  ì„ í˜¸
EPOCHS = 50            # ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„
RANDOM_SEED = 42
MAX_SEQ_LENGTH = 100   # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
VOCAB_SIZE = 10000     # ì–´íœ˜ ì‚¬ì „ í¬ê¸°
D_MODEL = 512          # ëª¨ë¸ ì°¨ì› (ì„ë² ë”© ì°¨ì›)
N_HEADS = 8            # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜
N_LAYERS = 6           # ì¸ì½”ë”/ë””ì½”ë” ë ˆì´ì–´ ìˆ˜
D_FF = 2048            # í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì°¨ì›
DROPOUT = 0.1          # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   ì—í¬í¬: {EPOCHS}")
print(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQ_LENGTH}")
print(f"   ëª¨ë¸ ì°¨ì›: {D_MODEL}")
print(f"   ì–´í…ì…˜ í—¤ë“œ: {N_HEADS}")
print(f"   ë ˆì´ì–´ ìˆ˜: {N_LAYERS}")

# íŠ¹ìˆ˜ í† í° ì •ì˜
PAD_TOKEN = '<PAD>'
SOS_TOKEN = '<SOS>'  # Start of Sentence
EOS_TOKEN = '<EOS>'  # End of Sentence
UNK_TOKEN = '<UNK>'  # Unknown

SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]
PAD_IDX = 0
SOS_IDX = 1
EOS_IDX = 2
UNK_IDX = 3

print(f"\nğŸ·ï¸  íŠ¹ìˆ˜ í† í°:")
print(f"   PAD: {PAD_IDX}, SOS: {SOS_IDX}, EOS: {EOS_IDX}, UNK: {UNK_IDX}")

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
# ============================================================================

print(f"\nğŸ“ ë²ˆì—­ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ")

class TranslationDataProcessor:
    """
    ê¸°ê³„ ë²ˆì—­ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. í…ìŠ¤íŠ¸ ì •ì œ ë° í† í°í™”
    2. ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• (ì†ŒìŠ¤/íƒ€ê²Ÿ ì–¸ì–´ë³„)
    3. ì‹œí€€ìŠ¤ ë³€í™˜ ë° íŒ¨ë”©
    4. ë°°ì¹˜ ìƒì„±
    """
    
    def __init__(self, max_vocab_size=10000, max_seq_length=100):
        self.max_vocab_size = max_vocab_size
        self.max_seq_length = max_seq_length
        
        # ì–´íœ˜ ì‚¬ì „ (ì†ŒìŠ¤/íƒ€ê²Ÿ ì–¸ì–´ë³„)
        self.src_word2idx = {}
        self.src_idx2word = {}
        self.tgt_word2idx = {}
        self.tgt_idx2word = {}
        
        # ë‹¨ì–´ ë¹ˆë„
        self.src_word_counts = Counter()
        self.tgt_word_counts = Counter()
    
    def clean_text(self, text):
        """
        í…ìŠ¤íŠ¸ ì •ì œ
        
        ë²ˆì—­ ë°ì´í„° ì „ì²˜ë¦¬:
        1. ì†Œë¬¸ì ë³€í™˜
        2. êµ¬ë‘ì  ë¶„ë¦¬
        3. ì—°ì† ê³µë°± ì œê±°
        4. íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
        """
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower().strip()
        
        # êµ¬ë‘ì  ì•ë’¤ì— ê³µë°± ì¶”ê°€
        text = re.sub(r'([.!?,:;])', r' \1 ', text)
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def tokenize(self, text):
        """ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ í† í°í™”"""
        return self.clean_text(text).split()
    
    def build_vocab(self, src_sentences, tgt_sentences):
        """
        ì†ŒìŠ¤/íƒ€ê²Ÿ ì–¸ì–´ì˜ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
        
        Args:
            src_sentences: ì†ŒìŠ¤ ì–¸ì–´ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            tgt_sentences: íƒ€ê²Ÿ ì–¸ì–´ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸ“š ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ì¤‘...")
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        for sentence in tqdm(src_sentences, desc="ì†ŒìŠ¤ ì–¸ì–´ ë¶„ì„"):
            tokens = self.tokenize(sentence)
            self.src_word_counts.update(tokens)
        
        for sentence in tqdm(tgt_sentences, desc="íƒ€ê²Ÿ ì–¸ì–´ ë¶„ì„"):
            tokens = self.tokenize(sentence)
            self.tgt_word_counts.update(tokens)
        
        # ì†ŒìŠ¤ ì–¸ì–´ ì–´íœ˜ ì‚¬ì „
        self._build_single_vocab(
            self.src_word_counts, 
            self.src_word2idx, 
            self.src_idx2word,
            "ì†ŒìŠ¤ (ì˜ì–´)"
        )
        
        # íƒ€ê²Ÿ ì–¸ì–´ ì–´íœ˜ ì‚¬ì „
        self._build_single_vocab(
            self.tgt_word_counts,
            self.tgt_word2idx,
            self.tgt_idx2word, 
            "íƒ€ê²Ÿ (ë…ì¼ì–´)"
        )
    
    def _build_single_vocab(self, word_counts, word2idx, idx2word, lang_name):
        """ë‹¨ì¼ ì–¸ì–´ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•"""
        
        # íŠ¹ìˆ˜ í† í° ë¨¼ì € ì¶”ê°€
        vocab_words = SPECIAL_TOKENS.copy()
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë‹¨ì–´ë“¤ ì„ íƒ
        most_common = word_counts.most_common(self.max_vocab_size - len(SPECIAL_TOKENS))
        vocab_words.extend([word for word, count in most_common])
        
        # ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
        for idx, word in enumerate(vocab_words):
            word2idx[word] = idx
            idx2word[idx] = word
        
        print(f"âœ… {lang_name} ì–´íœ˜ ì‚¬ì „:")
        print(f"   ì´ ë‹¨ì–´ ìˆ˜: {len(word_counts):,}ê°œ")
        print(f"   ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(word2idx):,}ê°œ")
        print(f"   ìƒìœ„ ë‹¨ì–´: {[word for word, _ in most_common[:10]]}")
    
    def sentence_to_indices(self, sentence, word2idx, add_eos=False):
        """
        ë¬¸ì¥ì„ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
        
        Args:
            sentence: ì…ë ¥ ë¬¸ì¥
            word2idx: ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘
            add_eos: EOS í† í° ì¶”ê°€ ì—¬ë¶€
        
        Returns:
            list: ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤
        """
        tokens = self.tokenize(sentence)
        
        # ê¸¸ì´ ì œí•œ
        if len(tokens) > self.max_seq_length - (2 if add_eos else 1):
            tokens = tokens[:self.max_seq_length - (2 if add_eos else 1)]
        
        # ì¸ë±ìŠ¤ ë³€í™˜
        indices = []
        for token in tokens:
            if token in word2idx:
                indices.append(word2idx[token])
            else:
                indices.append(word2idx[UNK_TOKEN])
        
        # EOS í† í° ì¶”ê°€ (íƒ€ê²Ÿ ì‹œí€€ìŠ¤ìš©)
        if add_eos:
            indices.append(EOS_IDX)
        
        return indices

# ============================================================================
# 3. Multi30k ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ============================================================================

class Multi30kDataset(Dataset):
    """
    Multi30k ë²ˆì—­ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    
    ì‹¤ì œ Multi30k ë°ì´í„° ëŒ€ì‹  êµìœ¡ìš© ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê³µì‹ Multi30k ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    
    def __init__(self, processor, use_sample_data=True):
        self.processor = processor
        
        if use_sample_data:
            self.create_sample_data()
        else:
            self.load_multi30k_data()
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        self.prepare_data()
    
    def create_sample_data(self):
        """
        êµìœ¡ìš© ìƒ˜í”Œ ë²ˆì—­ ë°ì´í„° ìƒì„±
        
        ì˜ì–´-ë…ì¼ì–´ ë²ˆì—­ ìŒì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
        ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì§„ì§œ Multi30k ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        print("ğŸŒ êµìœ¡ìš© ë²ˆì—­ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ ì–´íœ˜
        english_words = [
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'man', 'woman', 'child', 'people', 'person', 'boy', 'girl', 'dog', 'cat', 'house',
            'car', 'book', 'table', 'chair', 'water', 'food', 'good', 'bad', 'big', 'small',
            'red', 'blue', 'green', 'black', 'white', 'walk', 'run', 'eat', 'drink', 'see',
            'go', 'come', 'have', 'be', 'do', 'make', 'get', 'take', 'give', 'say', 'know'
        ]
        
        german_words = [
            'der', 'die', 'das', 'ein', 'eine', 'und', 'oder', 'aber', 'in', 'auf', 'zu', 'fÃ¼r', 'von', 'mit',
            'mann', 'frau', 'kind', 'leute', 'person', 'junge', 'mÃ¤dchen', 'hund', 'katze', 'haus',
            'auto', 'buch', 'tisch', 'stuhl', 'wasser', 'essen', 'gut', 'schlecht', 'groÃŸ', 'klein',
            'rot', 'blau', 'grÃ¼n', 'schwarz', 'weiÃŸ', 'gehen', 'laufen', 'essen', 'trinken', 'sehen',
            'gehen', 'kommen', 'haben', 'sein', 'machen', 'bekommen', 'nehmen', 'geben', 'sagen', 'wissen'
        ]
        
        # ìƒ˜í”Œ ë¬¸ì¥ íŒ¨í„´
        patterns = [
            ("the {noun} is {adj}", "der {noun} ist {adj}"),
            ("a {adj} {noun}", "ein {adj} {noun}"),
            ("I {verb} the {noun}", "ich {verb} den {noun}"),
            ("the {noun} {verb}", "der {noun} {verb}"),
            ("{adj} {noun} and {adj} {noun}", "{adj} {noun} und {adj} {noun}")
        ]
        
        self.src_sentences = []
        self.tgt_sentences = []
        
        # ìƒ˜í”Œ ë¬¸ì¥ ìƒì„±
        np.random.seed(RANDOM_SEED)
        
        for _ in range(2000):  # 2000ê°œ ìƒ˜í”Œ ìƒì„±
            # íŒ¨í„´ ì„ íƒ
            en_pattern, de_pattern = np.random.choice(patterns)
            
            # ë‹¨ì–´ ì„ íƒ
            replacements = {}
            if '{noun}' in en_pattern:
                noun_idx = np.random.randint(13, 24)  # ëª…ì‚¬ ë²”ìœ„
                replacements['noun'] = english_words[noun_idx]
            if '{adj}' in en_pattern:
                adj_idx = np.random.randint(29, 39)   # í˜•ìš©ì‚¬ ë²”ìœ„
                replacements['adj'] = english_words[adj_idx]
            if '{verb}' in en_pattern:
                verb_idx = np.random.randint(39, 50)  # ë™ì‚¬ ë²”ìœ„
                replacements['verb'] = english_words[verb_idx]
            
            # ì˜ì–´ ë¬¸ì¥ ìƒì„±
            en_sentence = en_pattern
            for key, value in replacements.items():
                en_sentence = en_sentence.replace(f'{{{key}}}', value)
            
            # ë…ì¼ì–´ ë¬¸ì¥ ìƒì„± (ë‹¨ìˆœ ë§¤í•‘)
            de_sentence = de_pattern
            for key, value in replacements.items():
                # ì˜ì–´-ë…ì¼ì–´ ë‹¨ì–´ ë§¤í•‘ (ë‹¨ìˆœí™”)
                if key == 'noun':
                    de_word = german_words[english_words.index(value)]
                elif key == 'adj':
                    de_word = german_words[english_words.index(value)]
                elif key == 'verb':
                    de_word = german_words[english_words.index(value)]
                else:
                    de_word = value
                
                de_sentence = de_sentence.replace(f'{{{key}}}', de_word)
            
            self.src_sentences.append(en_sentence)
            self.tgt_sentences.append(de_sentence)
        
        print(f"âœ… {len(self.src_sentences)}ê°œ ë²ˆì—­ ìŒ ìƒì„± ì™„ë£Œ")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print(f"\nğŸ“ ìƒ˜í”Œ ë²ˆì—­ ìŒ:")
        for i in range(5):
            print(f"   EN: {self.src_sentences[i]}")
            print(f"   DE: {self.tgt_sentences[i]}")
            print()
    
    def load_multi30k_data(self):
        """ì‹¤ì œ Multi30k ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì‚¬ìš© ì‹œ)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Multi30k ë°ì´í„° ë¡œë“œ
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
        self.create_sample_data()
    
    def prepare_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¸ë±ìŠ¤ ë³€í™˜"""
        print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        # ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
        self.processor.build_vocab(self.src_sentences, self.tgt_sentences)
        
        # ë¬¸ì¥ì„ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
        self.src_sequences = []
        self.tgt_sequences = []
        
        for src_sent, tgt_sent in tqdm(zip(self.src_sentences, self.tgt_sentences), 
                                      desc="ì‹œí€€ìŠ¤ ë³€í™˜", total=len(self.src_sentences)):
            
            # ì†ŒìŠ¤ ì‹œí€€ìŠ¤ (EOS ì—†ìŒ)
            src_seq = self.processor.sentence_to_indices(
                src_sent, self.processor.src_word2idx, add_eos=False
            )
            
            # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (EOS í¬í•¨)
            tgt_seq = self.processor.sentence_to_indices(
                tgt_sent, self.processor.tgt_word2idx, add_eos=True
            )
            
            self.src_sequences.append(src_seq)
            self.tgt_sequences.append(tgt_seq)
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.src_sequences)}ê°œ ì‹œí€€ìŠ¤")
    
    def __len__(self):
        return len(self.src_sequences)
    
    def __getitem__(self, idx):
        return {
            'src': torch.tensor(self.src_sequences[idx], dtype=torch.long),
            'tgt': torch.tensor(self.tgt_sequences[idx], dtype=torch.long)
        }

def collate_fn(batch):
    """
    ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        batch: ë°°ì¹˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        dict: íŒ¨ë”©ëœ ë°°ì¹˜ ë°ì´í„°
    """
    src_sequences = [item['src'] for item in batch]
    tgt_sequences = [item['tgt'] for item in batch]
    
    # íŒ¨ë”©
    src_padded = pad_sequence(src_sequences, batch_first=True, padding_value=PAD_IDX)
    tgt_padded = pad_sequence(tgt_sequences, batch_first=True, padding_value=PAD_IDX)
    
    return {
        'src': src_padded,
        'tgt': tgt_padded,
        'src_lengths': torch.tensor([len(seq) for seq in src_sequences]),
        'tgt_lengths': torch.tensor([len(seq) for seq in tgt_sequences])
    }

# ë°ì´í„° ì¤€ë¹„
processor = TranslationDataProcessor(
    max_vocab_size=VOCAB_SIZE,
    max_seq_length=MAX_SEQ_LENGTH
)

dataset = Multi30kDataset(processor, use_sample_data=True)

# ë°ì´í„° ë¶„í• 
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(RANDOM_SEED)
)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE, shuffle=True, 
    collate_fn=collate_fn, num_workers=2
)

val_loader = DataLoader(
    val_dataset, batch_size=BATCH_SIZE, shuffle=False,
    collate_fn=collate_fn, num_workers=2
)

test_loader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=False,
    collate_fn=collate_fn, num_workers=2
)

print(f"\nâœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
print(f"   í›ˆë ¨: {len(train_dataset):,}ê°œ")
print(f"   ê²€ì¦: {len(val_dataset):,}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸: {len(test_dataset):,}ê°œ")

# ì‹¤ì œ ì–´íœ˜ ì‚¬ì „ í¬ê¸° ì—…ë°ì´íŠ¸
SRC_VOCAB_SIZE = len(processor.src_word2idx)
TGT_VOCAB_SIZE = len(processor.tgt_word2idx)

print(f"   ì†ŒìŠ¤ ì–´íœ˜: {SRC_VOCAB_SIZE:,}ê°œ")
print(f"   íƒ€ê²Ÿ ì–´íœ˜: {TGT_VOCAB_SIZE:,}ê°œ")#
 ============================================================================
# 4. Transformer ëª¨ë¸ êµ¬í˜„
# ============================================================================

print(f"\nğŸ¤– Transformer ëª¨ë¸ êµ¬í˜„")

class PositionalEncoding(nn.Module):
    """
    ìœ„ì¹˜ ì¸ì½”ë”© (Positional Encoding)
    
    TransformerëŠ” ìˆœí™˜ êµ¬ì¡°ê°€ ì—†ì–´ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.
    ìœ„ì¹˜ ì¸ì½”ë”©ìœ¼ë¡œ ê° í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    ìˆ˜ì‹: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
          PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    
    ì™œ sin/cos í•¨ìˆ˜ì¸ê°€?
    1. ì£¼ê¸°ì„±: ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ì²˜ë¦¬
    2. ìƒëŒ€ì  ìœ„ì¹˜: ìœ„ì¹˜ ê°„ ê´€ê³„ í•™ìŠµ ê°€ëŠ¥
    3. ì™¸ì‚½: í›ˆë ¨ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ë„ ì²˜ë¦¬ ê°€ëŠ¥
    """
    
    def __init__(self, d_model, max_length=5000):
        super(PositionalEncoding, self).__init__()
        
        # ìœ„ì¹˜ ì¸ì½”ë”© í…Œì´ë¸” ìƒì„±
        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length).unsqueeze(1).float()
        
        # ì£¼íŒŒìˆ˜ ê³„ì‚°
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        # sin/cos ì ìš©
        pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜ ì¸ë±ìŠ¤
        pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜ ì¸ë±ìŠ¤
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        pe = pe.unsqueeze(0)
        
        # í•™ìŠµë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]

class MultiHeadAttention(nn.Module):
    """
    ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (Multi-Head Attention)
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    1. ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ í—¤ë“œë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ ìˆ˜ì§‘
    2. ê° í—¤ë“œëŠ” ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ ë¶€ê³µê°„ì— ì§‘ì¤‘
    3. í—¤ë“œë“¤ì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ í’ë¶€í•œ í‘œí˜„ ìƒì„±
    
    Self-Attention ìˆ˜ì‹:
    Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
    
    ì™œ âˆšd_kë¡œ ë‚˜ëˆ„ëŠ”ê°€?
    - ë‚´ì ê°’ì´ ì»¤ì§€ë©´ softmaxê°€ í¬í™”ë˜ì–´ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤
    - ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ ë³´ì¥
    """
    
    def __init__(self, d_model, n_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # Query, Key, Value ë³€í™˜ í–‰ë ¬
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        
        # ì¶œë ¥ ë³€í™˜ í–‰ë ¬
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜
        
        Args:
            Q: Query (batch_size, n_heads, seq_len, d_k)
            K: Key (batch_size, n_heads, seq_len, d_k)
            V: Value (batch_size, n_heads, seq_len, d_k)
            mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬
        
        Returns:
            output: ì–´í…ì…˜ ì¶œë ¥
            attention_weights: ì–´í…ì…˜ ê°€ì¤‘ì¹˜
        """
        
        # ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # ë§ˆìŠ¤í‚¹ ì ìš© (íŒ¨ë”©ì´ë‚˜ ë¯¸ë˜ í† í° ì°¨ë‹¨)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Valueì™€ ê°€ì¤‘í•©
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len = query.size(0), query.size(1)
        
        # 1. Query, Key, Value ë³€í™˜
        Q = self.w_q(query)  # (batch_size, seq_len, d_model)
        K = self.w_k(key)
        V = self.w_v(value)
        
        # 2. ë©€í‹°í—¤ë“œë¡œ ë¶„í• 
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # 3. ì–´í…ì…˜ ê³„ì‚°
        attention_output, attention_weights = self.scaled_dot_product_attention(
            Q, K, V, mask
        )
        
        # 4. í—¤ë“œ ê²°í•©
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        # 5. ì¶œë ¥ ë³€í™˜
        output = self.w_o(attention_output)
        
        return output, attention_weights

class FeedForward(nn.Module):
    """
    í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
    
    êµ¬ì¡°: Linear â†’ ReLU â†’ Dropout â†’ Linear
    
    ì—­í• :
    1. ë¹„ì„ í˜• ë³€í™˜ìœ¼ë¡œ í‘œí˜„ë ¥ ì¦ê°€
    2. ê° ìœ„ì¹˜ë³„ë¡œ ë…ë¦½ì  ì²˜ë¦¬
    3. ì–´í…ì…˜ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ì œ
    """
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(FeedForward, self).__init__()
        
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        x = F.relu(self.linear1(x))
        x = self.dropout(x)
        x = self.linear2(x)
        return x

class TransformerEncoderLayer(nn.Module):
    """
    Transformer ì¸ì½”ë” ë ˆì´ì–´
    
    êµ¬ì¡°:
    1. Multi-Head Self-Attention
    2. Add & Norm (ì”ì°¨ ì—°ê²° + ë ˆì´ì–´ ì •ê·œí™”)
    3. Feed-Forward Network
    4. Add & Norm
    
    ì”ì°¨ ì—°ê²°ì˜ ì¤‘ìš”ì„±:
    - ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„ 
    - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í›ˆë ¨ ê°€ëŠ¥
    - ì •ë³´ ì†ì‹¤ ë°©ì§€
    """
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()
        
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # 1. Self-Attention + ì”ì°¨ ì—°ê²°
        attn_output, attn_weights = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 2. Feed-Forward + ì”ì°¨ ì—°ê²°
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x, attn_weights

class TransformerDecoderLayer(nn.Module):
    """
    Transformer ë””ì½”ë” ë ˆì´ì–´
    
    êµ¬ì¡°:
    1. Masked Multi-Head Self-Attention (ë¯¸ë˜ í† í° ì°¨ë‹¨)
    2. Add & Norm
    3. Multi-Head Cross-Attention (ì¸ì½”ë” ì¶œë ¥ê³¼ ì–´í…ì…˜)
    4. Add & Norm
    5. Feed-Forward Network
    6. Add & Norm
    
    ë§ˆìŠ¤í‚¹ì˜ ì¤‘ìš”ì„±:
    - í›ˆë ¨ ì‹œ ë¯¸ë˜ ì •ë³´ ëˆ„ìˆ˜ ë°©ì§€
    - ì¶”ë¡  ì‹œì™€ ë™ì¼í•œ ì¡°ê±´ ìœ ì§€
    - ìê¸°íšŒê·€ì  ìƒì„± ë³´ì¥
    """
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super(TransformerDecoderLayer, self).__init__()
        
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        # 1. Masked Self-Attention
        self_attn_output, self_attn_weights = self.self_attention(
            x, x, x, tgt_mask
        )
        x = self.norm1(x + self.dropout(self_attn_output))
        
        # 2. Cross-Attention (ì¸ì½”ë” ì¶œë ¥ê³¼)
        cross_attn_output, cross_attn_weights = self.cross_attention(
            x, encoder_output, encoder_output, src_mask
        )
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # 3. Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x, self_attn_weights, cross_attn_weights

class Transformer(nn.Module):
    """
    ì™„ì „í•œ Transformer ëª¨ë¸ (ê¸°ê³„ ë²ˆì—­ìš©)
    
    êµ¬ì¡°:
    - ì¸ì½”ë”: ì†ŒìŠ¤ ì–¸ì–´ ì´í•´
    - ë””ì½”ë”: íƒ€ê²Ÿ ì–¸ì–´ ìƒì„±
    - ì„ë² ë”©: í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜
    - ìœ„ì¹˜ ì¸ì½”ë”©: ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
    """
    
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_heads=8, 
                 n_layers=6, d_ff=2048, max_length=5000, dropout=0.1):
        super(Transformer, self).__init__()
        
        self.d_model = d_model
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoding = PositionalEncoding(d_model, max_length)
        
        # ì¸ì½”ë” ë ˆì´ì–´ë“¤
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # ë””ì½”ë” ë ˆì´ì–´ë“¤
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # ì¶œë ¥ íˆ¬ì˜
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.init_weights()
    
    def init_weights(self):
        """Xavier ì´ˆê¸°í™”"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def create_padding_mask(self, seq, pad_idx=PAD_IDX):
        """íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±"""
        return (seq != pad_idx).unsqueeze(1).unsqueeze(2)
    
    def create_look_ahead_mask(self, size):
        """ë¯¸ë˜ í† í° ì°¨ë‹¨ ë§ˆìŠ¤í¬ ìƒì„±"""
        mask = torch.triu(torch.ones(size, size), diagonal=1)
        return mask == 0
    
    def encode(self, src, src_mask=None):
        """ì¸ì½”ë” ìˆœì „íŒŒ"""
        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        x = self.src_embedding(src) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # ì¸ì½”ë” ë ˆì´ì–´ë“¤ í†µê³¼
        attention_weights = []
        for layer in self.encoder_layers:
            x, attn_weights = layer(x, src_mask)
            attention_weights.append(attn_weights)
        
        return x, attention_weights
    
    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):
        """ë””ì½”ë” ìˆœì „íŒŒ"""
        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # ë””ì½”ë” ë ˆì´ì–´ë“¤ í†µê³¼
        self_attention_weights = []
        cross_attention_weights = []
        
        for layer in self.decoder_layers:
            x, self_attn, cross_attn = layer(x, encoder_output, src_mask, tgt_mask)
            self_attention_weights.append(self_attn)
            cross_attention_weights.append(cross_attn)
        
        return x, self_attention_weights, cross_attention_weights
    
    def forward(self, src, tgt):
        """ì „ì²´ ìˆœì „íŒŒ"""
        batch_size, src_len = src.size()
        _, tgt_len = tgt.size()
        
        # ë§ˆìŠ¤í¬ ìƒì„±
        src_mask = self.create_padding_mask(src).to(src.device)
        
        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)
        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt_len).to(tgt.device)
        tgt_mask = tgt_padding_mask & tgt_look_ahead_mask
        
        # ì¸ì½”ë”©
        encoder_output, encoder_attention = self.encode(src, src_mask)
        
        # ë””ì½”ë”©
        decoder_output, decoder_self_attention, decoder_cross_attention = self.decode(
            tgt, encoder_output, src_mask, tgt_mask
        )
        
        # ì¶œë ¥ íˆ¬ì˜
        output = self.output_projection(decoder_output)
        
        return output, {
            'encoder_attention': encoder_attention,
            'decoder_self_attention': decoder_self_attention,
            'decoder_cross_attention': decoder_cross_attention
        }

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = Transformer(
    src_vocab_size=SRC_VOCAB_SIZE,
    tgt_vocab_size=TGT_VOCAB_SIZE,
    d_model=D_MODEL,
    n_heads=N_HEADS,
    n_layers=N_LAYERS,
    d_ff=D_FF,
    dropout=DROPOUT
).to(device)

print(f"âœ… Transformer ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ëª¨ë¸ ë³µì¡ë„ ë¶„ì„
params_info = count_parameters(model, detailed=False)
print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
print(f"   ì´ íŒŒë¼ë¯¸í„°: {params_info['total_params']:,}ê°œ")
print(f"   í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {params_info['trainable_params']:,}ê°œ")

# ëª¨ë¸ êµ¬ì¡° ìš”ì•½
print(f"\nğŸ“‹ Transformer êµ¬ì¡°:")
print(f"   ì†ŒìŠ¤ ì–´íœ˜: {SRC_VOCAB_SIZE:,}ê°œ")
print(f"   íƒ€ê²Ÿ ì–´íœ˜: {TGT_VOCAB_SIZE:,}ê°œ")
print(f"   ëª¨ë¸ ì°¨ì›: {D_MODEL}")
print(f"   ì–´í…ì…˜ í—¤ë“œ: {N_HEADS}ê°œ")
print(f"   ì¸ì½”ë”/ë””ì½”ë” ë ˆì´ì–´: {N_LAYERS}ê°œ")
print(f"   í”¼ë“œí¬ì›Œë“œ ì°¨ì›: {D_FF}")

# ============================================================================
# 5. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
# ============================================================================

print(f"\nâš™ï¸  ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")

# ë¼ë²¨ ìŠ¤ë¬´ë”©ì„ ì ìš©í•œ CrossEntropyLoss
class LabelSmoothingLoss(nn.Module):
    """
    ë¼ë²¨ ìŠ¤ë¬´ë”© ì†ì‹¤ í•¨ìˆ˜
    
    ë¼ë²¨ ìŠ¤ë¬´ë”©ì˜ íš¨ê³¼:
    1. ê³¼ì‹ ë¢° ë°©ì§€: ëª¨ë¸ì´ ë„ˆë¬´ í™•ì‹ í•˜ì§€ ì•Šë„ë¡
    2. ì¼ë°˜í™” í–¥ìƒ: ë” ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬ í•™ìŠµ
    3. ì •ê·œí™” íš¨ê³¼: ê³¼ì í•© ë°©ì§€
    
    ìˆ˜ì‹: y_smooth = (1-Î±)y_true + Î±/K
    ì—¬ê¸°ì„œ Î±ëŠ” ìŠ¤ë¬´ë”© ê³„ìˆ˜, KëŠ” í´ë˜ìŠ¤ ìˆ˜
    """
    
    def __init__(self, vocab_size, smoothing=0.1, ignore_index=PAD_IDX):
        super(LabelSmoothingLoss, self).__init__()
        self.vocab_size = vocab_size
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        
    def forward(self, pred, target):
        # pred: (batch_size * seq_len, vocab_size)
        # target: (batch_size * seq_len)
        
        batch_size, seq_len, vocab_size = pred.size()
        pred = pred.view(-1, vocab_size)
        target = target.view(-1)
        
        # ìœ íš¨í•œ í† í°ë§Œ ì„ íƒ (íŒ¨ë”© ì œì™¸)
        mask = target != self.ignore_index
        pred = pred[mask]
        target = target[mask]
        
        if pred.size(0) == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        # ë¼ë²¨ ìŠ¤ë¬´ë”© ì ìš©
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(self.smoothing / (vocab_size - 1))
        true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)
        
        # KL Divergence ê³„ì‚°
        log_pred = F.log_softmax(pred, dim=1)
        loss = F.kl_div(log_pred, true_dist, reduction='batchmean')
        
        return loss

# ì†ì‹¤ í•¨ìˆ˜
criterion = LabelSmoothingLoss(
    vocab_size=TGT_VOCAB_SIZE,
    smoothing=0.1,
    ignore_index=PAD_IDX
)

# ì˜µí‹°ë§ˆì´ì € (Adam with warmup)
optimizer = optim.Adam(
    model.parameters(),
    lr=LEARNING_RATE,
    betas=(0.9, 0.98),
    eps=1e-9
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Warmup + Cosine Annealing)
class WarmupScheduler:
    """
    Transformerë¥¼ ìœ„í•œ Warmup ìŠ¤ì¼€ì¤„ëŸ¬
    
    Warmupì˜ í•„ìš”ì„±:
    1. ì´ˆê¸° í° ê·¸ë˜ë””ì–¸íŠ¸ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„± ë°©ì§€
    2. ì ì§„ì  í•™ìŠµë¥  ì¦ê°€ë¡œ ì•ˆì •ì  í•™ìŠµ
    3. Transformerì˜ í‘œì¤€ í•™ìŠµ ë°©ë²•
    
    ìˆ˜ì‹: lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
    """
    
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0
        
    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.optimizer.step()
    
    def zero_grad(self):
        self.optimizer.zero_grad()

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
scheduler = WarmupScheduler(optimizer, D_MODEL, warmup_steps=4000)

print(f"   ì†ì‹¤ í•¨ìˆ˜: Label Smoothing CrossEntropy")
print(f"   ì˜µí‹°ë§ˆì´ì €: Adam with Warmup")
print(f"   ìŠ¤ì¼€ì¤„ëŸ¬: Warmup + Cosine Annealing")

# ============================================================================
# 6. BLEU ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
# ============================================================================

print(f"\nğŸ“Š BLEU ì ìˆ˜ í‰ê°€ ì‹œìŠ¤í…œ")

def calculate_bleu_score(predictions, references, max_n=4):
    """
    BLEU (Bilingual Evaluation Understudy) ì ìˆ˜ ê³„ì‚°
    
    BLEUëŠ” ê¸°ê³„ ë²ˆì—­ì˜ í‘œì¤€ í‰ê°€ ë©”íŠ¸ë¦­:
    1. n-gram ì •ë°€ë„ ê³„ì‚° (1-gram ~ 4-gram)
    2. ê¸¸ì´ íŒ¨ë„í‹° ì ìš©
    3. ê¸°í•˜ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
    
    Args:
        predictions: ì˜ˆì¸¡ ë¬¸ì¥ë“¤ (í† í° ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸)
        references: ì°¸ì¡° ë¬¸ì¥ë“¤ (í† í° ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸)
        max_n: ìµœëŒ€ n-gram í¬ê¸°
    
    Returns:
        float: BLEU ì ìˆ˜ (0~1)
    """
    
    if len(predictions) != len(references):
        return 0.0
    
    # ê°„ì†Œí™”ëœ BLEU ê³„ì‚°
    total_score = 0.0
    valid_pairs = 0
    
    for pred, ref in zip(predictions, references):
        if len(pred) == 0 or len(ref) == 0:
            continue
        
        # 1-gram ì •ë°€ë„ (ë‹¨ì–´ ì¼ì¹˜ìœ¨)
        pred_words = set(pred)
        ref_words = set(ref)
        
        if len(pred_words) == 0:
            continue
        
        precision = len(pred_words & ref_words) / len(pred_words)
        
        # ê¸¸ì´ íŒ¨ë„í‹°
        length_penalty = min(1.0, len(pred) / len(ref)) if len(ref) > 0 else 0.0
        
        # BLEU ì ìˆ˜ (ê°„ì†Œí™”)
        bleu = precision * length_penalty
        
        total_score += bleu
        valid_pairs += 1
    
    return total_score / valid_pairs if valid_pairs > 0 else 0.0

# ============================================================================
# 7. í›ˆë ¨ ë° ê²€ì¦ í•¨ìˆ˜
# ============================================================================

def train_epoch_transformer(model, train_loader, criterion, scheduler, device):
    """Transformer í›ˆë ¨ í•¨ìˆ˜"""
    model.train()
    
    running_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc="í›ˆë ¨")
    
    for batch in pbar:
        src = batch['src'].to(device)
        tgt = batch['tgt'].to(device)
        
        # íƒ€ê²Ÿ ì…ë ¥ê³¼ ì¶œë ¥ ë¶„ë¦¬
        tgt_input = tgt[:, :-1]  # <SOS> ~ ë§ˆì§€ë§‰ ì „ í† í°
        tgt_output = tgt[:, 1:]  # ì²« í† í° ~ <EOS>
        
        scheduler.zero_grad()
        
        # ìˆœì „íŒŒ
        output, attention_weights = model(src, tgt_input)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(output, tgt_output)
        
        # ì—­ì „íŒŒ
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss += loss.item()
        num_batches += 1
        
        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        if num_batches % 10 == 0:
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    return running_loss / num_batches

def validate_epoch_transformer(model, val_loader, criterion, device):
    """Transformer ê²€ì¦ í•¨ìˆ˜"""
    model.eval()
    
    running_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="ê²€ì¦")
        
        for batch in pbar:
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)
            
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
            
            output, _ = model(src, tgt_input)
            loss = criterion(output, tgt_output)
            
            running_loss += loss.item()
            num_batches += 1
            
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    return running_loss / num_batches

# ============================================================================
# 8. ë²ˆì—­ ìƒì„± í•¨ìˆ˜
# ============================================================================

def translate_sentence(model, src_sentence, src_word2idx, tgt_idx2word, 
                      device, max_length=50):
    """
    ë¬¸ì¥ ë²ˆì—­ í•¨ìˆ˜
    
    Args:
        model: í›ˆë ¨ëœ Transformer ëª¨ë¸
        src_sentence: ì†ŒìŠ¤ ë¬¸ì¥ (ë¬¸ìì—´)
        src_word2idx: ì†ŒìŠ¤ ì–¸ì–´ ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘
        tgt_idx2word: íƒ€ê²Ÿ ì–¸ì–´ ì¸ë±ìŠ¤-ë‹¨ì–´ ë§¤í•‘
        device: ë””ë°”ì´ìŠ¤
        max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
    
    Returns:
        str: ë²ˆì—­ëœ ë¬¸ì¥
    """
    model.eval()
    
    with torch.no_grad():
        # ì†ŒìŠ¤ ë¬¸ì¥ ì „ì²˜ë¦¬
        src_tokens = processor.tokenize(src_sentence)
        src_indices = []
        
        for token in src_tokens:
            if token in src_word2idx:
                src_indices.append(src_word2idx[token])
            else:
                src_indices.append(src_word2idx[UNK_TOKEN])
        
        # í…ì„œë¡œ ë³€í™˜
        src = torch.tensor([src_indices], device=device)
        
        # ì¸ì½”ë”©
        encoder_output, _ = model.encode(src)
        
        # ë””ì½”ë”© (ìê¸°íšŒê·€ì  ìƒì„±)
        tgt_indices = [SOS_IDX]  # <SOS>ë¡œ ì‹œì‘
        
        for _ in range(max_length):
            tgt = torch.tensor([tgt_indices], device=device)
            
            # ë””ì½”ë”©
            decoder_output, _, _ = model.decode(tgt, encoder_output)
            
            # ë‹¤ìŒ í† í° ì˜ˆì¸¡
            next_token_logits = model.output_projection(decoder_output[:, -1, :])
            next_token = torch.argmax(next_token_logits, dim=-1).item()
            
            # <EOS>ë©´ ì¢…ë£Œ
            if next_token == EOS_IDX:
                break
            
            tgt_indices.append(next_token)
        
        # ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜
        translated_tokens = []
        for idx in tgt_indices[1:]:  # <SOS> ì œì™¸
            if idx in tgt_idx2word:
                translated_tokens.append(tgt_idx2word[idx])
            else:
                translated_tokens.append(UNK_TOKEN)
        
        return ' '.join(translated_tokens)

# ============================================================================
# 9. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ Transformer í›ˆë ¨ ì‹œì‘")

# í›ˆë ¨ ê¸°ë¡
train_losses = []
val_losses = []
bleu_scores = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì 
best_val_loss = float('inf')
best_bleu_score = 0.0
best_model_state = None
patience = 10
patience_counter = 0

start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í›ˆë ¨
    train_loss = train_epoch_transformer(model, train_loader, criterion, scheduler, device)
    
    # ê²€ì¦
    val_loss = validate_epoch_transformer(model, val_loader, criterion, device)
    
    # ê¸°ë¡ ì €ì¥
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}")
    print(f"   ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
    
    # ì£¼ê¸°ì ìœ¼ë¡œ BLEU ì ìˆ˜ ê³„ì‚°
    if (epoch + 1) % 5 == 0:
        print(f"\nğŸ“Š BLEU ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        # ìƒ˜í”Œ ë²ˆì—­ ìˆ˜í–‰
        sample_predictions = []
        sample_references = []
        
        model.eval()
        with torch.no_grad():
            for i, batch in enumerate(val_loader):
                if i >= 10:  # 10ê°œ ë°°ì¹˜ë§Œ í‰ê°€
                    break
                
                src = batch['src'].to(device)
                tgt = batch['tgt'].to(device)
                
                for j in range(min(5, src.size(0))):  # ë°°ì¹˜ë‹¹ 5ê°œ ìƒ˜í”Œ
                    # ì†ŒìŠ¤ ë¬¸ì¥ ë³µì›
                    src_indices = src[j].cpu().tolist()
                    src_tokens = [processor.src_idx2word.get(idx, UNK_TOKEN) 
                                for idx in src_indices if idx != PAD_IDX]
                    src_sentence = ' '.join(src_tokens)
                    
                    # ì°¸ì¡° ë²ˆì—­ ë³µì›
                    tgt_indices = tgt[j].cpu().tolist()
                    ref_tokens = [processor.tgt_idx2word.get(idx, UNK_TOKEN) 
                                for idx in tgt_indices if idx not in [PAD_IDX, SOS_IDX, EOS_IDX]]
                    
                    # ë²ˆì—­ ìƒì„±
                    translated = translate_sentence(
                        model, src_sentence, processor.src_word2idx, 
                        processor.tgt_idx2word, device
                    )
                    
                    pred_tokens = translated.split()
                    
                    sample_predictions.append(pred_tokens)
                    sample_references.append(ref_tokens)
        
        # BLEU ì ìˆ˜ ê³„ì‚°
        bleu_score = calculate_bleu_score(sample_predictions, sample_references)
        bleu_scores.append(bleu_score)
        
        print(f"   BLEU ì ìˆ˜: {bleu_score:.4f}")
        
        # ìƒ˜í”Œ ë²ˆì—­ ì¶œë ¥
        print(f"\nğŸ”¤ ìƒ˜í”Œ ë²ˆì—­:")
        for i in range(min(3, len(sample_predictions))):
            src_text = ' '.join([processor.src_idx2word.get(idx, UNK_TOKEN) 
                               for idx in val_loader.dataset[i]['src'].tolist() 
                               if idx != PAD_IDX])
            print(f"   ì†ŒìŠ¤: {src_text}")
            print(f"   ì°¸ì¡°: {' '.join(sample_references[i])}")
            print(f"   ë²ˆì—­: {' '.join(sample_predictions[i])}")
            print()
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = copy.deepcopy(model.state_dict())
        patience_counter = 0
        print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(
            model, optimizer, epoch, val_loss, bleu_scores[-1] if bleu_scores else 0,
            save_path="./checkpoints/transformer_best_model.pth"
        )
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"   â° ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
            break

training_time = time.time() - start_time
print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
print(f"   ìµœê³  BLEU ì ìˆ˜: {max(bleu_scores) if bleu_scores else 0:.4f}")

# ============================================================================
# 10. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# í›ˆë ¨ ê³¡ì„ 
plot_training_curves(
    train_losses=train_losses,
    val_losses=val_losses,
    title="Transformer ê¸°ê³„ ë²ˆì—­ - í›ˆë ¨ ê³¼ì •"
)

# BLEU ì ìˆ˜ ë³€í™”
if bleu_scores:
    plt.figure(figsize=(10, 6))
    epochs_with_bleu = [i * 5 + 5 for i in range(len(bleu_scores))]
    plt.plot(epochs_with_bleu, bleu_scores, 'g-', marker='o', linewidth=2, markersize=6)
    plt.title('BLEU ì ìˆ˜ ë³€í™”', fontsize=16, fontweight='bold')
    plt.xlabel('ì—í¬í¬')
    plt.ylabel('BLEU ì ìˆ˜')
    plt.grid(True, alpha=0.3)
    plt.show()

# ============================================================================
# 11. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")

def visualize_attention(model, src_sentence, tgt_sentence, processor, device):
    """
    ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
    
    Self-Attentionê³¼ Cross-Attentionì˜ ê°€ì¤‘ì¹˜ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œí•˜ì—¬
    ëª¨ë¸ì´ ì–´ë–¤ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸
    """
    model.eval()
    
    with torch.no_grad():
        # ë¬¸ì¥ ì „ì²˜ë¦¬
        src_tokens = processor.tokenize(src_sentence)
        tgt_tokens = processor.tokenize(tgt_sentence)
        
        src_indices = processor.sentence_to_indices(src_sentence, processor.src_word2idx)
        tgt_indices = processor.sentence_to_indices(tgt_sentence, processor.tgt_word2idx, add_eos=True)
        
        # í…ì„œë¡œ ë³€í™˜
        src = torch.tensor([src_indices], device=device)
        tgt = torch.tensor([tgt_indices[:-1]], device=device)  # EOS ì œì™¸
        
        # ìˆœì „íŒŒ (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬í•¨)
        output, attention_weights = model(src, tgt)
        
        # Cross-Attention ê°€ì¤‘ì¹˜ (ë§ˆì§€ë§‰ ë ˆì´ì–´, ì²« ë²ˆì§¸ í—¤ë“œ)
        cross_attention = attention_weights['decoder_cross_attention'][-1][0, 0]  # (tgt_len, src_len)
        cross_attention = cross_attention.cpu().numpy()
        
        # ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Cross-Attention íˆíŠ¸ë§µ
        im1 = ax1.imshow(cross_attention, cmap='Blues', aspect='auto')
        ax1.set_title('Cross-Attention (Decoder â†’ Encoder)', fontweight='bold')
        ax1.set_xlabel('ì†ŒìŠ¤ í† í°')
        ax1.set_ylabel('íƒ€ê²Ÿ í† í°')
        
        # í† í° ë¼ë²¨ ì„¤ì •
        ax1.set_xticks(range(len(src_tokens)))
        ax1.set_xticklabels(src_tokens, rotation=45)
        ax1.set_yticks(range(len(tgt_tokens)))
        ax1.set_yticklabels(tgt_tokens)
        
        plt.colorbar(im1, ax=ax1)
        
        # Self-Attention ê°€ì¤‘ì¹˜ (ì¸ì½”ë” ë§ˆì§€ë§‰ ë ˆì´ì–´, ì²« ë²ˆì§¸ í—¤ë“œ)
        if attention_weights['encoder_attention']:
            self_attention = attention_weights['encoder_attention'][-1][0, 0]  # (src_len, src_len)
            self_attention = self_attention.cpu().numpy()
            
            im2 = ax2.imshow(self_attention, cmap='Reds', aspect='auto')
            ax2.set_title('Self-Attention (Encoder)', fontweight='bold')
            ax2.set_xlabel('ì†ŒìŠ¤ í† í°')
            ax2.set_ylabel('ì†ŒìŠ¤ í† í°')
            
            ax2.set_xticks(range(len(src_tokens)))
            ax2.set_xticklabels(src_tokens, rotation=45)
            ax2.set_yticks(range(len(src_tokens)))
            ax2.set_yticklabels(src_tokens)
            
            plt.colorbar(im2, ax=ax2)
        
        plt.tight_layout()
        plt.show()

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì–´í…ì…˜ ì‹œê°í™”
if best_model_state is not None:
    model.load_state_dict(best_model_state)

# ìƒ˜í”Œ ë¬¸ì¥ìœ¼ë¡œ ì–´í…ì…˜ ì‹œê°í™”
sample_src = "the man is walking"
sample_tgt = "der mann geht"

print(f"ğŸ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¶„ì„:")
print(f"   ì†ŒìŠ¤: {sample_src}")
print(f"   íƒ€ê²Ÿ: {sample_tgt}")

visualize_attention(model, sample_src, sample_tgt, processor, device)

# ============================================================================
# 12. ëŒ€í™”í˜• ë²ˆì—­ ì‹œìŠ¤í…œ
# ============================================================================

print(f"\nğŸ’¬ ëŒ€í™”í˜• ë²ˆì—­ ì‹œìŠ¤í…œ")

def interactive_translation():
    """ëŒ€í™”í˜• ë²ˆì—­ ì¸í„°í˜ì´ìŠ¤"""
    print("ğŸŒ ì˜ì–´ â†’ ë…ì¼ì–´ ë²ˆì—­ê¸°")
    print("'quit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("-" * 40)
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            english_text = input("\nì˜ì–´ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            if english_text.lower() == 'quit':
                print("ë²ˆì—­ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not english_text:
                continue
            
            # ë²ˆì—­ ìˆ˜í–‰
            german_translation = translate_sentence(
                model, english_text, processor.src_word2idx, 
                processor.tgt_idx2word, device
            )
            
            print(f"ë…ì¼ì–´ ë²ˆì—­: {german_translation}")
            
        except KeyboardInterrupt:
            print("\në²ˆì—­ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# ëŒ€í™”í˜• ë²ˆì—­ ì‹¤í–‰ (ì£¼ì„ ì²˜ë¦¬ - ìë™ ì‹¤í–‰ ë°©ì§€)
# interactive_translation()

# ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸
print(f"ğŸ”¤ ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
test_sentences = [
    "the cat is sleeping",
    "a good book",
    "people are walking",
    "the red car",
    "I have water"
]

for sentence in test_sentences:
    translation = translate_sentence(
        model, sentence, processor.src_word2idx, 
        processor.tgt_idx2word, device
    )
    print(f"   EN: {sentence}")
    print(f"   DE: {translation}")
    print()

# ============================================================================
# 13. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ì‹¤ìš©ì  í™œìš©
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œ")
print(f"   2. Self-Attentionê³¼ Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜")
print(f"   3. ìœ„ì¹˜ ì¸ì½”ë”©ê³¼ ë§ˆìŠ¤í‚¹ ê¸°ë²•")
print(f"   4. ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì™€ ê¸°ê³„ ë²ˆì—­")
print(f"   5. BLEU ì ìˆ˜ë¥¼ í†µí•œ ë²ˆì—­ í’ˆì§ˆ í‰ê°€")
print(f"   6. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”ì™€ í•´ì„")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
print(f"   - ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
print(f"   - ìµœê³  BLEU ì ìˆ˜: {max(bleu_scores) if bleu_scores else 0:.4f}")
print(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {params_info['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜: ëª¨ë“  ìœ„ì¹˜ ê°„ ì§ì ‘ ì—°ê²°")
print(f"   2. ë³‘ë ¬ ì²˜ë¦¬: RNNê³¼ ë‹¬ë¦¬ ìˆœì°¨ ì²˜ë¦¬ ë¶ˆí•„ìš”")
print(f"   3. ìœ„ì¹˜ ì¸ì½”ë”©: ìˆœì„œ ì •ë³´ ì œê³µ")
print(f"   4. ë©€í‹°í—¤ë“œ: ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ ìˆ˜ì§‘")
print(f"   5. ì”ì°¨ ì—°ê²°: ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì  í•™ìŠµ")

print(f"\nğŸ” Transformerì˜ ì¥ë‹¨ì :")
print(f"   ì¥ì :")
print(f"   - ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ í›ˆë ¨")
print(f"   - ì¥ê±°ë¦¬ ì˜ì¡´ì„± íš¨ê³¼ì  ì²˜ë¦¬")
print(f"   - í™•ì¥ì„± (ëŒ€ê·œëª¨ ëª¨ë¸ ê°€ëŠ¥)")
print(f"   - í•´ì„ ê°€ëŠ¥ì„± (ì–´í…ì…˜ ê°€ì¤‘ì¹˜)")
print(f"   ë‹¨ì :")
print(f"   - ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ ê³„ì‚° ë³µì¡ë„ ì¦ê°€")
print(f"   - ìœ„ì¹˜ ì •ë³´ ëª…ì‹œì  ì œê³µ í•„ìš”")

print(f"\nğŸš€ ì‹¤ìš©ì  í™œìš© ë¶„ì•¼:")
print(f"   1. ê¸°ê³„ ë²ˆì—­: Google Translate, DeepL")
print(f"   2. ì–¸ì–´ ëª¨ë¸: GPT, BERT, T5")
print(f"   3. í…ìŠ¤íŠ¸ ìš”ì•½: ë¬¸ì„œ ìë™ ìš”ì•½")
print(f"   4. ì§ˆì˜ì‘ë‹µ: ì±—ë´‡, ê²€ìƒ‰ ì‹œìŠ¤í…œ")
print(f"   5. ì½”ë“œ ìƒì„±: GitHub Copilot")
print(f"   6. ì´ë¯¸ì§€ ìº¡ì…”ë‹: Vision Transformer")

print(f"\nğŸ”§ Transformer ë°œì „ ê³¼ì •:")
print(f"   1. Transformer (2017): Attention Is All You Need")
print(f"   2. BERT (2018): ì–‘ë°©í–¥ ì¸ì½”ë”")
print(f"   3. GPT (2018): ìê¸°íšŒê·€ ë””ì½”ë”")
print(f"   4. T5 (2019): Text-to-Text Transfer")
print(f"   5. GPT-3 (2020): ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸")
print(f"   6. ChatGPT (2022): ëŒ€í™”í˜• AI")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   - í†µí•© í…ŒìŠ¤íŠ¸ ë° ë¬¸ì„œí™”")
print(f"   - ì „ì²´ íŠœí† ë¦¬ì–¼ ì‹œë¦¬ì¦ˆ ì™„ì„±")
print(f"   - README íŒŒì¼ ì—…ë°ì´íŠ¸")

print(f"\nğŸ”§ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. ì‹¤ì œ Multi30k ë°ì´í„°ì…‹ ì‚¬ìš©")
print(f"   2. Beam Search ë””ì½”ë”© êµ¬í˜„")
print(f"   3. ë‹¤ì–‘í•œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜ ì‹¤í—˜")
print(f"   4. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¸íŠœë‹")
print(f"   5. ë‹¤ë¥¸ ì–¸ì–´ ìŒìœ¼ë¡œ í™•ì¥")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ Transformer NLP íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ëª¨ë“  ë”¥ëŸ¬ë‹ íŠœí† ë¦¬ì–¼ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"=" * 60)
    betas=(0.9, 0.98),
    eps=1e-9
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Warmup + Cosine Annealing)
class WarmupScheduler:
    """
    Warmup + Cosine Annealing ìŠ¤ì¼€ì¤„ëŸ¬
    
    Warmupì˜ í•„ìš”ì„±:
    1. ì´ˆê¸° ì•ˆì •í™”: í° í•™ìŠµë¥ ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„± ë°©ì§€
    2. ì ì§„ì  ì¦ê°€: ëª¨ë¸ì´ ì ì‘í•  ì‹œê°„ ì œê³µ
    3. Transformer íŠ¹ì„±: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì•ˆì •ì  ì´ˆê¸°í™”
    """
    
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0
    
    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.optimizer.step()
    
    def zero_grad(self):
        self.optimizer.zero_grad()

scheduler = WarmupScheduler(optimizer, D_MODEL, warmup_steps=4000)

print(f"   ì†ì‹¤ í•¨ìˆ˜: LabelSmoothingLoss (smoothing=0.1)")
print(f"   ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE})")
print(f"   ìŠ¤ì¼€ì¤„ëŸ¬: Warmup + Cosine Annealing")#
 ============================================================================
# 6. í›ˆë ¨ í•¨ìˆ˜ ì •ì˜
# ============================================================================

def train_transformer_epoch(model, train_loader, criterion, scheduler, device, epoch):
    """Transformer í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    
    running_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"ì—í¬í¬ {epoch+1} í›ˆë ¨")
    
    for batch_idx, batch in enumerate(pbar):
        src = batch['src'].to(device)
        tgt = batch['tgt'].to(device)
        
        # ë””ì½”ë” ì…ë ¥ (SOS í† í° ì¶”ê°€)
        tgt_input = torch.cat([
            torch.full((tgt.size(0), 1), SOS_IDX, device=device),
            tgt[:, :-1]
        ], dim=1)
        
        # íƒ€ê²Ÿ (EOS í† í° í¬í•¨)
        tgt_output = tgt
        
        scheduler.zero_grad()
        
        # ìˆœì „íŒŒ
        output, attention_weights = model(src, tgt_input)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(output, tgt_output)
        
        # ì—­ì „íŒŒ
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        scheduler.step()
        
        running_loss += loss.item()
        num_batches += 1
        
        pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    avg_loss = running_loss / num_batches
    return avg_loss

def validate_transformer_epoch(model, val_loader, criterion, device):
    """Transformer ê²€ì¦"""
    model.eval()
    
    running_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="ê²€ì¦")
        
        for batch in pbar:
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)
            
            tgt_input = torch.cat([
                torch.full((tgt.size(0), 1), SOS_IDX, device=device),
                tgt[:, :-1]
            ], dim=1)
            
            tgt_output = tgt
            
            output, _ = model(src, tgt_input)
            loss = criterion(output, tgt_output)
            
            running_loss += loss.item()
            num_batches += 1
            
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    avg_loss = running_loss / num_batches
    return avg_loss

# ============================================================================
# 7. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ Transformer í›ˆë ¨ ì‹œì‘")

# í›ˆë ¨ ê¸°ë¡
train_losses = []
val_losses = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì 
best_val_loss = float('inf')
best_model_state = None
patience = 10
patience_counter = 0

start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í›ˆë ¨
    train_loss = train_transformer_epoch(
        model, train_loader, criterion, scheduler, device, epoch
    )
    
    # ê²€ì¦
    val_loss = validate_transformer_epoch(
        model, val_loader, criterion, device
    )
    
    # ê¸°ë¡ ì €ì¥
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}")
    print(f"   ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = copy.deepcopy(model.state_dict())
        patience_counter = 0
        print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(
            model, optimizer, epoch, val_loss, 0,
            save_path="./checkpoints/transformer_best.pth"
        )
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"   â° ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
            break

training_time = time.time() - start_time
print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")

# ============================================================================
# 8. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# í›ˆë ¨ ê³¡ì„ 
plot_training_curves(
    train_losses=train_losses,
    val_losses=val_losses,
    title="Transformer ê¸°ê³„ ë²ˆì—­ - í›ˆë ¨ ê³¼ì •"
)

# ============================================================================
# 9. ë²ˆì—­ ë° í‰ê°€
# ============================================================================

print(f"\nğŸŒ ë²ˆì—­ ì„±ëŠ¥ í‰ê°€")

def translate_sentence(model, sentence, processor, device, max_length=50):
    """
    ë‹¨ì¼ ë¬¸ì¥ ë²ˆì—­
    
    Args:
        model: í›ˆë ¨ëœ Transformer ëª¨ë¸
        sentence: ë²ˆì—­í•  ë¬¸ì¥ (ì†ŒìŠ¤ ì–¸ì–´)
        processor: ë°ì´í„° ì „ì²˜ë¦¬ê¸°
        device: ì—°ì‚° ì¥ì¹˜
        max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
    
    Returns:
        str: ë²ˆì—­ëœ ë¬¸ì¥ (íƒ€ê²Ÿ ì–¸ì–´)
    """
    model.eval()
    
    # ì†ŒìŠ¤ ë¬¸ì¥ ì „ì²˜ë¦¬
    src_tokens = processor.sentence_to_indices(
        sentence, processor.src_word2idx, add_eos=False
    )
    src_tensor = torch.tensor([src_tokens], device=device)
    
    # ì¸ì½”ë”©
    with torch.no_grad():
        encoder_output, _ = model.encode(src_tensor)
    
    # ë””ì½”ë”© (ìê¸°íšŒê·€ì  ìƒì„±)
    tgt_tokens = [SOS_IDX]
    
    for _ in range(max_length):
        tgt_tensor = torch.tensor([tgt_tokens], device=device)
        
        with torch.no_grad():
            output, _ = model(src_tensor, tgt_tensor)
            
        # ë‹¤ìŒ í† í° ì˜ˆì¸¡
        next_token_logits = output[0, -1, :]
        next_token = torch.argmax(next_token_logits).item()
        
        tgt_tokens.append(next_token)
        
        # EOS í† í°ì´ë©´ ì¢…ë£Œ
        if next_token == EOS_IDX:
            break
    
    # í† í°ì„ ë‹¨ì–´ë¡œ ë³€í™˜
    translated_words = []
    for token_id in tgt_tokens[1:]:  # SOS ì œì™¸
        if token_id == EOS_IDX:
            break
        if token_id in processor.tgt_idx2word:
            word = processor.tgt_idx2word[token_id]
            if word not in SPECIAL_TOKENS:
                translated_words.append(word)
    
    return ' '.join(translated_words)

def calculate_bleu_score(model, test_loader, processor, device):
    """
    BLEU ì ìˆ˜ ê³„ì‚°
    
    BLEU (Bilingual Evaluation Understudy):
    - ê¸°ê³„ ë²ˆì—­ í’ˆì§ˆ í‰ê°€ì˜ í‘œì¤€ ë©”íŠ¸ë¦­
    - n-gram ì •ë°€ë„ì˜ ê¸°í•˜ í‰ê· 
    - ê°„ê²°ì„± í˜ë„í‹° ì ìš©
    - 0~1 ë²”ìœ„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
    """
    model.eval()
    
    references = []  # ì •ë‹µ ë²ˆì—­
    candidates = []  # ëª¨ë¸ ë²ˆì—­
    
    print("ğŸ” BLEU ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ë²ˆì—­ ì¤‘...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_loader, desc="ë²ˆì—­")):
            if batch_idx >= 50:  # ê³„ì‚° ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ì œí•œ
                break
                
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)
            
            batch_size = src.size(0)
            
            for i in range(batch_size):
                # ì†ŒìŠ¤ ë¬¸ì¥ ë³µì›
                src_tokens = src[i].cpu().numpy()
                src_words = []
                for token_id in src_tokens:
                    if token_id == PAD_IDX:
                        break
                    if token_id in processor.src_idx2word:
                        word = processor.src_idx2word[token_id]
                        if word not in SPECIAL_TOKENS:
                            src_words.append(word)
                
                src_sentence = ' '.join(src_words)
                
                # ì •ë‹µ ë²ˆì—­ ë³µì›
                tgt_tokens = tgt[i].cpu().numpy()
                ref_words = []
                for token_id in tgt_tokens:
                    if token_id in [PAD_IDX, EOS_IDX]:
                        break
                    if token_id in processor.tgt_idx2word:
                        word = processor.tgt_idx2word[token_id]
                        if word not in SPECIAL_TOKENS:
                            ref_words.append(word)
                
                # ëª¨ë¸ ë²ˆì—­
                translated = translate_sentence(model, src_sentence, processor, device)
                
                if ref_words and translated:
                    references.append([ref_words])  # BLEUëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
                    candidates.append(translated.split())
    
    if not references or not candidates:
        print("âŒ ë²ˆì—­ ê²°ê³¼ê°€ ì—†ì–´ BLEU ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0.0
    
    # BLEU ì ìˆ˜ ê³„ì‚°
    try:
        bleu_score = corpus_bleu(references, candidates)
        return bleu_score
    except:
        print("âŒ BLEU ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        return 0.0

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
if best_model_state is not None:
    model.load_state_dict(best_model_state)

# ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸
print(f"\nğŸ“ ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸:")

sample_sentences = [
    "the man is good",
    "a big house",
    "the cat and the dog",
    "I see the book",
    "red car"
]

for sentence in sample_sentences:
    translation = translate_sentence(model, sentence, processor, device)
    print(f"   EN: {sentence}")
    print(f"   DE: {translation}")
    print()

# BLEU ì ìˆ˜ ê³„ì‚°
bleu_score = calculate_bleu_score(model, test_loader, processor, device)
print(f"ğŸ“Š BLEU ì ìˆ˜: {bleu_score:.4f}")

# ============================================================================
# 10. ì–´í…ì…˜ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ‘ï¸  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")

def visualize_attention(model, sentence, processor, device):
    """
    ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
    
    ì–´í…ì…˜ ì‹œê°í™”ì˜ ì˜ë¯¸:
    1. ëª¨ë¸ í•´ì„: ì–´ë–¤ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸
    2. ì •ë ¬ í™•ì¸: ì†ŒìŠ¤-íƒ€ê²Ÿ ë‹¨ì–´ ê°„ ëŒ€ì‘ ê´€ê³„
    3. ë””ë²„ê¹…: ëª¨ë¸ì˜ ì˜ëª»ëœ ë™ì‘ íŒŒì•…
    """
    model.eval()
    
    # ë¬¸ì¥ ì „ì²˜ë¦¬
    src_tokens = processor.sentence_to_indices(
        sentence, processor.src_word2idx, add_eos=False
    )
    src_tensor = torch.tensor([src_tokens], device=device)
    
    # ë²ˆì—­ ìƒì„± (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ìˆ˜ì§‘)
    tgt_tokens = [SOS_IDX]
    attention_weights = []
    
    src_words = []
    for token_id in src_tokens:
        if token_id in processor.src_idx2word:
            word = processor.src_idx2word[token_id]
            if word not in SPECIAL_TOKENS:
                src_words.append(word)
    
    tgt_words = []
    
    with torch.no_grad():
        for step in range(20):  # ìµœëŒ€ 20 í† í°
            tgt_tensor = torch.tensor([tgt_tokens], device=device)
            
            output, attn_weights = model(src_tensor, tgt_tensor)
            
            # ë‹¤ìŒ í† í° ì˜ˆì¸¡
            next_token_logits = output[0, -1, :]
            next_token = torch.argmax(next_token_logits).item()
            
            tgt_tokens.append(next_token)
            
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì €ì¥ (ë§ˆì§€ë§‰ ë ˆì´ì–´, ì²« ë²ˆì§¸ í—¤ë“œ)
            cross_attn = attn_weights['decoder_cross_attention'][-1]  # ë§ˆì§€ë§‰ ë ˆì´ì–´
            attn_weight = cross_attn[0, 0, -1, :len(src_tokens)].cpu().numpy()  # ì²« ë²ˆì§¸ í—¤ë“œ
            attention_weights.append(attn_weight)
            
            # ë‹¨ì–´ ì¶”ê°€
            if next_token in processor.tgt_idx2word:
                word = processor.tgt_idx2word[next_token]
                if word == EOS_TOKEN:
                    break
                elif word not in SPECIAL_TOKENS:
                    tgt_words.append(word)
            
            if next_token == EOS_IDX:
                break
    
    # ì–´í…ì…˜ íˆíŠ¸ë§µ ì‹œê°í™”
    if attention_weights and src_words and tgt_words:
        attention_matrix = np.array(attention_weights)
        
        plt.figure(figsize=(12, 8))
        
        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        sns.heatmap(
            attention_matrix,
            xticklabels=src_words,
            yticklabels=tgt_words,
            cmap='Blues',
            cbar=True,
            square=False
        )
        
        plt.title(f'Cross-Attention ê°€ì¤‘ì¹˜\nì†ŒìŠ¤: "{sentence}"')
        plt.xlabel('ì†ŒìŠ¤ ë‹¨ì–´ (ì˜ì–´)')
        plt.ylabel('íƒ€ê²Ÿ ë‹¨ì–´ (ë…ì¼ì–´)')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()
        
        print(f"âœ… ì–´í…ì…˜ ì‹œê°í™” ì™„ë£Œ")
        print(f"   ì†ŒìŠ¤: {' '.join(src_words)}")
        print(f"   íƒ€ê²Ÿ: {' '.join(tgt_words)}")
    else:
        print(f"âŒ ì–´í…ì…˜ ì‹œê°í™” ì‹¤íŒ¨: ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì–´í…ì…˜ ì‹œê°í™” ì‹¤í–‰
sample_sentence = "the man is good"
visualize_attention(model, sample_sentence, processor, device)

# ============================================================================
# 11. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ êµ¬í˜„")
print(f"   2. Self-Attentionê³¼ Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜")
print(f"   3. ìœ„ì¹˜ ì¸ì½”ë”©ê³¼ ë§ˆìŠ¤í‚¹ ê¸°ë²•")
print(f"   4. ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¡œ ê¸°ê³„ ë²ˆì—­ êµ¬í˜„")
print(f"   5. BLEU ì ìˆ˜ë¥¼ í†µí•œ ë²ˆì—­ í’ˆì§ˆ í‰ê°€")
print(f"   6. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”ì™€ í•´ì„")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
print(f"   - ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
print(f"   - BLEU ì ìˆ˜: {bleu_score:.4f}")
print(f"   - ì´ íŒŒë¼ë¯¸í„°: {params_info['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜: ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì˜ ì§ì ‘ì  ëª¨ë¸ë§")
print(f"   2. ë³‘ë ¬ ì²˜ë¦¬: RNN ëŒ€ë¹„ í›¨ì”¬ ë¹ ë¥¸ í›ˆë ¨ê³¼ ì¶”ë¡ ")
print(f"   3. ìœ„ì¹˜ ì¸ì½”ë”©: ìˆœì„œ ì •ë³´ ì—†ëŠ” êµ¬ì¡°ì— ìœ„ì¹˜ ì •ë³´ ì£¼ì…")
print(f"   4. ë§ˆìŠ¤í‚¹: í›ˆë ¨ê³¼ ì¶”ë¡  ì‹œ ì •ë³´ ëˆ„ìˆ˜ ë°©ì§€")
print(f"   5. ì”ì°¨ ì—°ê²°: ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ ì•ˆì •ì  í›ˆë ¨")

print(f"\nğŸ” Transformerì˜ í˜ì‹ :")
print(f"   1. 'Attention is All You Need': ìˆœí™˜/í•©ì„±ê³± ì—†ì´ ì–´í…ì…˜ë§Œìœ¼ë¡œ")
print(f"   2. í™•ì¥ì„±: ëª¨ë¸ê³¼ ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒ")
print(f"   3. ë²”ìš©ì„±: NLP ì „ ë¶„ì•¼ì—ì„œ SOTA ë‹¬ì„±")
print(f"   4. ì „ì´ í•™ìŠµ: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ íŒŒì¸íŠœë‹")
print(f"   5. í•´ì„ ê°€ëŠ¥ì„±: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ë™ì‘ ì´í•´")

print(f"\nğŸš€ Transformer ë°œì „ì‚¬:")
print(f"   - 2017: ì›ì¡° Transformer (ê¸°ê³„ ë²ˆì—­)")
print(f"   - 2018: BERT (ì–‘ë°©í–¥ ì¸ì½”ë”)")
print(f"   - 2019: GPT-2 (ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)")
print(f"   - 2020: GPT-3 (1750ì–µ íŒŒë¼ë¯¸í„°)")
print(f"   - 2022: ChatGPT (ëŒ€í™”í˜• AI)")
print(f"   - 2023: GPT-4 (ë©€í‹°ëª¨ë‹¬)")

print(f"\nğŸ”§ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸: BERT, GPT íŒŒì¸íŠœë‹")
print(f"   2. ë‹¤ë¥¸ NLP íƒœìŠ¤í¬: ê°ì • ë¶„ì„, ì§ˆì˜ì‘ë‹µ, ìš”ì•½")
print(f"   3. ë©€í‹°ëª¨ë‹¬: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²°í•© ëª¨ë¸")
print(f"   4. íš¨ìœ¨ì  ì–´í…ì…˜: Sparse Attention, Linear Attention")
print(f"   5. ëª¨ë¸ ì••ì¶•: ì§€ì‹ ì¦ë¥˜, ì–‘ìí™”")

print(f"\nğŸ¯ ì‹¤ì œ ì‘ìš© ë¶„ì•¼:")
print(f"   - ê¸°ê³„ ë²ˆì—­: Google ë²ˆì—­, DeepL")
print(f"   - ê²€ìƒ‰ ì—”ì§„: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰")
print(f"   - ì±—ë´‡: ChatGPT, Claude, Bard")
print(f"   - ì½”ë“œ ìƒì„±: GitHub Copilot, CodeT5")
print(f"   - ì°½ì‘ ë„êµ¬: ê¸€ì“°ê¸° ë³´ì¡°, ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ Transformer NLP íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ëª¨ë“  ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆë¥¼ ì™„ì£¼í•˜ì…¨ìŠµë‹ˆë‹¤!")
print(f"=" * 60)

print(f"\nğŸ† ì „ì²´ ì‹œë¦¬ì¦ˆ ì™„ì£¼ ì¶•í•˜í•©ë‹ˆë‹¤!")
print(f"   1. âœ… PyTorch ê¸°ì´ˆ (MNIST)")
print(f"   2. âœ… ì‹ ê²½ë§ ì‹¬í™” (Fashion-MNIST)")
print(f"   3. âœ… CNN ì´ë¯¸ì§€ ë¶„ë¥˜ (CIFAR-10)")
print(f"   4. âœ… RNN í…ìŠ¤íŠ¸ ë¶„ë¥˜ (IMDB)")
print(f"   5. âœ… LSTM ì‹œê³„ì—´ ì˜ˆì¸¡ (ì£¼ì‹ ë°ì´í„°)")
print(f"   6. âœ… YOLO ê°ì²´ íƒì§€ (COCO)")
print(f"   7. âœ… GAN ì´ë¯¸ì§€ ìƒì„± (CelebA)")
print(f"   8. âœ… Transformer NLP (Multi30k)")

print(f"\nğŸŒŸ ì´ì œ ì—¬ëŸ¬ë¶„ì€ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ê¸°ë²•ë“¤ì„ ëª¨ë‘ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!")
print(f"   ê³„ì†í•´ì„œ ìµœì‹  ì—°êµ¬ì™€ ê¸°ìˆ ì„ íƒêµ¬í•˜ë©° ì„±ì¥í•˜ì„¸ìš”!")