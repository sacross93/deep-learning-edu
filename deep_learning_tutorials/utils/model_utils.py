"""
λ¨λΈ μ ν‹Έλ¦¬ν‹° ν•¨μ λ¨λ“

μ΄ λ¨λ“μ€ λ”¥λ¬λ‹ λ¨λΈ κ΄€λ ¨ κ³µν†µ κΈ°λ¥μ„ μ κ³µν•©λ‹λ‹¤.
- λ¨λΈ νλΌλ―Έν„° μ κ³„μ‚° λ° λ¶„μ„
- λ¨λΈ μ²΄ν¬ν¬μΈνΈ μ €μ¥/λ΅λ“
- λ¨λΈ μ„±λ¥ ν‰κ°€ λ° λ©”νΈλ¦­ κ³„μ‚°
- λ¨λΈ κµ¬μ΅° λ¶„μ„ λ° μ‹κ°ν™”

κ° ν•¨μλ” κµμ΅ λ©μ μ— λ§κ² μƒμ„Έν• μ„¤λ…κ³Ό λ¶„μ„ κ²°κ³Όλ¥Ό ν¬ν•¨ν•©λ‹λ‹¤.
"""

import torch
import torch.nn as nn
import numpy as np
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
import time
from collections import OrderedDict
import json


def count_parameters(
    model: nn.Module, 
    trainable_only: bool = False,
    detailed: bool = True
) -> Dict[str, Any]:
    """
    λ¨λΈμ νλΌλ―Έν„° μλ¥Ό κ³„μ‚°ν•κ³  λ¶„μ„ν•©λ‹λ‹¤.
    
    Args:
        model: λ¶„μ„ν•  PyTorch λ¨λΈ
        trainable_only: Trueμ΄λ©΄ ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°λ§ κ³„μ‚°
        detailed: Trueμ΄λ©΄ λ μ΄μ–΄λ³„ μƒμ„Έ μ •λ³΄ μ¶λ ¥
    
    Returns:
        Dict: νλΌλ―Έν„° μ λ° λ¶„μ„ κ²°κ³Ό
    
    κµμ΅μ  λ©μ :
    - λ¨λΈμ λ³µμ΅λ„λ¥Ό νλΌλ―Έν„° μλ΅ μ •λ‰ν™”ν•μ—¬ μ΄ν•΄
    - λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ° μ—°μ‚°λ‰ μ¶”μ •μ— λ„μ›€
    - λ¨λΈ κ²½λ‰ν™”λ‚ μ••μ¶• ν•„μ”μ„± νλ‹¨ κΈ°μ¤€ μ κ³µ
    - λ μ΄μ–΄λ³„ νλΌλ―Έν„° λ¶„ν¬λ΅ λ¨λΈ κµ¬μ΅° μ΄ν•΄
    """
    
    print("π” λ¨λΈ νλΌλ―Έν„° λ¶„μ„ μ¤‘...")
    
    total_params = 0
    trainable_params = 0
    layer_info = []
    
    # λ μ΄μ–΄λ³„ νλΌλ―Έν„° κ³„μ‚°
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        if param.requires_grad:
            trainable_params += param_count
        
        if detailed:
            layer_info.append({
                'name': name,
                'shape': list(param.shape),
                'params': param_count,
                'trainable': param.requires_grad,
                'dtype': str(param.dtype)
            })
    
    # κ²°κ³Ό λ”•μ…”λ„λ¦¬ μƒμ„±
    result = {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'non_trainable_params': total_params - trainable_params,
        'layer_info': layer_info if detailed else None
    }
    
    # κ²°κ³Ό μ¶λ ¥
    print(f"\nπ“ λ¨λΈ νλΌλ―Έν„° λ¶„μ„ κ²°κ³Ό:")
    print(f"  - μ „μ²΄ νλΌλ―Έν„°: {total_params:,}κ°")
    print(f"  - ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°: {trainable_params:,}κ°")
    print(f"  - κ³ μ •λ νλΌλ―Έν„°: {total_params - trainable_params:,}κ°")
    
    # λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”μ • (float32 κΈ°μ¤€)
    memory_mb = (total_params * 4) / (1024 * 1024)  # 4 bytes per float32
    print(f"  - μμƒ λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {memory_mb:.2f} MB")
    
    # λ¨λΈ ν¬κΈ° λ¶„λ¥
    if total_params < 1e6:
        size_category = "μ†ν• λ¨λΈ (< 1M)"
    elif total_params < 1e7:
        size_category = "μ¤‘ν• λ¨λΈ (1M - 10M)"
    elif total_params < 1e8:
        size_category = "λ€ν• λ¨λΈ (10M - 100M)"
    else:
        size_category = "μ΄λ€ν• λ¨λΈ (> 100M)"
    
    print(f"  - λ¨λΈ ν¬κΈ° λ¶„λ¥: {size_category}")
    
    # μƒμ„Έ μ •λ³΄ μ¶λ ¥
    if detailed and layer_info:
        print(f"\nπ“‹ λ μ΄μ–΄λ³„ νλΌλ―Έν„° λ¶„ν¬:")
        print(f"{'λ μ΄μ–΄ μ΄λ¦„':<30} {'ν•νƒ':<20} {'νλΌλ―Έν„° μ':<15} {'ν›λ ¨κ°€λ¥'}")
        print("-" * 80)
        
        for info in layer_info:
            shape_str = str(info['shape'])
            trainable_str = "β“" if info['trainable'] else "β—"
            print(f"{info['name']:<30} {shape_str:<20} {info['params']:<15,} {trainable_str}")
    
    # μ„±λ¥ λ¶„μ„ λ° μ΅°μ–Έ
    print(f"\nπ’΅ λ¨λΈ λ¶„μ„ λ° μ΅°μ–Έ:")
    
    if total_params > 1e8:
        print(f"  β οΈ  λ§¤μ° ν° λ¨λΈμ…λ‹λ‹¤. λ‹¤μμ„ κ³ λ ¤ν•μ„Έμ”:")
        print(f"     - GPU λ©”λ¨λ¦¬ λ¶€μ΅± μ‹ λ°°μΉ ν¬κΈ° κ°μ†")
        print(f"     - κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν… μ‚¬μ©")
        print(f"     - λ¨λΈ λ³‘λ ¬ν™” κ³ λ ¤")
    elif total_params < 1e5:
        print(f"  π’΅ μ‘μ€ λ¨λΈμ…λ‹λ‹¤. μ„±λ¥ ν–¥μƒμ„ μ„ν•΄:")
        print(f"     - λ¨λΈ κΉμ΄λ‚ λ„λΉ„ μ¦κ°€ κ³ λ ¤")
        print(f"     - λ” λ³µμ΅ν• μ•„ν‚¤ν…μ² μ‹λ„")
    
    # ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„° λΉ„μ¨ λ¶„μ„
    trainable_ratio = trainable_params / total_params if total_params > 0 else 0
    if trainable_ratio < 0.5:
        print(f"  π“ κ³ μ •λ νλΌλ―Έν„°κ°€ λ§μµλ‹λ‹¤ ({trainable_ratio:.1%} ν›λ ¨ κ°€λ¥)")
        print(f"     - μ „μ΄ ν•™μµμ΄λ‚ νμΈ νλ‹ μ¤‘μΈ κ²ƒμΌλ΅ λ³΄μ…λ‹λ‹¤")
    
    return result


def save_checkpoint(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    loss: float,
    accuracy: Optional[float] = None,
    save_path: str = "checkpoint.pth",
    additional_info: Optional[Dict] = None
) -> None:
    """
    λ¨λΈ μ²΄ν¬ν¬μΈνΈλ¥Ό μ €μ¥ν•©λ‹λ‹¤.
    
    Args:
        model: μ €μ¥ν•  λ¨λΈ
        optimizer: μµν‹°λ§μ΄μ € μƒνƒ
        epoch: ν„μ¬ μ—ν¬ν¬
        loss: ν„μ¬ μ†μ‹¤κ°’
        accuracy: ν„μ¬ μ •ν™•λ„ (μ„ νƒμ‚¬ν•­)
        save_path: μ €μ¥ κ²½λ΅
        additional_info: μ¶”κ°€ μ •λ³΄ λ”•μ…”λ„λ¦¬
    
    κµμ΅μ  λ©μ :
    - κΈ΄ ν›λ ¨ κ³Όμ •μ—μ„ μ¤‘κ°„ κ²°κ³Ό λ³΄μ΅΄
    - μµμ  λ¨λΈ μƒνƒ μ €μ¥μΌλ΅ κ³Όμ ν•© λ°©μ§€
    - ν›λ ¨ μ¬κ° λ° μ‹¤ν— μ¬ν„μ„± ν™•λ³΄
    - λ¨λΈ λ°°ν¬λ¥Ό μ„ν• μƒνƒ μ €μ¥
    """
    
    print(f"π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ¤‘: {save_path}")
    
    # μ €μ¥ν•  μ •λ³΄ κµ¬μ„±
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'timestamp': time.time(),
        'model_class': model.__class__.__name__
    }
    
    # μ„ νƒμ  μ •λ³΄ μ¶”κ°€
    if accuracy is not None:
        checkpoint['accuracy'] = accuracy
    
    if additional_info:
        checkpoint.update(additional_info)
    
    # λ””λ ‰ν† λ¦¬ μƒμ„±
    save_dir = Path(save_path).parent
    save_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        torch.save(checkpoint, save_path)
        
        # μ €μ¥ μ •λ³΄ μ¶λ ¥
        file_size = os.path.getsize(save_path) / (1024 * 1024)  # MB
        print(f"β… μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ™„λ£:")
        print(f"  - νμΌ ν¬κΈ°: {file_size:.2f} MB")
        print(f"  - μ—ν¬ν¬: {epoch}")
        print(f"  - μ†μ‹¤: {loss:.6f}")
        if accuracy is not None:
            print(f"  - μ •ν™•λ„: {accuracy:.4f}")
        
    except Exception as e:
        print(f"β μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ‹¤ν¨: {e}")


def load_checkpoint(
    model: nn.Module,
    checkpoint_path: str,
    optimizer: Optional[torch.optim.Optimizer] = None,
    device: str = 'cpu',
    strict: bool = True
) -> Dict[str, Any]:
    """
    μ €μ¥λ μ²΄ν¬ν¬μΈνΈλ¥Ό λ΅λ“ν•©λ‹λ‹¤.
    
    Args:
        model: μ²΄ν¬ν¬μΈνΈλ¥Ό λ΅λ“ν•  λ¨λΈ
        checkpoint_path: μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅
        optimizer: μµν‹°λ§μ΄μ € (μƒνƒ λ³µμ› μ‹ ν•„μ”)
        device: λ΅λ“ν•  μ¥μΉ
        strict: μ—„κ²©ν• μƒνƒ λ”•μ…”λ„λ¦¬ λ§¤μΉ­ μ—¬λ¶€
    
    Returns:
        Dict: λ΅λ“λ μ²΄ν¬ν¬μΈνΈ μ •λ³΄
    
    κµμ΅μ  λ©μ :
    - μ €μ¥λ λ¨λΈ μƒνƒ λ³µμ›μΌλ΅ ν›λ ¨ μ¬κ°
    - μ‚¬μ „ ν›λ ¨λ λ¨λΈ ν™μ© (μ „μ΄ ν•™μµ)
    - λ¨λΈ λ°°ν¬ λ° μ¶”λ΅  ν™κ²½ κµ¬μ„±
    - μ‹¤ν— κ²°κ³Ό μ¬ν„ λ° λΉ„κµ
    """
    
    print(f"π“‚ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ¤‘: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {checkpoint_path}")
    
    try:
        # μ²΄ν¬ν¬μΈνΈ λ΅λ“
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # λ¨λΈ μƒνƒ λ΅λ“
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=strict)
            print(f"β… λ¨λΈ μƒνƒ λ΅λ“ μ™„λ£")
        else:
            print(f"β οΈ  λ¨λΈ μƒνƒλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤")
        
        # μµν‹°λ§μ΄μ € μƒνƒ λ΅λ“
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print(f"β… μµν‹°λ§μ΄μ € μƒνƒ λ΅λ“ μ™„λ£")
        
        # λ΅λ“ μ •λ³΄ μ¶λ ¥
        print(f"π“ μ²΄ν¬ν¬μΈνΈ μ •λ³΄:")
        if 'epoch' in checkpoint:
            print(f"  - μ—ν¬ν¬: {checkpoint['epoch']}")
        if 'loss' in checkpoint:
            print(f"  - μ†μ‹¤: {checkpoint['loss']:.6f}")
        if 'accuracy' in checkpoint:
            print(f"  - μ •ν™•λ„: {checkpoint['accuracy']:.4f}")
        if 'timestamp' in checkpoint:
            save_time = time.ctime(checkpoint['timestamp'])
            print(f"  - μ €μ¥ μ‹κ°„: {save_time}")
        
        return checkpoint
        
    except Exception as e:
        print(f"β μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}")
        raise


def evaluate_model(
    model: nn.Module,
    dataloader: torch.utils.data.DataLoader,
    criterion: nn.Module,
    device: str = 'cpu',
    num_classes: Optional[int] = None
) -> Dict[str, float]:
    """
    λ¨λΈμ μ„±λ¥μ„ ν‰κ°€ν•©λ‹λ‹¤.
    
    Args:
        model: ν‰κ°€ν•  λ¨λΈ
        dataloader: ν‰κ°€ λ°μ΄ν„° λ΅λ”
        criterion: μ†μ‹¤ ν•¨μ
        device: μ—°μ‚° μ¥μΉ
        num_classes: ν΄λμ¤ μ (λ¶„λ¥ λ¬Έμ μΈ κ²½μ°)
    
    Returns:
        Dict: ν‰κ°€ λ©”νΈλ¦­ κ²°κ³Ό
    
    κµμ΅μ  λ©μ :
    - λ¨λΈμ μΌλ°ν™” μ„±λ¥ μ •λ‰μ  μΈ΅μ •
    - λ‹¤μ–‘ν• λ©”νΈλ¦­μΌλ΅ λ¨λΈ μ„±λ¥ λ‹¤κ°λ„ λ¶„μ„
    - ν΄λμ¤λ³„ μ„±λ¥ λ¶„μ„μΌλ΅ λ¨λΈ νΈν–¥μ„± ν™•μΈ
    - μ¶”λ΅  μ†λ„ μΈ΅μ •μΌλ΅ μ‹¤μ©μ„± ν‰κ°€
    """
    
    print("π― λ¨λΈ μ„±λ¥ ν‰κ°€ μ¤‘...")
    
    model.eval()
    model = model.to(device)
    
    total_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    # ν΄λμ¤λ³„ ν†µκ³„ (λ¶„λ¥ λ¬Έμ μΈ κ²½μ°)
    if num_classes:
        class_correct = np.zeros(num_classes)
        class_total = np.zeros(num_classes)
    
    # μ¶”λ΅  μ‹κ°„ μΈ΅μ •
    inference_times = []
    
    with torch.no_grad():
        for batch_idx, (data, targets) in enumerate(dataloader):
            data, targets = data.to(device), targets.to(device)
            
            # μ¶”λ΅  μ‹κ°„ μΈ΅μ •
            start_time = time.time()
            outputs = model(data)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            # μ†μ‹¤ κ³„μ‚°
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            
            # μ •ν™•λ„ κ³„μ‚° (λ¶„λ¥ λ¬Έμ μΈ κ²½μ°)
            if len(outputs.shape) > 1 and outputs.shape[1] > 1:
                predictions = torch.argmax(outputs, dim=1)
                correct_predictions += (predictions == targets).sum().item()
                
                # ν΄λμ¤λ³„ μ •ν™•λ„ κ³„μ‚°
                if num_classes:
                    for i in range(len(targets)):
                        label = targets[i].item()
                        class_total[label] += 1
                        if predictions[i] == targets[i]:
                            class_correct[label] += 1
            
            total_samples += len(targets)
    
    # ν‰κ·  λ©”νΈλ¦­ κ³„μ‚°
    avg_loss = total_loss / len(dataloader)
    accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0
    avg_inference_time = np.mean(inference_times)
    
    # κ²°κ³Ό λ”•μ…”λ„λ¦¬ κµ¬μ„±
    results = {
        'loss': avg_loss,
        'accuracy': accuracy,
        'total_samples': total_samples,
        'avg_inference_time': avg_inference_time,
        'throughput': total_samples / sum(inference_times)  # μ΄λ‹Ή μ²λ¦¬ μƒν” μ
    }
    
    # κ²°κ³Ό μ¶λ ¥
    print(f"\nπ“ ν‰κ°€ κ²°κ³Ό:")
    print(f"  - ν‰κ·  μ†μ‹¤: {avg_loss:.6f}")
    print(f"  - μ •ν™•λ„: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"  - μ΄ μƒν” μ: {total_samples:,}κ°")
    print(f"  - ν‰κ·  μ¶”λ΅  μ‹κ°„: {avg_inference_time*1000:.2f} ms/λ°°μΉ")
    print(f"  - μ²λ¦¬λ‰: {results['throughput']:.1f} μƒν”/μ΄")
    
    # ν΄λμ¤λ³„ μ„±λ¥ λ¶„μ„
    if num_classes and class_total.sum() > 0:
        print(f"\nπ“‹ ν΄λμ¤λ³„ μ •ν™•λ„:")
        class_accuracies = []
        
        for i in range(num_classes):
            if class_total[i] > 0:
                class_acc = class_correct[i] / class_total[i]
                class_accuracies.append(class_acc)
                print(f"  - ν΄λμ¤ {i}: {class_acc:.4f} ({class_acc*100:.2f}%) - {int(class_total[i])}κ° μƒν”")
            else:
                class_accuracies.append(0.0)
        
        # ν΄λμ¤ κ°„ μ„±λ¥ νΈμ°¨ λ¶„μ„
        if len(class_accuracies) > 1:
            acc_std = np.std(class_accuracies)
            acc_min = min(class_accuracies)
            acc_max = max(class_accuracies)
            
            results['class_accuracies'] = class_accuracies
            results['accuracy_std'] = acc_std
            results['accuracy_range'] = acc_max - acc_min
            
            print(f"\nπ“ ν΄λμ¤ μ„±λ¥ λ¶„μ„:")
            print(f"  - μ„±λ¥ ν‘μ¤€νΈμ°¨: {acc_std:.4f}")
            print(f"  - μ„±λ¥ λ²”μ„: {acc_min:.4f} ~ {acc_max:.4f}")
            
            if acc_std > 0.1:
                print(f"  β οΈ  ν΄λμ¤ κ°„ μ„±λ¥ νΈμ°¨κ°€ ν½λ‹λ‹¤")
                print(f"     -> λ°μ΄ν„° λ¶κ· ν•μ΄λ‚ λ¨λΈ νΈν–¥μ„± ν™•μΈ ν•„μ”")
    
    # μ„±λ¥ λ¶„μ„ λ° κ°μ„  μ μ•
    print(f"\nπ’΅ μ„±λ¥ λ¶„μ„ λ° μ μ•:")
    
    if accuracy < 0.6:
        print(f"  π“ μ„±λ¥ κ°μ„  ν•„μ”:")
        print(f"     - λ¨λΈ μ•„ν‚¤ν…μ² λ³µμ΅λ„ μ¦κ°€")
        print(f"     - ν•μ΄νΌνλΌλ―Έν„° νλ‹")
        print(f"     - λ°μ΄ν„° μ „μ²λ¦¬ κ°μ„ ")
        print(f"     - ν›λ ¨ μ—ν¬ν¬ μ¦κ°€")
    elif accuracy > 0.95:
        print(f"  π― μ°μν• μ„±λ¥:")
        print(f"     - κ³Όμ ν•© μ—¬λ¶€ ν™•μΈ")
        print(f"     - λ” μ–΄λ ¤μ΄ λ°μ΄ν„°μ…‹μΌλ΅ μΌλ°ν™” ν…μ¤νΈ")
    
    if avg_inference_time > 0.1:  # 100ms μ΄μƒ
        print(f"  β΅ μ¶”λ΅  μ†λ„ κ°μ„  κ³ λ ¤:")
        print(f"     - λ¨λΈ κ²½λ‰ν™” (pruning, quantization)")
        print(f"     - λ°°μΉ ν¬κΈ° μµμ ν™”")
        print(f"     - GPU ν™μ© μµμ ν™”")
    
    return results


def get_model_summary(
    model: nn.Module,
    input_size: Tuple[int, ...],
    device: str = 'cpu'
) -> Dict[str, Any]:
    """
    λ¨λΈμ κµ¬μ΅°μ™€ μ •λ³΄λ¥Ό μ”μ•½ν•©λ‹λ‹¤.
    
    Args:
        model: λ¶„μ„ν•  λ¨λΈ
        input_size: μ…λ ¥ ν…μ„ ν¬κΈ° (λ°°μΉ μ°¨μ› μ μ™Έ)
        device: μ—°μ‚° μ¥μΉ
    
    Returns:
        Dict: λ¨λΈ μ”μ•½ μ •λ³΄
    
    κµμ΅μ  λ©μ :
    - λ¨λΈ κµ¬μ΅°μ μ „μ²΄μ μΈ μ΄ν•΄
    - κ° λ μ΄μ–΄μ μ¶λ ¥ ν¬κΈ° λ° νλΌλ―Έν„° μ νμ•…
    - λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ° μ—°μ‚°λ‰ μ¶”μ •
    - λ¨λΈ μ„¤κ³„μ μ μ μ„± κ²€μ¦
    """
    
    print(f"π“‹ λ¨λΈ κµ¬μ΅° λ¶„μ„ μ¤‘...")
    
    model = model.to(device)
    model.eval()
    
    # λ”λ―Έ μ…λ ¥ μƒμ„±
    dummy_input = torch.randn(1, *input_size).to(device)
    
    # λ μ΄μ–΄λ³„ μ •λ³΄ μμ§‘
    layer_info = []
    hooks = []
    
    def register_hook(module, name):
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                output_shape = list(output.shape)
                num_params = sum(p.numel() for p in module.parameters())
            elif isinstance(output, (list, tuple)):
                output_shape = [list(o.shape) if isinstance(o, torch.Tensor) else str(type(o)) for o in output]
                num_params = sum(p.numel() for p in module.parameters())
            else:
                output_shape = str(type(output))
                num_params = sum(p.numel() for p in module.parameters())
            
            layer_info.append({
                'name': name,
                'type': module.__class__.__name__,
                'output_shape': output_shape,
                'num_params': num_params
            })
        
        return hook
    
    # λ¨λ“  λ¨λ“μ— ν›… λ“±λ΅
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # λ¦¬ν”„ λ¨λ“λ§
            hook = module.register_forward_hook(register_hook(module, name))
            hooks.append(hook)
    
    # μμ „ν μ‹¤ν–‰
    try:
        with torch.no_grad():
            _ = model(dummy_input)
    finally:
        # ν›… μ κ±°
        for hook in hooks:
            hook.remove()
    
    # μ „μ²΄ νλΌλ―Έν„° μ κ³„μ‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”μ •
    param_memory = total_params * 4 / (1024**2)  # MB, float32 κΈ°μ¤€
    
    # κ²°κ³Ό κµ¬μ„±
    summary = {
        'model_name': model.__class__.__name__,
        'input_size': input_size,
        'total_params': total_params,
        'trainable_params': trainable_params,
        'param_memory_mb': param_memory,
        'layer_info': layer_info
    }
    
    # κ²°κ³Ό μ¶λ ¥
    print(f"\nπ“ λ¨λΈ μ”μ•½:")
    print(f"  - λ¨λΈλ…: {summary['model_name']}")
    print(f"  - μ…λ ¥ ν¬κΈ°: {input_size}")
    print(f"  - μ „μ²΄ νλΌλ―Έν„°: {total_params:,}κ°")
    print(f"  - ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°: {trainable_params:,}κ°")
    print(f"  - νλΌλ―Έν„° λ©”λ¨λ¦¬: {param_memory:.2f} MB")
    
    print(f"\nπ“‹ λ μ΄μ–΄λ³„ κµ¬μ΅°:")
    print(f"{'λ μ΄μ–΄λ…':<25} {'νƒ€μ…':<15} {'μ¶λ ¥ ν¬κΈ°':<25} {'νλΌλ―Έν„°'}")
    print("-" * 85)
    
    for info in layer_info:
        if info['name']:  # λΉ μ΄λ¦„ μ μ™Έ
            output_str = str(info['output_shape'])[:23]
            print(f"{info['name']:<25} {info['type']:<15} {output_str:<25} {info['num_params']:,}")
    
    return summary


def compare_models(
    models: Dict[str, nn.Module],
    test_dataloader: torch.utils.data.DataLoader,
    criterion: nn.Module,
    device: str = 'cpu'
) -> Dict[str, Dict[str, float]]:
    """
    μ—¬λ¬ λ¨λΈμ μ„±λ¥μ„ λΉ„κµν•©λ‹λ‹¤.
    
    Args:
        models: λΉ„κµν•  λ¨λΈλ“¤ (μ΄λ¦„: λ¨λΈ)
        test_dataloader: ν…μ¤νΈ λ°μ΄ν„° λ΅λ”
        criterion: μ†μ‹¤ ν•¨μ
        device: μ—°μ‚° μ¥μΉ
    
    Returns:
        Dict: λ¨λΈλ³„ μ„±λ¥ λΉ„κµ κ²°κ³Ό
    
    κµμ΅μ  λ©μ :
    - λ‹¤μ–‘ν• λ¨λΈ μ•„ν‚¤ν…μ²μ μ„±λ¥ λΉ„κµ
    - λ¨λΈ μ„ νƒμ„ μ„ν• κ°κ΄€μ  κΈ°μ¤€ μ κ³µ
    - μ„±λ¥-λ³µμ΅λ„ νΈλ μ΄λ“μ¤ν”„ λ¶„μ„
    - μ•™μƒλΈ” λ¨λΈ κµ¬μ„±μ„ μ„ν• κΈ°μ΄ μλ£
    """
    
    print(f"π† λ¨λΈ μ„±λ¥ λΉ„κµ μ¤‘... ({len(models)}κ° λ¨λΈ)")
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\nπ“ {model_name} ν‰κ°€ μ¤‘...")
        
        # λ¨λΈ ν‰κ°€
        model_results = evaluate_model(model, test_dataloader, criterion, device)
        
        # λ¨λΈ λ³µμ΅λ„ μ •λ³΄ μ¶”κ°€
        param_info = count_parameters(model, detailed=False)
        model_results.update({
            'total_params': param_info['total_params'],
            'trainable_params': param_info['trainable_params']
        })
        
        results[model_name] = model_results
    
    # λΉ„κµ κ²°κ³Ό μ¶λ ¥
    print(f"\nπ† λ¨λΈ μ„±λ¥ λΉ„κµ κ²°κ³Ό:")
    print(f"{'λ¨λΈλ…':<20} {'μ •ν™•λ„':<10} {'μ†μ‹¤':<12} {'νλΌλ―Έν„° μ':<15} {'μ¶”λ΅ μ‹κ°„(ms)'}")
    print("-" * 70)
    
    # μ„±λ¥μμΌλ΅ μ •λ ¬
    sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
    
    for model_name, result in sorted_models:
        accuracy = result['accuracy']
        loss = result['loss']
        params = result['total_params']
        inference_time = result['avg_inference_time'] * 1000  # ms
        
        print(f"{model_name:<20} {accuracy:<10.4f} {loss:<12.6f} {params:<15,} {inference_time:<10.2f}")
    
    # μµκ³  μ„±λ¥ λ¨λΈ μ‹λ³„
    best_model = sorted_models[0]
    print(f"\nπ¥‡ μµκ³  μ„±λ¥ λ¨λΈ: {best_model[0]} (μ •ν™•λ„: {best_model[1]['accuracy']:.4f})")
    
    # ν¨μ¨μ„± λ¶„μ„ (μ •ν™•λ„ λ€λΉ„ νλΌλ―Έν„° μ)
    print(f"\nπ“ ν¨μ¨μ„± λ¶„μ„ (μ •ν™•λ„/νλΌλ―Έν„° λΉ„μ¨):")
    efficiency_scores = []
    
    for model_name, result in results.items():
        efficiency = result['accuracy'] / (result['total_params'] / 1e6)  # μ •ν™•λ„ per million params
        efficiency_scores.append((model_name, efficiency))
    
    efficiency_scores.sort(key=lambda x: x[1], reverse=True)
    
    for model_name, efficiency in efficiency_scores:
        print(f"  - {model_name}: {efficiency:.2f} (μ •ν™•λ„/MνλΌλ―Έν„°)")
    
    print(f"\nπ― κ°€μ¥ ν¨μ¨μ μΈ λ¨λΈ: {efficiency_scores[0][0]}")
    
    return results