#!/usr/bin/env python3
"""
Transformer í€´ì¦ˆ

ì´ í€´ì¦ˆëŠ” ë‹¤ìŒ ë‚´ìš©ì„ í‰ê°€í•©ë‹ˆë‹¤:
1. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ë°œì „ì‚¬ (Bahdanau â†’ Luong â†’ Self-Attention)
2. Self-Attention vs Cross-Attention ì°¨ì´ì 
3. Multi-Head Attention ê³„ì‚° ê³¼ì •
4. ìœ„ì¹˜ ì¸ì½”ë”© í•„ìš”ì„± ë° êµ¬í˜„ ë°©ë²•
5. BERT, GPT ë“± Transformer ê¸°ë°˜ ëª¨ë¸ íŠ¹ì„±

ìš”êµ¬ì‚¬í•­ 8.1, 8.2, 8.3, 7.1, 7.2, 7.3, 7.4, 7.5ë¥¼ ì¶©ì¡±í•©ë‹ˆë‹¤.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

from quiz_utils import DeepLearningQuizManager, QuizQuestion
import math


class TransformerQuiz:
    def __init__(self):
        self.quiz_manager = DeepLearningQuizManager("Transformer í€´ì¦ˆ")
        self.setup_questions()
    
    def setup_questions(self):
        """í€´ì¦ˆ ë¬¸ì œ ì„¤ì •"""
        
        # 1. ì „í†µì  seq2seq í•œê³„ - Easy
        self.quiz_manager.add_question_simple(
            question_id="transformer_001",
            question_type="multiple_choice",
            question="ì „í†µì ì¸ seq2seq ëª¨ë¸ì˜ ì£¼ìš” í•œê³„ëŠ”?",
            options=[
                "ê³„ì‚° ì†ë„ê°€ ë„ˆë¬´ ë¹ ë¦„",
                "ëª¨ë“  ì…ë ¥ ì •ë³´ë¥¼ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ì••ì¶•í•˜ëŠ” ì •ë³´ ë³‘ëª©",
                "GPU ë©”ëª¨ë¦¬ë¥¼ ë„ˆë¬´ ë§ì´ ì‚¬ìš©í•¨",
                "ì˜¤ì§ ì˜ì–´ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•¨"
            ],
            correct_answer=2,
            explanation="""
ì •ë‹µ: 2) ëª¨ë“  ì…ë ¥ ì •ë³´ë¥¼ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ì••ì¶•í•˜ëŠ” ì •ë³´ ë³‘ëª©

í•´ì„¤:
ì „í†µì ì¸ seq2seq ëª¨ë¸ì€ ì¸ì½”ë”ì˜ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ê³ ì • í¬ê¸° ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¡œ 
ì••ì¶•í•´ì•¼ í•˜ë¯€ë¡œ, ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì´ˆê¸° ì •ë³´ê°€ ì†ì‹¤ë˜ëŠ” ì •ë³´ ë³‘ëª© í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤.
ì´ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ í•´ê²°í•˜ê³ ì í•œ í•µì‹¬ ë¬¸ì œì…ë‹ˆë‹¤.
            """,
            difficulty="easy",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ë°œì „ì‚¬"
        )
        
        # 2. Bahdanau Attention ì´í•´ - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_002",
            question_type="multiple_choice",
            question="Bahdanau Attention (2014)ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ”?",
            options=[
                "RNNì„ ì™„ì „íˆ ì œê±°í•¨",
                "ë””ì½”ë”ê°€ ê° ì‹œì ì—ì„œ ì¸ì½”ë”ì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•¨",
                "GPU ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨",
                "ìœ„ì¹˜ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•¨"
            ],
            correct_answer=2,
            explanation="""
ì •ë‹µ: 2) ë””ì½”ë”ê°€ ê° ì‹œì ì—ì„œ ì¸ì½”ë”ì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•¨

í•´ì„¤:
Bahdanau Attentionì€ ë””ì½”ë”ì˜ ê° ì‹œì ì—ì„œ ì¸ì½”ë”ì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœì— 
ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
ì´ë¥¼ í†µí•´ ì •ë³´ ë³‘ëª© ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Bahdanau Attention"
        )
        
        # 3. Luong vs Bahdanau - Hard
        self.quiz_manager.add_question_simple(
            question_id="transformer_003",
            question_type="multiple_choice",
            question="Luong Attention (2015)ì´ Bahdanau Attention ëŒ€ë¹„ ê°œì„ í•œ ì ì€?",
            options=[
                "ì™„ì „íˆ ë‹¤ë¥¸ ìˆ˜ì‹ ì‚¬ìš©",
                "Global/Local Attention êµ¬ë¶„ ë° ë‹¤ì–‘í•œ ìŠ¤ì½”ì–´ í•¨ìˆ˜ ì œì•ˆ",
                "RNN êµ¬ì¡° ì™„ì „ ì œê±°",
                "Self-Attention ë„ì…"
            ],
            correct_answer=2,
            explanation="""
ì •ë‹µ: 2) Global/Local Attention êµ¬ë¶„ ë° ë‹¤ì–‘í•œ ìŠ¤ì½”ì–´ í•¨ìˆ˜ ì œì•ˆ

í•´ì„¤:
Luong Attentionì€ Global Attention(ëª¨ë“  ìœ„ì¹˜ ì°¸ì¡°)ê³¼ Local Attention(íŠ¹ì • ìœˆë„ìš°ë§Œ ì°¸ì¡°)ì„ 
êµ¬ë¶„í•˜ê³ , dot, general, concat ë“± ë‹¤ì–‘í•œ ìŠ¤ì½”ì–´ í•¨ìˆ˜ë¥¼ ì œì•ˆí•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
            """,
            difficulty="hard",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Luong Attention"
        )
        
        # 4. Self-Attention í˜ì‹  - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_004",
            question_type="short_answer",
            question="Self-Attentionì´ ê¸°ì¡´ ì–´í…ì…˜ê³¼ ë‹¤ë¥¸ í•µì‹¬ì ì¸ ì°¨ì´ì ì€?",
            correct_answer="ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ê´€ê³„ë¥¼ í•™ìŠµ",
            explanation="""
ì •ë‹µ: ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ê´€ê³„ë¥¼ í•™ìŠµ

í•´ì„¤:
ê¸°ì¡´ ì–´í…ì…˜ì€ ì¸ì½”ë”-ë””ì½”ë” ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí–ˆì§€ë§Œ, 
Self-Attentionì€ ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ í† í°ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ ë¬¸ë§¥ì  í‘œí˜„ì„ ë” í’ë¶€í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Self-Attention"
        )
        
        # 5. Cross-Attention ì—°ê´€ì„± - Hard
        self.quiz_manager.add_question_simple(
            question_id="transformer_005",
            question_type="true_false",
            question="Transformerì˜ Cross-Attentionì€ ì „í†µì ì¸ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ì˜ ì¼ë°˜í™”ëœ í˜•íƒœì´ë‹¤.",
            correct_answer=True,
            explanation="""
ì •ë‹µ: ì°¸ (True)

í•´ì„¤:
Cross-Attentionì€ Queryê°€ ë””ì½”ë”ì—ì„œ, Key/Valueê°€ ì¸ì½”ë”ì—ì„œ ì˜¤ëŠ” êµ¬ì¡°ë¡œ,
ì „í†µì ì¸ ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ê³¼ ë³¸ì§ˆì ìœ¼ë¡œ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤.
ë‹¤ë§Œ Self-Attentionì„ ê±°ì¹œ ë” í’ë¶€í•œ í‘œí˜„ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ ë‹¤ë¦…ë‹ˆë‹¤.
            """,
            difficulty="hard",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Cross-Attention"
        )
        
        # 6. Scaled Dot-Product Attention ìˆ˜ì‹ - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_006",
            question_type="multiple_choice",
            question="Scaled Dot-Product Attentionì—ì„œ âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ”?",
            options=[
                "ê³„ì‚° ì†ë„ í–¥ìƒ",
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ",
                "ë‚´ì  ê°’ì´ ì»¤ì ¸ì„œ softmaxê°€ í¬í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€",
                "ìŒìˆ˜ ê°’ì„ ì–‘ìˆ˜ë¡œ ë³€í™˜"
            ],
            correct_answer=3,
            explanation="""
ì •ë‹µ: 3) ë‚´ì  ê°’ì´ ì»¤ì ¸ì„œ softmaxê°€ í¬í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€

í•´ì„¤:
d_kê°€ í´ ë•Œ Qì™€ Kì˜ ë‚´ì  ê°’ì´ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆê³ , 
ì´ëŠ” softmax í•¨ìˆ˜ë¥¼ í¬í™” ì˜ì—­ìœ¼ë¡œ ë°€ì–´ë„£ì–´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë§¤ìš° ì‘ì•„ì§€ê²Œ í•©ë‹ˆë‹¤.
âˆšd_kë¡œ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ì´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Self-Attention ë©”ì»¤ë‹ˆì¦˜"
        )
        
        # 7. Multi-Head Attention ê³„ì‚° - Hard
        self.quiz_manager.add_question_simple(
            question_id="transformer_007",
            question_type="calculation",
            question="d_model=512, num_heads=8ì¼ ë•Œ, ê° í—¤ë“œì˜ d_k ì°¨ì›ì€?",
            correct_answer=64,
            explanation="""
ì •ë‹µ: 64

í•´ì„¤:
Multi-Head Attentionì—ì„œ ê° í—¤ë“œì˜ ì°¨ì›ì€:
d_k = d_model / num_heads = 512 / 8 = 64

ì´ë ‡ê²Œ ì°¨ì›ì„ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ í—¤ë“œê°€ ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ ê³µê°„ì—ì„œ 
ì–´í…ì…˜ì„ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
            """,
            difficulty="hard",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Multi-Head Attention",
            formula="d_k = d_model / num_heads",
            tolerance=0
        )
        
        # 8. ìœ„ì¹˜ ì¸ì½”ë”© í•„ìš”ì„± - Easy
        self.quiz_manager.add_question_simple(
            question_id="transformer_008",
            question_type="multiple_choice",
            question="Transformerì—ì„œ ìœ„ì¹˜ ì¸ì½”ë”©ì´ í•„ìš”í•œ ì´ìœ ëŠ”?",
            options=[
                "ê³„ì‚° ì†ë„ í–¥ìƒì„ ìœ„í•´",
                "Self-Attentionì€ ìˆœì„œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸",
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œë¥¼ ìœ„í•´",
                "GPU ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´"
            ],
            correct_answer=2,
            explanation="""
ì •ë‹µ: 2) Self-Attentionì€ ìˆœì„œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸

í•´ì„¤:
Self-Attentionì€ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ì°¸ì¡°í•˜ë¯€ë¡œ í† í°ì˜ ìˆœì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.
ë”°ë¼ì„œ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ì¶”ê°€í•˜ì—¬ ê° í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
            """,
            difficulty="easy",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ìœ„ì¹˜ ì¸ì½”ë”©"
        )
        
        # 9. ì‚¬ì¸/ì½”ì‚¬ì¸ ìœ„ì¹˜ ì¸ì½”ë”© - Hard
        self.quiz_manager.add_question_simple(
            question_id="transformer_009",
            question_type="multiple_choice",
            question="ì‚¬ì¸/ì½”ì‚¬ì¸ ìœ„ì¹˜ ì¸ì½”ë”©ì˜ ì¥ì ì€?",
            options=[
                "í•™ìŠµì´ í•„ìš” ì—†ìŒ",
                "í›ˆë ¨ ì‹œë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì—ë„ ì ìš© ê°€ëŠ¥",
                "ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„ í•™ìŠµ ê°€ëŠ¥",
                "ìœ„ì˜ ëª¨ë“  ê²ƒ"
            ],
            correct_answer=4,
            explanation="""
ì •ë‹µ: 4) ìœ„ì˜ ëª¨ë“  ê²ƒ

í•´ì„¤:
ì‚¬ì¸/ì½”ì‚¬ì¸ ìœ„ì¹˜ ì¸ì½”ë”©ì€ ê³ ì •ëœ ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ë¯€ë¡œ í•™ìŠµì´ ë¶ˆí•„ìš”í•˜ê³ ,
ì£¼ê¸°ì  íŒ¨í„´ìœ¼ë¡œ ì¸í•´ í›ˆë ¨ ì‹œë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë©°,
ì‚¼ê°í•¨ìˆ˜ì˜ ì„±ì§ˆë¡œ ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„ë„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """,
            difficulty="hard",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ìœ„ì¹˜ ì¸ì½”ë”©"
        )
        
        # 10. Masked Self-Attention - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_010",
            question_type="multiple_choice",
            question="ë””ì½”ë”ì˜ Masked Self-Attentionì—ì„œ ë§ˆìŠ¤í‚¹í•˜ëŠ” ì´ìœ ëŠ”?",
            options=[
                "ê³„ì‚° ì†ë„ í–¥ìƒ",
                "ë©”ëª¨ë¦¬ ì ˆì•½",
                "ë¯¸ë˜ í† í°ì„ ì°¸ì¡°í•˜ì§€ ëª»í•˜ê²Œ í•˜ì—¬ ìê¸°íšŒê·€ì  ìƒì„± ë³´ì¥",
                "ë…¸ì´ì¦ˆ ì œê±°"
            ],
            correct_answer=3,
            explanation="""
ì •ë‹µ: 3) ë¯¸ë˜ í† í°ì„ ì°¸ì¡°í•˜ì§€ ëª»í•˜ê²Œ í•˜ì—¬ ìê¸°íšŒê·€ì  ìƒì„± ë³´ì¥

í•´ì„¤:
ë””ì½”ë”ëŠ” ìˆœì°¨ì ìœ¼ë¡œ í† í°ì„ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ, í˜„ì¬ ìœ„ì¹˜ì—ì„œ 
ë¯¸ë˜ ìœ„ì¹˜ì˜ í† í°ì„ ì°¸ì¡°í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ì„ í†µí•´ ì´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ë””ì½”ë” êµ¬ì¡°"
        )
        
        # 11. Layer Normalization - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_011",
            question_type="true_false",
            question="Transformerì—ì„œ Layer Normalizationì€ Residual Connection ì´í›„ì— ì ìš©ëœë‹¤.",
            correct_answer=True,
            explanation="""
ì •ë‹µ: ì°¸ (True)

í•´ì„¤:
Transformerì˜ ê° ì„œë¸Œë ˆì´ì–´ëŠ” LayerNorm(x + Sublayer(x)) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì¦‰, ì”ì°¨ ì—°ê²°(x + Sublayer(x)) í›„ì— Layer Normalizationì„ ì ìš©í•©ë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ì•„í‚¤í…ì²˜ ìƒì„¸ ë¶„ì„"
        )
        
        # 12. BERT vs GPT - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_012",
            question_type="multiple_choice",
            question="BERTì™€ GPTì˜ ì£¼ìš” ì°¨ì´ì ì€?",
            options=[
                "BERTëŠ” ì¸ì½”ë”ë§Œ, GPTëŠ” ë””ì½”ë”ë§Œ ì‚¬ìš©",
                "BERTëŠ” ì–‘ë°©í–¥, GPTëŠ” ë‹¨ë°©í–¥ ì–´í…ì…˜",
                "BERTëŠ” MLM, GPTëŠ” ìê¸°íšŒê·€ í•™ìŠµ",
                "ìœ„ì˜ ëª¨ë“  ê²ƒ"
            ],
            correct_answer=4,
            explanation="""
ì •ë‹µ: 4) ìœ„ì˜ ëª¨ë“  ê²ƒ

í•´ì„¤:
BERTëŠ” ì¸ì½”ë” êµ¬ì¡°ë¡œ ì–‘ë°©í–¥ ì–´í…ì…˜ê³¼ MLM(Masked Language Model)ì„ ì‚¬ìš©í•˜ê³ ,
GPTëŠ” ë””ì½”ë” êµ¬ì¡°ë¡œ ë‹¨ë°©í–¥ ì–´í…ì…˜ê³¼ ìê¸°íšŒê·€ì  ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Transformer ê¸°ë°˜ ëª¨ë¸ë“¤"
        )
        
        # 13. ì–´í…ì…˜ ë³µì¡ë„ - Hard
        self.quiz_manager.add_question_simple(
            question_id="transformer_013",
            question_type="multiple_choice",
            question="Self-Attentionì˜ ì‹œê°„ ë³µì¡ë„ëŠ”?",
            options=[
                "O(n)",
                "O(n log n)",
                "O(nÂ²)",
                "O(nÂ³)"
            ],
            correct_answer=3,
            explanation="""
ì •ë‹µ: 3) O(nÂ²)

í•´ì„¤:
Self-Attentionì€ ê¸¸ì´ nì¸ ì‹œí€€ìŠ¤ì—ì„œ ëª¨ë“  ìœ„ì¹˜ ìŒ ê°„ì˜ ì–´í…ì…˜ì„ ê³„ì‚°í•˜ë¯€ë¡œ
ì‹œê°„ ë³µì¡ë„ê°€ O(nÂ²)ì…ë‹ˆë‹¤. ì´ëŠ” ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë¹„ìš©ì´ 
ì œê³±ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
            """,
            difficulty="hard",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ì‹¤ìš©ì  ê³ ë ¤ì‚¬í•­"
        )
        
        # 14. Transformer ì¥ì  - Easy
        self.quiz_manager.add_question_simple(
            question_id="transformer_014",
            question_type="multiple_choice",
            question="Transformerì˜ ì£¼ìš” ì¥ì ì´ ì•„ë‹Œ ê²ƒì€?",
            options=[
                "ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥",
                "ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ",
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ",
                "í•´ì„ ê°€ëŠ¥ì„±"
            ],
            correct_answer=3,
            explanation="""
ì •ë‹µ: 3) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ

í•´ì„¤:
TransformerëŠ” O(nÂ²) ë³µì¡ë„ë¡œ ì¸í•´ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ìŠµë‹ˆë‹¤.
ë³‘ë ¬ ì²˜ë¦¬, ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ, ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ í†µí•œ í•´ì„ ê°€ëŠ¥ì„±ì€ ì¥ì ì…ë‹ˆë‹¤.
            """,
            difficulty="easy",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ì‹¤ìš©ì  ê³ ë ¤ì‚¬í•­"
        )
        
        # 15. ì–´í…ì…˜ ë°œì „ì‚¬ ì¢…í•© - Hard
        self.quiz_manager.add_question_simple(
            question_id="transformer_015",
            question_type="multiple_choice",
            question="ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë°œì „ ìˆœì„œë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?",
            options=[
                "Self-Attention â†’ Bahdanau â†’ Luong â†’ Cross-Attention",
                "Bahdanau â†’ Self-Attention â†’ Luong â†’ Cross-Attention",
                "Bahdanau â†’ Luong â†’ Self-Attention â†’ Cross-Attention",
                "Luong â†’ Bahdanau â†’ Self-Attention â†’ Cross-Attention"
            ],
            correct_answer=3,
            explanation="""
ì •ë‹µ: 3) Bahdanau â†’ Luong â†’ Self-Attention â†’ Cross-Attention

í•´ì„¤:
ì–´í…ì…˜ ë°œì „ì‚¬:
1. Bahdanau (2014): ìµœì´ˆ ì–´í…ì…˜, ì •ë³´ ë³‘ëª© í•´ê²°
2. Luong (2015): íš¨ìœ¨ì„± ê°œì„ , ë‹¤ì–‘í•œ ìŠ¤ì½”ì–´ í•¨ìˆ˜
3. Self-Attention (2017): ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ë¶€ ê´€ê³„ í•™ìŠµ
4. Cross-Attention: Transformerì—ì„œ ì¸ì½”ë”-ë””ì½”ë” ê°„ ì–´í…ì…˜
            """,
            difficulty="hard",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ë°œì „ì‚¬"
        )
        
        # 16. T5 ëª¨ë¸ íŠ¹ì§• - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_016",
            question_type="short_answer",
            question="T5 ëª¨ë¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•˜ë©´?",
            correct_answer="Text-to-Text",
            explanation="""
ì •ë‹µ: Text-to-Text

í•´ì„¤:
T5(Text-to-Text Transfer Transformer)ëŠ” ëª¨ë“  NLP ì‘ì—…ì„ 
í…ìŠ¤íŠ¸ ì…ë ¥ì„ í…ìŠ¤íŠ¸ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µì¼ëœ í”„ë ˆì„ì›Œí¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - T5"
        )
        
        # 17. Vision Transformer - Medium
        self.quiz_manager.add_question_simple(
            question_id="transformer_017",
            question_type="multiple_choice",
            question="Vision Transformer(ViT)ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€?",
            options=[
                "í”½ì…€ ë‹¨ìœ„ë¡œ ì²˜ë¦¬",
                "ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬",
                "CNNê³¼ ê²°í•©í•˜ì—¬ ì²˜ë¦¬",
                "ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬"
            ],
            correct_answer=2,
            explanation="""
ì •ë‹µ: 2) ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬

í•´ì„¤:
ViTëŠ” ì´ë¯¸ì§€ë¥¼ ê³ ì • í¬ê¸° íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê³ , ê° íŒ¨ì¹˜ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬
ì‹œí€€ìŠ¤ë¡œ ë§Œë“  í›„ í‘œì¤€ Transformer êµ¬ì¡°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            """,
            difficulty="medium",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - Vision Transformer"
        )
        
        # 18. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í•´ì„ - Easy
        self.quiz_manager.add_question_simple(
            question_id="transformer_018",
            question_type="true_false",
            question="ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ë©´ ëª¨ë¸ì´ ì–´ë–¤ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.",
            correct_answer=True,
            explanation="""
ì •ë‹µ: ì°¸ (True)

í•´ì„¤:
ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” ê° í† í°ì´ ë‹¤ë¥¸ í† í°ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë¯€ë¡œ,
ì´ë¥¼ ì‹œê°í™”í•˜ë©´ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ì–´ëŠ ì •ë„ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """,
            difficulty="easy",
            topic="Transformer",
            related_theory_section="Transformer ì´ë¡  - í•´ì„ ê°€ëŠ¥ì„±"
        )
    
    def run_quiz(self, num_questions: int = 18, difficulty: str = None):
        """í€´ì¦ˆ ì‹¤í–‰"""
        print("ğŸ¤– Transformer í€´ì¦ˆì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print("ì´ í€´ì¦ˆëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë°œì „ì‚¬ì™€ Transformer êµ¬ì¡°ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.")
        print("-" * 70)
        
        results = self.quiz_manager.run_full_quiz(
            topic="Transformer",
            difficulty=difficulty,
            num_questions=num_questions
        )
        
        return results
    
    def run_retry_quiz(self):
        """í‹€ë¦° ë¬¸ì œ ì¬ì‹œë„"""
        return self.quiz_manager.retry_wrong_questions()
    
    def run_attention_history_quiz(self):
        """ì–´í…ì…˜ ë°œì „ì‚¬ íŠ¹í™” í€´ì¦ˆ"""
        print("ğŸ“š ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ë°œì „ì‚¬ íŠ¹í™” í€´ì¦ˆ")
        print("Bahdanau â†’ Luong â†’ Self-Attention â†’ Cross-Attention")
        print("-" * 60)
        
        # ì–´í…ì…˜ ë°œì „ì‚¬ ê´€ë ¨ ë¬¸ì œë§Œ ì„ ë³„
        attention_history_questions = [
            "transformer_001", "transformer_002", "transformer_003", 
            "transformer_004", "transformer_005", "transformer_015"
        ]
        
        selected_questions = [self.quiz_manager.questions[qid] for qid in attention_history_questions 
                            if qid in self.quiz_manager.questions]
        
        if not selected_questions:
            print("ì–´í…ì…˜ ë°œì „ì‚¬ ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        self.quiz_manager.current_session = selected_questions
        self.quiz_manager.session_results = []
        self.quiz_manager.start_time = datetime.now()
        
        for i, question in enumerate(selected_questions, 1):
            print(f"\n{'='*15} ë¬¸ì œ {i}/{len(selected_questions)} {'='*15}")
            result = self.quiz_manager.ask_question(question)
            if result:
                self.quiz_manager.session_results.append(result)
            
            if i < len(selected_questions):
                input("\në‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°€ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        self.quiz_manager.show_final_results()
        return self.quiz_manager.session_results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    quiz = TransformerQuiz()
    
    print("Transformer í€´ì¦ˆ ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("1. ì „ì²´ í€´ì¦ˆ (18ë¬¸ì œ)")
    print("2. ì–´í…ì…˜ ë°œì „ì‚¬ íŠ¹í™” í€´ì¦ˆ")
    print("3. ì‰¬ìš´ ë¬¸ì œë§Œ (Easy)")
    print("4. ë³´í†µ ë¬¸ì œë§Œ (Medium)")
    print("5. ì–´ë ¤ìš´ ë¬¸ì œë§Œ (Hard)")
    print("6. ë§ì¶¤í˜• í€´ì¦ˆ")
    
    while True:
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-6, q: ì¢…ë£Œ): ").strip()
        
        if choice.lower() == 'q':
            print("í€´ì¦ˆë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!")
            break
        elif choice == '1':
            results = quiz.run_quiz()
        elif choice == '2':
            results = quiz.run_attention_history_quiz()
        elif choice == '3':
            results = quiz.run_quiz(difficulty="easy")
        elif choice == '4':
            results = quiz.run_quiz(difficulty="medium")
        elif choice == '5':
            results = quiz.run_quiz(difficulty="hard")
        elif choice == '6':
            try:
                num_q = int(input("ë¬¸ì œ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-18): "))
                if 1 <= num_q <= 18:
                    results = quiz.run_quiz(num_questions=num_q)
                else:
                    print("1-18 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                    continue
            except ValueError:
                print("ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                continue
        else:
            print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        
        # ì¬ì‹œë„ ì˜µì…˜
        if results and any(not r.is_correct for r in results):
            retry = input("\ní‹€ë¦° ë¬¸ì œë¥¼ ë‹¤ì‹œ í’€ì–´ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if retry == 'y':
                quiz.run_retry_quiz()


if __name__ == "__main__":
    main()