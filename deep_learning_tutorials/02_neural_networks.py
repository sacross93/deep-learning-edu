"""
ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 2: ì‹ ê²½ë§ ì‹¬í™”

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Fashion-MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 
ë” ë³µì¡í•œ ì‹ ê²½ë§ê³¼ ê³ ê¸‰ ê¸°ë²•ë“¤ì„ í•™ìŠµí•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
1. Fashion-MNIST ë°ì´í„°ì…‹ì˜ íŠ¹ì„± ì´í•´
2. ë” ê¹Šê³  ë³µì¡í•œ ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„
3. ë°°ì¹˜ ì •ê·œí™”(Batch Normalization) ì ìš©
4. ë‹¤ì–‘í•œ ì •ê·œí™” ê¸°ë²• ë¹„êµ
5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ êµ¬í˜„
6. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¸°ë²•

ë°ì´í„°ì…‹ ì„ íƒ ì´ìœ  - Fashion-MNIST:
- 28x28 í”½ì…€ì˜ í‘ë°± ì˜ë¥˜ ì´ë¯¸ì§€ (10ê°œ ì¹´í…Œê³ ë¦¬)
- 60,000ê°œ í›ˆë ¨ + 10,000ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
- MNISTë³´ë‹¤ ë³µì¡í•˜ì—¬ ì‹ ê²½ë§ ì„±ëŠ¥ ê°œì„  ê¸°ë²• í•™ìŠµì— ì í•©
- ì‹¤ì œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì— ë” ê°€ê¹Œìš´ ë‚œì´ë„
- í´ë˜ìŠ¤ ê°„ ì‹œê°ì  ìœ ì‚¬ì„±ìœ¼ë¡œ ë” ë„ì „ì ì¸ ë¶„ë¥˜ ë¬¸ì œ
- ì •ê·œí™” ê¸°ë²•ì˜ íš¨ê³¼ë¥¼ ëª…í™•íˆ í™•ì¸í•  ìˆ˜ ìˆìŒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import time
import copy

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.data_utils import explore_dataset, visualize_samples, create_data_split
from utils.visualization import (plot_training_curves, plot_confusion_matrix, 
                               plot_model_predictions, plot_learning_rate_schedule)
from utils.model_utils import count_parameters, save_checkpoint, evaluate_model, compare_models

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 2: ì‹ ê²½ë§ ì‹¬í™”")
print("=" * 60)

# ============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
# Fashion-MNISTëŠ” MNISTë³´ë‹¤ ë³µì¡í•˜ë¯€ë¡œ ë” ì‹ ì¤‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒì´ í•„ìš”
BATCH_SIZE = 128       # ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ ë” ì•ˆì •ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
LEARNING_RATE = 0.001  # ì´ˆê¸° í•™ìŠµë¥ 
EPOCHS = 20            # ë” ë§ì€ ì—í¬í¬ë¡œ ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í™•ë³´
RANDOM_SEED = 42
VALIDATION_SPLIT = 0.1  # í›ˆë ¨ ë°ì´í„°ì˜ 10%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)

print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   ì´ˆê¸° í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   ì—í¬í¬: {EPOCHS}")
print(f"   ê²€ì¦ ë°ì´í„° ë¹„ìœ¨: {VALIDATION_SPLIT}")

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”©
# ============================================================================

print(f"\nğŸ“ Fashion-MNIST ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

# Fashion-MNISTë¥¼ ìœ„í•œ ë°ì´í„° ì¦ê°• ê¸°ë²•
# ì™œ ë°ì´í„° ì¦ê°•ì´ í•„ìš”í•œê°€?
# 1. ì œí•œëœ ë°ì´í„°ë¡œ ë” ë§ì€ í•™ìŠµ ìƒ˜í”Œ ìƒì„±
# 2. ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
# 3. ê³¼ì í•© ë°©ì§€
# 4. ì‹¤ì œ í™˜ê²½ì˜ ë‹¤ì–‘í•œ ë³€í˜•ì— ëŒ€í•œ ê°•ê±´ì„± í™•ë³´

# í›ˆë ¨ìš© ë³€í™˜ (ë°ì´í„° ì¦ê°• í¬í•¨)
train_transform = transforms.Compose([
    # RandomHorizontalFlip: 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „
    # ì˜ë¥˜ ì´ë¯¸ì§€ì—ì„œ ì¢Œìš° ë°˜ì „ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë³€í˜•
    transforms.RandomHorizontalFlip(p=0.5),
    
    # RandomRotation: Â±10ë„ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ íšŒì „
    # ì‹¤ì œ ì°©ìš© ì‹œ ì•½ê°„ì˜ ê¸°ìš¸ì–´ì§ì„ ì‹œë®¬ë ˆì´ì…˜
    transforms.RandomRotation(degrees=10),
    
    # RandomAffine: ì•½ê°„ì˜ ì´ë™ê³¼ í™•ëŒ€/ì¶•ì†Œ
    # ì´¬ì˜ ê°ë„ë‚˜ ê±°ë¦¬ì˜ ë³€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
    
    transforms.ToTensor(),
    
    # Fashion-MNISTì˜ ì‹¤ì œ í†µê³„ê°’ìœ¼ë¡œ ì •ê·œí™”
    # ì´ ê°’ë“¤ì€ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê³„ì‚°ëœ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
    transforms.Normalize((0.2860,), (0.3530,))
])

# í…ŒìŠ¤íŠ¸ìš© ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ìŒ)
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.2860,), (0.3530,))
])

# ë°ì´í„°ì…‹ ë¡œë“œ
train_dataset = torchvision.datasets.FashionMNIST(
    root='./data', 
    train=True, 
    download=True, 
    transform=train_transform
)

test_dataset = torchvision.datasets.FashionMNIST(
    root='./data', 
    train=False, 
    download=True, 
    transform=test_transform
)

print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")

# ============================================================================
# 3. ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
# ============================================================================

print(f"\nğŸ” Fashion-MNIST ë°ì´í„°ì…‹ íƒìƒ‰")

# Fashion-MNIST í´ë˜ìŠ¤ ì´ë¦„
class_names = [
    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
]

# ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ í™•ì¸
explore_dataset(train_dataset, "Fashion-MNIST í›ˆë ¨ ë°ì´í„°ì…‹", show_samples=3)

# ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”
print(f"\nğŸ–¼ï¸  Fashion-MNIST ìƒ˜í”Œ ì‹œê°í™”")
visualize_samples(
    train_dataset, 
    num_samples=20, 
    num_cols=5,
    title="Fashion-MNIST í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ",
    class_names=class_names
)

# Fashion-MNIST vs MNIST ë³µì¡ë„ ë¹„êµ ì„¤ëª…
print(f"\nğŸ“ˆ Fashion-MNIST vs MNIST ë³µì¡ë„ ë¹„êµ:")
print(f"   1. í´ë˜ìŠ¤ ë‚´ ë³€ì´ì„±: ê°™ì€ ì˜ë¥˜ ì¢…ë¥˜ë„ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ ì¡´ì¬")
print(f"   2. í´ë˜ìŠ¤ ê°„ ìœ ì‚¬ì„±: ì…”ì¸ ì™€ í‹°ì…”ì¸ , ìƒŒë“¤ê³¼ ë¶€ì¸  ë“± êµ¬ë¶„ì´ ì–´ë ¤ì›€")
print(f"   3. í…ìŠ¤ì²˜ ë³µì¡ì„±: ì˜ë¥˜ì˜ íŒ¨í„´, ì£¼ë¦„ ë“± ë³µì¡í•œ í…ìŠ¤ì²˜")
print(f"   4. í˜•íƒœ ë‹¤ì–‘ì„±: ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œë„ ë‹¤ì–‘í•œ í˜•íƒœ")
print(f"   â†’ ì´ëŸ¬í•œ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ë” ì •êµí•œ ëª¨ë¸ê³¼ ê¸°ë²•ì´ í•„ìš”")

# ============================================================================
# 4. ë°ì´í„° ë¶„í•  ë° ë¡œë” ìƒì„±
# ============================================================================

print(f"\nğŸ“¦ ë°ì´í„° ë¶„í•  ë° ë¡œë” ìƒì„±")

# í›ˆë ¨ ë°ì´í„°ë¥¼ í›ˆë ¨/ê²€ì¦ìœ¼ë¡œ ë¶„í• 
# ì™œ ê²€ì¦ ì„¸íŠ¸ê°€ í•„ìš”í•œê°€?
# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ ê°ê´€ì  í‰ê°€
# 2. ê³¼ì í•© ì¡°ê¸° ê°ì§€
# 3. ëª¨ë¸ ì„ íƒì„ ìœ„í•œ ê¸°ì¤€
# 4. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì˜¤ì—¼ ë°©ì§€

train_size = int((1 - VALIDATION_SPLIT) * len(train_dataset))
val_size = len(train_dataset) - train_size

train_subset, val_subset = torch.utils.data.random_split(
    train_dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(RANDOM_SEED)
)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
print(f"   í›ˆë ¨: {len(train_subset):,}ê°œ")
print(f"   ê²€ì¦: {len(val_subset):,}ê°œ") 
print(f"   í…ŒìŠ¤íŠ¸: {len(test_dataset):,}ê°œ")

# ============================================================================
# 5. ê³ ê¸‰ ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
# ============================================================================

print(f"\nğŸ§  ê³ ê¸‰ ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜")

class AdvancedNN(nn.Module):
    """
    ê³ ê¸‰ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  with ë°°ì¹˜ ì •ê·œí™”
    
    ê°œì„ ì‚¬í•­:
    1. ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ (4ê°œ ì€ë‹‰ì¸µ)
    2. ë°°ì¹˜ ì •ê·œí™”ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ
    3. ì ì‘ì  ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    4. ì”ì°¨ ì—°ê²° (Residual Connection) ê°œë… ë„ì…
    
    ì™œ ì´ëŸ° êµ¬ì¡°ë¥¼ ì„ íƒí–ˆëŠ”ê°€?
    - ê¹Šì€ ë„¤íŠ¸ì›Œí¬: ë” ë³µì¡í•œ íŠ¹ì§• í•™ìŠµ ê°€ëŠ¥
    - ë°°ì¹˜ ì •ê·œí™”: ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ ë¬¸ì œ í•´ê²°, ë¹ ë¥¸ í•™ìŠµ
    - ì ì§„ì  ì°¨ì› ì¶•ì†Œ: ì •ë³´ ì†ì‹¤ ìµœì†Œí™”í•˜ë©° íŠ¹ì§• ì••ì¶•
    """
    
    def __init__(self, dropout_rate=0.3):
        super(AdvancedNN, self).__init__()
        
        # ì™„ì „ì—°ê²°ì¸µë“¤
        self.fc1 = nn.Linear(28 * 28, 512)  # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ (ë” í° ìš©ëŸ‰)
        self.fc2 = nn.Linear(512, 256)      # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ
        self.fc3 = nn.Linear(256, 128)      # ì„¸ ë²ˆì§¸ ì€ë‹‰ì¸µ
        self.fc4 = nn.Linear(128, 64)       # ë„¤ ë²ˆì§¸ ì€ë‹‰ì¸µ
        self.fc5 = nn.Linear(64, 10)        # ì¶œë ¥ì¸µ
        
        # ë°°ì¹˜ ì •ê·œí™” ë ˆì´ì–´ë“¤
        # ì™œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?
        # 1. ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™(Internal Covariate Shift) ë¬¸ì œ í•´ê²°
        # 2. ë” ë†’ì€ í•™ìŠµë¥  ì‚¬ìš© ê°€ëŠ¥
        # 3. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ëœ ë¯¼ê°
        # 4. ì •ê·œí™” íš¨ê³¼ë¡œ ê³¼ì í•© ë°©ì§€
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.bn3 = nn.BatchNorm1d(128)
        self.bn4 = nn.BatchNorm1d(64)
        
        # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë“¤ (ì¸µë³„ë¡œ ë‹¤ë¥¸ ë¹„ìœ¨ ì ìš©)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.dropout2 = nn.Dropout(dropout_rate * 0.8)  # ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
        self.dropout3 = nn.Dropout(dropout_rate * 0.6)
        self.dropout4 = nn.Dropout(dropout_rate * 0.4)
    
    def forward(self, x):
        # ì…ë ¥ í‰íƒ„í™”
        x = x.view(x.size(0), -1)
        
        # ì²« ë²ˆì§¸ ë¸”ë¡: Linear â†’ BatchNorm â†’ ReLU â†’ Dropout
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x)
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout2(x)
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡
        x = self.fc3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout3(x)
        
        # ë„¤ ë²ˆì§¸ ë¸”ë¡
        x = self.fc4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.dropout4(x)
        
        # ì¶œë ¥ì¸µ (í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ)
        x = self.fc5(x)
        
        return x

class SimpleNN(nn.Module):
    """
    ë¹„êµë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì‹ ê²½ë§ (ì´ì „ íŠœí† ë¦¬ì–¼ê³¼ ìœ ì‚¬)
    """
    
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
advanced_model = AdvancedNN(dropout_rate=0.3).to(device)
simple_model = SimpleNN().to(device)

print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ëª¨ë¸ êµ¬ì¡° ë¹„êµ
print(f"\nğŸ“‹ ê³ ê¸‰ ëª¨ë¸ êµ¬ì¡°:")
print(advanced_model)

print(f"\nğŸ“Š ëª¨ë¸ ë³µì¡ë„ ë¹„êµ:")
advanced_params = count_parameters(advanced_model, detailed=False)
simple_params = count_parameters(simple_model, detailed=False)

print(f"   ê³ ê¸‰ ëª¨ë¸: {advanced_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")
print(f"   ê°„ë‹¨ ëª¨ë¸: {simple_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")
print(f"   ë³µì¡ë„ ë¹„ìœ¨: {advanced_params['total_params'] / simple_params['total_params']:.1f}ë°°")

# ============================================================================
# 6. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
# ============================================================================

print(f"\nâš™ï¸  ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")

# ì†ì‹¤ í•¨ìˆ˜
criterion = nn.CrossEntropyLoss()

# ì˜µí‹°ë§ˆì´ì € (Adam with weight decay)
# Weight Decay: L2 ì •ê·œí™”ì™€ ìœ ì‚¬í•œ íš¨ê³¼ë¡œ ê³¼ì í•© ë°©ì§€
optimizer = optim.Adam(
    advanced_model.parameters(), 
    lr=LEARNING_RATE,
    weight_decay=1e-4  # L2 ì •ê·œí™” ê³„ìˆ˜
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
# ì™œ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ì´ í•„ìš”í•œê°€?
# 1. ì´ˆê¸°ì—ëŠ” í° í•™ìŠµë¥ ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
# 2. í›„ë°˜ì—ëŠ” ì‘ì€ í•™ìŠµë¥ ë¡œ ì„¸ë°€í•œ ì¡°ì •
# 3. ì§€ì—­ ìµœì†Ÿê°’ íƒˆì¶œ ë„ì›€
# 4. ë” ë‚˜ì€ ìµœì¢… ì„±ëŠ¥ ë‹¬ì„±

scheduler = optim.lr_scheduler.StepLR(
    optimizer, 
    step_size=7,    # 7 ì—í¬í¬ë§ˆë‹¤
    gamma=0.5       # í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
)

print(f"   ì†ì‹¤ í•¨ìˆ˜: {criterion.__class__.__name__}")
print(f"   ì˜µí‹°ë§ˆì´ì €: {optimizer.__class__.__name__}")
print(f"   ì´ˆê¸° í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   Weight Decay: {1e-4}")
print(f"   ìŠ¤ì¼€ì¤„ëŸ¬: StepLR (7 ì—í¬í¬ë§ˆë‹¤ 0.5ë°°)")

# ============================================================================
# 7. í–¥ìƒëœ í›ˆë ¨ í•¨ìˆ˜
# ============================================================================

def train_epoch_advanced(model, train_loader, criterion, optimizer, device, epoch):
    """
    í–¥ìƒëœ í›ˆë ¨ í•¨ìˆ˜ (ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ í¬í•¨)
    """
    model.train()
    
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    pbar = tqdm(train_loader, desc=f"ì—í¬í¬ {epoch+1} í›ˆë ¨")
    
    for batch_idx, (data, targets) in enumerate(pbar):
        data, targets = data.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€)
        # ì™œ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ì´ í•„ìš”í•œê°€?
        # 1. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€
        # 2. ì•ˆì •ì ì¸ í›ˆë ¨ ë³´ì¥
        # 3. í•™ìŠµë¥ ì„ ë” í¬ê²Œ ì„¤ì • ê°€ëŠ¥
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss += loss.item()
        predictions = torch.argmax(outputs, dim=1)
        correct_predictions += (predictions == targets).sum().item()
        total_samples += targets.size(0)
        
        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        current_accuracy = correct_predictions / total_samples
        current_lr = optimizer.param_groups[0]['lr']
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{current_accuracy:.4f}',
            'LR': f'{current_lr:.6f}'
        })
    
    avg_loss = running_loss / len(train_loader)
    accuracy = correct_predictions / total_samples
    
    return avg_loss, accuracy

def validate_epoch_advanced(model, val_loader, criterion, device):
    """
    í–¥ìƒëœ ê²€ì¦ í•¨ìˆ˜
    """
    model.eval()
    
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="ê²€ì¦")
        
        for data, targets in pbar:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            running_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            correct_predictions += (predictions == targets).sum().item()
            total_samples += targets.size(0)
            
            current_accuracy = correct_predictions / total_samples
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_accuracy:.4f}'
            })
    
    avg_loss = running_loss / len(val_loader)
    accuracy = correct_predictions / total_samples
    
    return avg_loss, accuracy

# ============================================================================
# 8. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

# í›ˆë ¨ ê¸°ë¡
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []
learning_rates = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì 
best_val_accuracy = 0.0
best_model_state = None
patience = 5  # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ì¸ë‚´ì‹¬
patience_counter = 0

start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í˜„ì¬ í•™ìŠµë¥  ê¸°ë¡
    current_lr = optimizer.param_groups[0]['lr']
    learning_rates.append(current_lr)
    
    # í›ˆë ¨
    train_loss, train_acc = train_epoch_advanced(
        advanced_model, train_loader, criterion, optimizer, device, epoch
    )
    
    # ê²€ì¦
    val_loss, val_acc = validate_epoch_advanced(
        advanced_model, val_loader, criterion, device
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler.step()
    
    # ê¸°ë¡ ì €ì¥
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   í›ˆë ¨ - ì†ì‹¤: {train_loss:.4f}, ì •í™•ë„: {train_acc:.4f}")
    print(f"   ê²€ì¦ - ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_acc:.4f}")
    print(f"   í•™ìŠµë¥ : {current_lr:.6f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if val_acc > best_val_accuracy:
        best_val_accuracy = val_acc
        best_model_state = copy.deepcopy(advanced_model.state_dict())
        patience_counter = 0
        print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(
            advanced_model, optimizer, epoch, val_loss, val_acc,
            save_path="./checkpoints/fashion_mnist_best_model.pth"
        )
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"   â° ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
            break

training_time = time.time() - start_time
print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f}")

# ============================================================================
# 9. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# í›ˆë ¨ ê³¡ì„ 
plot_training_curves(
    train_losses=train_losses,
    val_losses=val_losses,
    train_accuracies=train_accuracies,
    val_accuracies=val_accuracies,
    title="Fashion-MNIST ë¶„ë¥˜ - ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨ ê³¼ì •"
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ì‹œê°í™”
plot_learning_rate_schedule(
    lr_schedule=learning_rates,
    title="í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§"
)

# ============================================================================
# 10. ëª¨ë¸ ë¹„êµ ì‹¤í—˜
# ============================================================================

print(f"\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

# ê°„ë‹¨í•œ ëª¨ë¸ë„ í›ˆë ¨ (ë¹„êµìš©)
print(f"\nğŸ“Š ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨ ì¤‘...")

simple_optimizer = optim.Adam(simple_model.parameters(), lr=LEARNING_RATE)
simple_criterion = nn.CrossEntropyLoss()

# ê°„ë‹¨í•œ ëª¨ë¸ ë¹ ë¥¸ í›ˆë ¨ (5 ì—í¬í¬ë§Œ)
for epoch in range(5):
    simple_model.train()
    for data, targets in train_loader:
        data, targets = data.to(device), targets.to(device)
        simple_optimizer.zero_grad()
        outputs = simple_model(data)
        loss = simple_criterion(outputs, targets)
        loss.backward()
        simple_optimizer.step()

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
if best_model_state is not None:
    advanced_model.load_state_dict(best_model_state)

# ëª¨ë¸ ë¹„êµ
models_to_compare = {
    "ê³ ê¸‰ ëª¨ë¸ (BatchNorm + ê¹Šì€ êµ¬ì¡°)": advanced_model,
    "ê°„ë‹¨í•œ ëª¨ë¸ (ê¸°ë³¸ êµ¬ì¡°)": simple_model
}

comparison_results = compare_models(
    models=models_to_compare,
    test_dataloader=test_loader,
    criterion=criterion,
    device=device
)

# ============================================================================
# 11. ìµœì¢… í‰ê°€ ë° ì‹œê°í™”
# ============================================================================

print(f"\nğŸ¯ ìµœì¢… ëª¨ë¸ í‰ê°€")

# ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€
final_results = evaluate_model(
    model=advanced_model,
    dataloader=test_loader,
    criterion=criterion,
    device=device,
    num_classes=10
)

# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
print(f"\nğŸ–¼ï¸  ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”")
plot_model_predictions(
    model=advanced_model,
    dataloader=test_loader,
    class_names=class_names,
    num_samples=16,
    device=device,
    title="Fashion-MNIST ë¶„ë¥˜ - ê³ ê¸‰ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼"
)

# í˜¼ë™ í–‰ë ¬ ìƒì„±
print(f"\nğŸ“Š í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")

all_predictions = []
all_targets = []

advanced_model.eval()
with torch.no_grad():
    for data, targets in tqdm(test_loader, desc="ì˜ˆì¸¡ ìˆ˜ì§‘"):
        data, targets = data.to(device), targets.to(device)
        outputs = advanced_model(data)
        predictions = torch.argmax(outputs, dim=1)
        
        all_predictions.extend(predictions.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())

plot_confusion_matrix(
    y_true=np.array(all_targets),
    y_pred=np.array(all_predictions),
    class_names=class_names,
    title="Fashion-MNIST ë¶„ë¥˜ - í˜¼ë™ í–‰ë ¬"
)

# ============================================================================
# 12. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ê°œì„  ì œì•ˆ
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. Fashion-MNIST ë°ì´í„°ì…‹ ë¶„ì„ ë° ì „ì²˜ë¦¬")
print(f"   2. ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©")
print(f"   3. ë°°ì¹˜ ì •ê·œí™”ê°€ í¬í•¨ëœ ê¹Šì€ ì‹ ê²½ë§ êµ¬í˜„")
print(f"   4. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ë° ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘")
print(f"   5. ì¡°ê¸° ì¢…ë£Œ ë° ëª¨ë¸ ë¹„êµ")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
print(f"   - ê³ ê¸‰ ëª¨ë¸ ìµœê³  ì •í™•ë„: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)")
print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {advanced_params['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ë°°ì¹˜ ì •ê·œí™”ì˜ íš¨ê³¼: ì•ˆì •ì ì´ê³  ë¹ ë¥¸ í•™ìŠµ")
print(f"   2. ë°ì´í„° ì¦ê°•ì˜ ì¤‘ìš”ì„±: ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")
print(f"   3. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: ìµœì í™” ì„±ëŠ¥ ê°œì„ ")
print(f"   4. ì •ê·œí™” ê¸°ë²•ë“¤ì˜ ì¡°í•©: ê³¼ì í•© ë°©ì§€")
print(f"   5. ëª¨ë¸ ë³µì¡ë„ì™€ ì„±ëŠ¥ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„")

print(f"\nğŸ” Fashion-MNIST íŠ¹ì„± ë¶„ì„:")
print(f"   - MNIST ëŒ€ë¹„ ë‚®ì€ ì •í™•ë„: ë” ë³µì¡í•œ ì‹œê°ì  íŒ¨í„´")
print(f"   - í´ë˜ìŠ¤ ê°„ í˜¼ë™: ìœ ì‚¬í•œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ì˜ ì–´ë ¤ì›€")
print(f"   - í…ìŠ¤ì²˜ ì¤‘ìš”ì„±: ë‹¨ìˆœí•œ í˜•íƒœë³´ë‹¤ ë³µì¡í•œ íŒ¨í„´ ì¸ì‹ í•„ìš”")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   - 03_cnn_image_classification.py: í•©ì„±ê³± ì‹ ê²½ë§ìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ")
print(f"   - CIFAR-10ìœ¼ë¡œ ì»¬ëŸ¬ ì´ë¯¸ì§€ ë¶„ë¥˜ ë„ì „")
print(f"   - ì „ì´ í•™ìŠµ ë° ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í™œìš©")

print(f"\nğŸ”§ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. ë‹¤ë¥¸ ì •ê·œí™” ê¸°ë²• ë¹„êµ (Layer Norm, Group Norm)")
print(f"   2. ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜ ì‹¤í—˜ (Swish, GELU)")
print(f"   3. ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ")
print(f"   4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ (Optuna ë“±)")
print(f"   5. ëª¨ë¸ ì••ì¶• ê¸°ë²• (Pruning, Quantization)")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ ì‹ ê²½ë§ ì‹¬í™” íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œ CNNì„ ë°°ì›Œë³´ì„¸ìš”!")
print(f"=" * 60)