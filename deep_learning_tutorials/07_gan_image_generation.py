"""
ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 7: GAN ì´ë¯¸ì§€ ìƒì„±

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” CelebA ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 
GAN(Generative Adversarial Networks)ì˜ ì´ë¯¸ì§€ ìƒì„±ì„ í•™ìŠµí•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
1. GANì˜ í•µì‹¬ ê°œë…ê³¼ ì ëŒ€ì  í•™ìŠµ ì›ë¦¬
2. ìƒì„±ì(Generator)ì™€ íŒë³„ì(Discriminator) êµ¬ì¡°
3. ëª¨ë“œ ë¶•ê´´(Mode Collapse) ë¬¸ì œì™€ í•´ê²° ë°©ë²•
4. ìƒì„± í’ˆì§ˆ í‰ê°€ ë°©ë²• (FID, IS)
5. ì ì¬ ê³µê°„(Latent Space) íƒìƒ‰ê³¼ í•´ì„
6. DCGANê³¼ ê°œì„ ëœ GAN ê¸°ë²•ë“¤

ë°ì´í„°ì…‹ ì„ íƒ ì´ìœ  - CelebA (Celebrity Faces):
- 200,000ê°œ ì´ìƒì˜ ìœ ëª…ì¸ ì–¼êµ´ ì´ë¯¸ì§€
- ê³ í’ˆì§ˆì˜ ì •ë ¬ëœ ì–¼êµ´ ë°ì´í„°
- ë‹¤ì–‘í•œ ì—°ë ¹, ì„±ë³„, ì¸ì¢…ì˜ ì–¼êµ´ í¬í•¨
- GAN ì„±ëŠ¥ì„ ëª…í™•íˆ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë°ì´í„°
- ì–¼êµ´ ìƒì„±ì€ GANì˜ ëŒ€í‘œì  ì‘ìš© ë¶„ì•¼
- ì‹œê°ì ìœ¼ë¡œ ìƒì„± í’ˆì§ˆì„ ì‰½ê²Œ í‰ê°€ ê°€ëŠ¥
- ì ë‹¹í•œ ë³µì¡ë„ë¡œ í•™ìŠµì— ì í•©

ì™œ GANì„ ì‚¬ìš©í•˜ëŠ”ê°€?
1. ìƒì„± ëª¨ë¸ë§: ìƒˆë¡œìš´ ë°ì´í„° ìƒ˜í”Œ ìƒì„±
2. ë°ì´í„° ì¦ê°•: í›ˆë ¨ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°
3. ì°½ì‘ ë„êµ¬: ì˜ˆìˆ , ë””ìì¸ ë¶„ì•¼ í™œìš©
4. ë°ì´í„° í”„ë¼ì´ë²„ì‹œ: ì‹¤ì œ ë°ì´í„° ëŒ€ì‹  í•©ì„± ë°ì´í„°
5. í‘œí˜„ í•™ìŠµ: ì˜ë¯¸ìˆëŠ” ì ì¬ ê³µê°„ í•™ìŠµ

GAN vs ë‹¤ë¥¸ ìƒì„± ëª¨ë¸:
- VAE: íë¦¿í•œ ì´ë¯¸ì§€, ì•ˆì •ì  í•™ìŠµ
- GAN: ì„ ëª…í•œ ì´ë¯¸ì§€, ë¶ˆì•ˆì •í•œ í•™ìŠµ
- Diffusion: ìµœê³  í’ˆì§ˆ, ëŠë¦° ìƒì„±
- Flow: ì •í™•í•œ í™•ë¥  ê³„ì‚°, ì œí•œì  í‘œí˜„ë ¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import time
import copy
import os
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.visualization import plot_training_curves
from utils.model_utils import count_parameters, save_checkpoint

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 7: GAN ì´ë¯¸ì§€ ìƒì„±")
print("=" * 60)

# ============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}")

# GANì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
BATCH_SIZE = 64        # GANì€ í° ë°°ì¹˜ í¬ê¸°ê°€ ì•ˆì •ì„±ì— ë„ì›€
LEARNING_RATE_G = 0.0002  # ìƒì„±ì í•™ìŠµë¥ 
LEARNING_RATE_D = 0.0002  # íŒë³„ì í•™ìŠµë¥ 
BETA1 = 0.5            # Adam ì˜µí‹°ë§ˆì´ì € ë² íƒ€1 (GANì—ì„œ ì¼ë°˜ì )
EPOCHS = 100           # GANì€ ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í•„ìš”
RANDOM_SEED = 42
IMG_SIZE = 64          # ìƒì„±í•  ì´ë¯¸ì§€ í¬ê¸°
LATENT_DIM = 100       # ì ì¬ ë²¡í„° ì°¨ì›
NUM_CHANNELS = 3       # RGB ì±„ë„

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   ìƒì„±ì í•™ìŠµë¥ : {LEARNING_RATE_G}")
print(f"   íŒë³„ì í•™ìŠµë¥ : {LEARNING_RATE_D}")
print(f"   ì—í¬í¬: {EPOCHS}")
print(f"   ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}x{IMG_SIZE}")
print(f"   ì ì¬ ì°¨ì›: {LATENT_DIM}")

# ============================================================================
# 2. ìƒ˜í”Œ ì–¼êµ´ ë°ì´í„°ì…‹ ìƒì„± (CelebA ëŒ€ì‹ )
# ============================================================================

print(f"\nğŸ“ ìƒ˜í”Œ ì–¼êµ´ ë°ì´í„°ì…‹ ìƒì„±")

class SampleFaceDataset(Dataset):
    """
    êµìœ¡ìš© ìƒ˜í”Œ ì–¼êµ´ ë°ì´í„°ì…‹
    
    ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” CelebA ì‚¬ìš©
    ì—¬ê¸°ì„œëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œ ê°„ë‹¨í•œ í•©ì„± ì–¼êµ´ ë°ì´í„° ìƒì„±
    """
    
    def __init__(self, num_samples=5000, img_size=64, transform=None):
        self.num_samples = num_samples
        self.img_size = img_size
        self.transform = transform
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        self.images = self._generate_face_samples()
        
    def _generate_face_samples(self):
        """í•©ì„± ì–¼êµ´ ë°ì´í„° ìƒì„±"""
        print("ğŸ¨ í•©ì„± ì–¼êµ´ ë°ì´í„° ìƒì„± ì¤‘...")
        
        images = []
        np.random.seed(42)
        
        for i in tqdm(range(self.num_samples), desc="ì–¼êµ´ ë°ì´í„° ìƒì„±"):
            # ê¸°ë³¸ ì–¼êµ´ í˜•íƒœ ìƒì„±
            image = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)
            
            # ë°°ê²½ìƒ‰ (í”¼ë¶€í†¤)
            skin_color = np.random.randint(180, 255, 3)
            image[:, :] = skin_color
            
            # ì–¼êµ´ ìœ¤ê³½ (íƒ€ì›)
            center_x, center_y = self.img_size // 2, self.img_size // 2
            face_width = np.random.randint(self.img_size // 3, self.img_size // 2)
            face_height = np.random.randint(self.img_size // 2, int(self.img_size * 0.7))
            
            # ëˆˆ ì˜ì—­
            eye_y = center_y - face_height // 4
            left_eye_x = center_x - face_width // 4
            right_eye_x = center_x + face_width // 4
            eye_size = np.random.randint(3, 8)
            
            # ëˆˆ ê·¸ë¦¬ê¸°
            for dy in range(-eye_size, eye_size):
                for dx in range(-eye_size, eye_size):
                    if dx*dx + dy*dy <= eye_size*eye_size:
                        if (0 <= eye_y + dy < self.img_size and 
                            0 <= left_eye_x + dx < self.img_size):
                            image[eye_y + dy, left_eye_x + dx] = [0, 0, 0]
                        if (0 <= eye_y + dy < self.img_size and 
                            0 <= right_eye_x + dx < self.img_size):
                            image[eye_y + dy, right_eye_x + dx] = [0, 0, 0]
            
            # ì½” ì˜ì—­
            nose_y = center_y
            nose_x = center_x
            nose_size = np.random.randint(2, 5)
            
            for dy in range(-nose_size, nose_size):
                for dx in range(-nose_size//2, nose_size//2):
                    if (0 <= nose_y + dy < self.img_size and 
                        0 <= nose_x + dx < self.img_size):
                        image[nose_y + dy, nose_x + dx] = skin_color * 0.8
            
            # ì… ì˜ì—­
            mouth_y = center_y + face_height // 4
            mouth_width = np.random.randint(face_width // 4, face_width // 2)
            mouth_height = np.random.randint(2, 5)
            
            for dy in range(-mouth_height, mouth_height):
                for dx in range(-mouth_width, mouth_width):
                    if (0 <= mouth_y + dy < self.img_size and 
                        0 <= center_x + dx < self.img_size):
                        image[mouth_y + dy, center_x + dx] = [100, 50, 50]
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ìš´ ë³€í™”)
            noise = np.random.normal(0, 10, image.shape)
            image = np.clip(image + noise, 0, 255).astype(np.uint8)
            
            images.append(image)
        
        print(f"âœ… {len(images)}ê°œ ì–¼êµ´ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ")
        return images
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = self.images[idx]
        
        if self.transform:
            # PIL Imageë¡œ ë³€í™˜ í›„ transform ì ìš©
            image = Image.fromarray(image)
            image = self.transform(image)
        else:
            # í…ì„œë¡œ ë³€í™˜
            image = torch.FloatTensor(image).permute(2, 0, 1) / 255.0
        
        return image

# ë°ì´í„° ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
])

# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±
dataset = SampleFaceDataset(
    num_samples=3000,
    img_size=IMG_SIZE,
    transform=transform
)

dataloader = DataLoader(
    dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=True, 
    num_workers=2,
    drop_last=True  # ë§ˆì§€ë§‰ ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì œê±°
)

print(f"\nâœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
print(f"   ì´ ìƒ˜í”Œ: {len(dataset):,}ê°œ")
print(f"   ë°°ì¹˜ ìˆ˜: {len(dataloader)}")

# ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”
def show_sample_images(dataloader, num_samples=16):
    """ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”"""
    data_iter = iter(dataloader)
    images = next(data_iter)
    
    # ì •ê·œí™” í•´ì œ [-1, 1] â†’ [0, 1]
    images = (images + 1) / 2
    
    # ê·¸ë¦¬ë“œë¡œ ë°°ì¹˜
    grid = make_grid(images[:num_samples], nrow=4, padding=2)
    
    plt.figure(figsize=(10, 10))
    plt.imshow(grid.permute(1, 2, 0))
    plt.title('ìƒ˜í”Œ ì–¼êµ´ ë°ì´í„°', fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.show()

print(f"\nğŸ–¼ï¸  ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”")
show_sample_images(dataloader)

# ============================================================================
# 3. DCGAN ëª¨ë¸ ì •ì˜
# ============================================================================

print(f"\nğŸ§  DCGAN ëª¨ë¸ ì •ì˜")

def weights_init(m):
    """
    ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•¨ìˆ˜
    
    DCGAN ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ì´ˆê¸°í™” ë°©ë²•:
    - Conv, ConvTranspose: í‰ê·  0, í‘œì¤€í¸ì°¨ 0.02ì˜ ì •ê·œë¶„í¬
    - BatchNorm: ê°€ì¤‘ì¹˜ 1, í¸í–¥ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    """
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

class Generator(nn.Module):
    """
    DCGAN ìƒì„±ì
    
    êµ¬ì¡°:
    - ì ì¬ ë²¡í„° â†’ ì „ì¹˜ í•©ì„±ê³±ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
    - ë°°ì¹˜ ì •ê·œí™”ì™€ ReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
    - ë§ˆì§€ë§‰ ì¸µì—ì„œ Tanhë¡œ [-1, 1] ë²”ìœ„ ì¶œë ¥
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    - ì „ì¹˜ í•©ì„±ê³±(Transposed Convolution)ìœ¼ë¡œ ì—…ìƒ˜í”Œë§
    - ì ì§„ì ìœ¼ë¡œ í•´ìƒë„ ì¦ê°€: 4x4 â†’ 8x8 â†’ 16x16 â†’ 32x32 â†’ 64x64
    - ë°°ì¹˜ ì •ê·œí™”ë¡œ ì•ˆì •ì  í•™ìŠµ
    """
    
    def __init__(self, latent_dim=100, img_channels=3, feature_dim=64):
        super(Generator, self).__init__()
        
        self.latent_dim = latent_dim
        
        # ì²« ë²ˆì§¸ ì¸µ: ì ì¬ ë²¡í„° â†’ 4x4 íŠ¹ì„± ë§µ
        self.initial = nn.Sequential(
            # ì…ë ¥: (batch_size, latent_dim, 1, 1)
            nn.ConvTranspose2d(latent_dim, feature_dim * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(feature_dim * 8),
            nn.ReLU(True)
            # ì¶œë ¥: (batch_size, feature_dim * 8, 4, 4)
        )
        
        # ì—…ìƒ˜í”Œë§ ì¸µë“¤
        self.upsample_layers = nn.Sequential(
            # 4x4 â†’ 8x8
            nn.ConvTranspose2d(feature_dim * 8, feature_dim * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 4),
            nn.ReLU(True),
            
            # 8x8 â†’ 16x16
            nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 2),
            nn.ReLU(True),
            
            # 16x16 â†’ 32x32
            nn.ConvTranspose2d(feature_dim * 2, feature_dim, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(True),
            
            # 32x32 â†’ 64x64
            nn.ConvTranspose2d(feature_dim, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()  # [-1, 1] ë²”ìœ„ë¡œ ì¶œë ¥
        )
        
    def forward(self, z):
        # z: (batch_size, latent_dim)
        # 4ì°¨ì›ìœ¼ë¡œ ë³€í™˜: (batch_size, latent_dim, 1, 1)
        z = z.view(z.size(0), z.size(1), 1, 1)
        
        # ì´ˆê¸° íŠ¹ì„± ë§µ ìƒì„±
        x = self.initial(z)
        
        # ì—…ìƒ˜í”Œë§ì„ í†µí•œ ì´ë¯¸ì§€ ìƒì„±
        x = self.upsample_layers(x)
        
        return x

class Discriminator(nn.Module):
    """
    DCGAN íŒë³„ì
    
    êµ¬ì¡°:
    - ì´ë¯¸ì§€ â†’ í•©ì„±ê³±ìœ¼ë¡œ íŠ¹ì„± ì¶”ì¶œ â†’ ì§„ì§œ/ê°€ì§œ íŒë³„
    - LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
    - ë°°ì¹˜ ì •ê·œí™” (ì²« ë²ˆì§¸ ì¸µ ì œì™¸)
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    - í•©ì„±ê³±ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§: 64x64 â†’ 32x32 â†’ 16x16 â†’ 8x8 â†’ 4x4
    - LeakyReLUë¡œ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„ 
    - ë§ˆì§€ë§‰ì— ì‹œê·¸ëª¨ì´ë“œë¡œ í™•ë¥  ì¶œë ¥
    """
    
    def __init__(self, img_channels=3, feature_dim=64):
        super(Discriminator, self).__init__()
        
        self.main = nn.Sequential(
            # 64x64 â†’ 32x32
            nn.Conv2d(img_channels, feature_dim, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 32x32 â†’ 16x16
            nn.Conv2d(feature_dim, feature_dim * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 2),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 16x16 â†’ 8x8
            nn.Conv2d(feature_dim * 2, feature_dim * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 4),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 8x8 â†’ 4x4
            nn.Conv2d(feature_dim * 4, feature_dim * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 8),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 4x4 â†’ 1x1 (ìµœì¢… íŒë³„)
            nn.Conv2d(feature_dim * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        return self.main(x).view(-1, 1).squeeze(1)

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
generator = Generator(
    latent_dim=LATENT_DIM,
    img_channels=NUM_CHANNELS,
    feature_dim=64
).to(device)

discriminator = Discriminator(
    img_channels=NUM_CHANNELS,
    feature_dim=64
).to(device)

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
generator.apply(weights_init)
discriminator.apply(weights_init)

print(f"âœ… DCGAN ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ëª¨ë¸ ë³µì¡ë„
gen_params = count_parameters(generator, detailed=False)
disc_params = count_parameters(discriminator, detailed=False)

print(f"\nğŸ“Š ëª¨ë¸ ë³µì¡ë„:")
print(f"   ìƒì„±ì: {gen_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")
print(f"   íŒë³„ì: {disc_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")

# ============================================================================
# 4. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
# ============================================================================

print(f"\nâš™ï¸  ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")

# ì†ì‹¤ í•¨ìˆ˜ (Binary Cross Entropy)
criterion = nn.BCELoss()

# ë¼ë²¨ ì •ì˜
real_label = 1.0
fake_label = 0.0

# ì˜µí‹°ë§ˆì´ì € (Adam with beta1=0.5)
# GANì—ì„œëŠ” beta1ì„ 0.5ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(BETA1, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(BETA1, 0.999))

print(f"   ì†ì‹¤ í•¨ìˆ˜: Binary Cross Entropy")
print(f"   ìƒì„±ì ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE_G}, beta1={BETA1})")
print(f"   íŒë³„ì ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE_D}, beta1={BETA1})")

# ê³ ì •ëœ ë…¸ì´ì¦ˆ (ìƒì„± ê³¼ì • ì‹œê°í™”ìš©)
fixed_noise = torch.randn(64, LATENT_DIM, device=device)

# ============================================================================
# 5. GAN í›ˆë ¨ í•¨ìˆ˜
# ============================================================================

def train_gan_epoch(generator, discriminator, dataloader, criterion, 
                   optimizer_G, optimizer_D, device, epoch):
    """
    GAN í›ˆë ¨ í•¨ìˆ˜
    
    GAN í›ˆë ¨ì˜ í•µì‹¬:
    1. íŒë³„ì í›ˆë ¨: ì§„ì§œ ì´ë¯¸ì§€ëŠ” 1, ê°€ì§œ ì´ë¯¸ì§€ëŠ” 0ìœ¼ë¡œ ë¶„ë¥˜
    2. ìƒì„±ì í›ˆë ¨: íŒë³„ìë¥¼ ì†ì´ë„ë¡ í•™ìŠµ (ê°€ì§œ ì´ë¯¸ì§€ë¥¼ 1ë¡œ ë¶„ë¥˜í•˜ê²Œ)
    
    ì£¼ì˜ì‚¬í•­:
    - íŒë³„ìì™€ ìƒì„±ìì˜ ê· í˜•ì´ ì¤‘ìš”
    - í•œìª½ì´ ë„ˆë¬´ ê°•í•´ì§€ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§
    """
    
    generator.train()
    discriminator.train()
    
    running_loss_D = 0.0
    running_loss_G = 0.0
    num_batches = 0
    
    pbar = tqdm(dataloader, desc=f"ì—í¬í¬ {epoch+1} í›ˆë ¨")
    
    for batch_idx, real_images in enumerate(pbar):
        batch_size = real_images.size(0)
        real_images = real_images.to(device)
        
        # ========================================
        # 1. íŒë³„ì í›ˆë ¨
        # ========================================
        
        discriminator.zero_grad()
        
        # 1-1. ì§„ì§œ ì´ë¯¸ì§€ë¡œ í›ˆë ¨
        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)
        output_real = discriminator(real_images)
        loss_D_real = criterion(output_real, label)
        loss_D_real.backward()
        
        # 1-2. ê°€ì§œ ì´ë¯¸ì§€ë¡œ í›ˆë ¨
        noise = torch.randn(batch_size, LATENT_DIM, device=device)
        fake_images = generator(noise)
        label.fill_(fake_label)
        output_fake = discriminator(fake_images.detach())  # detachë¡œ ìƒì„±ì ê·¸ë˜ë””ì–¸íŠ¸ ì°¨ë‹¨
        loss_D_fake = criterion(output_fake, label)
        loss_D_fake.backward()
        
        # íŒë³„ì íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer_D.step()
        
        loss_D = loss_D_real + loss_D_fake
        
        # ========================================
        # 2. ìƒì„±ì í›ˆë ¨
        # ========================================
        
        generator.zero_grad()
        
        # ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì§„ì§œë¡œ ë¶„ë¥˜í•˜ë„ë¡ í•™ìŠµ
        label.fill_(real_label)  # ìƒì„±ìëŠ” íŒë³„ìë¥¼ ì†ì´ë ¤ê³  í•¨
        output = discriminator(fake_images)
        loss_G = criterion(output, label)
        loss_G.backward()
        
        # ìƒì„±ì íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer_G.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss_D += loss_D.item()
        running_loss_G += loss_G.item()
        num_batches += 1
        
        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        if batch_idx % 10 == 0:
            pbar.set_postfix({
                'D_Loss': f'{loss_D.item():.4f}',
                'G_Loss': f'{loss_G.item():.4f}',
                'D_Real': f'{output_real.mean().item():.4f}',
                'D_Fake': f'{output_fake.mean().item():.4f}'
            })
    
    avg_loss_D = running_loss_D / num_batches
    avg_loss_G = running_loss_G / num_batches
    
    return avg_loss_D, avg_loss_G

# ============================================================================
# 6. ìƒì„± í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜
# ============================================================================

def calculate_inception_score(images, batch_size=32, splits=10):
    """
    Inception Score (IS) ê³„ì‚°
    
    ISëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ì¸¡ì •:
    - ë†’ì€ í’ˆì§ˆ: ê° ì´ë¯¸ì§€ê°€ ëª…í™•í•œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë¨
    - ë†’ì€ ë‹¤ì–‘ì„±: ì „ì²´ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ë¶„í¬
    
    ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ì „ í›ˆë ¨ëœ Inception ëª¨ë¸ í•„ìš”
    ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ëœ ë²„ì „ìœ¼ë¡œ êµ¬í˜„
    """
    # ê°„ì†Œí™”ëœ IS ê³„ì‚° (ì‹¤ì œë¡œëŠ” Inception v3 ëª¨ë¸ ì‚¬ìš©)
    # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ì§€ì˜ í†µê³„ì  íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´
    
    mean_pixel = images.mean(dim=[2, 3])  # ê° ì±„ë„ì˜ í‰ê· 
    std_pixel = images.std(dim=[2, 3])    # ê° ì±„ë„ì˜ í‘œì¤€í¸ì°¨
    
    # ë‹¤ì–‘ì„± ì¸¡ì • (í‘œì¤€í¸ì°¨ì˜ í‰ê· )
    diversity = std_pixel.mean().item()
    
    # í’ˆì§ˆ ì¸¡ì • (í”½ì…€ ê°’ì˜ ì¼ê´€ì„±)
    quality = 1.0 / (1.0 + mean_pixel.std().item())
    
    # ê°„ì†Œí™”ëœ IS ì ìˆ˜
    is_score = quality * diversity * 10  # ìŠ¤ì¼€ì¼ë§
    
    return is_score

def generate_and_save_images(generator, epoch, device, num_images=64):
    """ìƒì„±ëœ ì´ë¯¸ì§€ ì €ì¥ ë° ì‹œê°í™”"""
    generator.eval()
    
    with torch.no_grad():
        # ê³ ì •ëœ ë…¸ì´ì¦ˆë¡œ ì´ë¯¸ì§€ ìƒì„±
        fake_images = generator(fixed_noise[:num_images])
        
        # ì •ê·œí™” í•´ì œ [-1, 1] â†’ [0, 1]
        fake_images = (fake_images + 1) / 2
        
        # ê·¸ë¦¬ë“œë¡œ ë°°ì¹˜
        grid = make_grid(fake_images, nrow=8, padding=2)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 12))
        plt.imshow(grid.cpu().permute(1, 2, 0))
        plt.title(f'ìƒì„±ëœ ì–¼êµ´ ì´ë¯¸ì§€ - ì—í¬í¬ {epoch+1}', fontsize=16, fontweight='bold')
        plt.axis('off')
        plt.show()
        
        # IS ì ìˆ˜ ê³„ì‚°
        is_score = calculate_inception_score(fake_images)
        
        return is_score

# ============================================================================
# 7. GAN í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ GAN í›ˆë ¨ ì‹œì‘")

# í›ˆë ¨ ê¸°ë¡
G_losses = []
D_losses = []
IS_scores = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì  (IS ì ìˆ˜ ê¸°ì¤€)
best_is_score = 0.0
best_generator_state = None

start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í›ˆë ¨
    loss_D, loss_G = train_gan_epoch(
        generator, discriminator, dataloader, criterion,
        optimizer_G, optimizer_D, device, epoch
    )
    
    # ê¸°ë¡ ì €ì¥
    D_losses.append(loss_D)
    G_losses.append(loss_G)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   íŒë³„ì ì†ì‹¤: {loss_D:.4f}")
    print(f"   ìƒì„±ì ì†ì‹¤: {loss_G:.4f}")
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ë° í‰ê°€
    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(f"\nğŸ–¼ï¸  ìƒì„±ëœ ì´ë¯¸ì§€ í™•ì¸ (ì—í¬í¬ {epoch+1})")
        is_score = generate_and_save_images(generator, epoch, device)
        IS_scores.append(is_score)
        
        print(f"   Inception Score: {is_score:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if is_score > best_is_score:
            best_is_score = is_score
            best_generator_state = copy.deepcopy(generator.state_dict())
            print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  IS ì ìˆ˜!")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_checkpoint(
                generator, optimizer_G, epoch, loss_G, is_score,
                save_path="./checkpoints/gan_best_generator.pth"
            )
    
    # í•™ìŠµ ë¶ˆì•ˆì •ì„± ê°ì§€
    if loss_D < 0.01 or loss_G > 10:
        print(f"   âš ï¸ í•™ìŠµ ë¶ˆì•ˆì •ì„± ê°ì§€ - D_loss: {loss_D:.4f}, G_loss: {loss_G:.4f}")

training_time = time.time() - start_time
print(f"\nâœ… GAN í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  IS ì ìˆ˜: {best_is_score:.4f}")

# ============================================================================
# 8. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ GAN í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# ì†ì‹¤ ê³¡ì„ 
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(D_losses, label='íŒë³„ì ì†ì‹¤', color='blue', linewidth=2)
plt.plot(G_losses, label='ìƒì„±ì ì†ì‹¤', color='red', linewidth=2)
plt.title('GAN í›ˆë ¨ ì†ì‹¤')
plt.xlabel('ì—í¬í¬')
plt.ylabel('ì†ì‹¤')
plt.legend()
plt.grid(True, alpha=0.3)

# IS ì ìˆ˜ ë³€í™”
plt.subplot(1, 2, 2)
if IS_scores:
    epochs_with_is = [i * 10 for i in range(len(IS_scores))]
    plt.plot(epochs_with_is, IS_scores, 'g-', marker='o', linewidth=2, markersize=6)
    plt.title('Inception Score ë³€í™”')
    plt.xlabel('ì—í¬í¬')
    plt.ylabel('IS ì ìˆ˜')
    plt.grid(True, alpha=0.3)

plt.suptitle('GAN í›ˆë ¨ ê³¼ì • ë¶„ì„', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# ============================================================================
# 9. ì ì¬ ê³µê°„ íƒìƒ‰
# ============================================================================

print(f"\nğŸ” ì ì¬ ê³µê°„ íƒìƒ‰")

def interpolate_latent_space(generator, device, num_steps=10):
    """
    ì ì¬ ê³µê°„ì—ì„œì˜ ë³´ê°„ (Interpolation)
    
    ë‘ ëœë¤ ë²¡í„° ì‚¬ì´ë¥¼ ì„ í˜• ë³´ê°„í•˜ì—¬
    ìƒì„±ëœ ì´ë¯¸ì§€ì˜ ë¶€ë“œëŸ¬ìš´ ë³€í™” ê´€ì°°
    """
    generator.eval()
    
    # ì‹œì‘ì ê³¼ ëì  ëœë¤ ë²¡í„°
    z1 = torch.randn(1, LATENT_DIM, device=device)
    z2 = torch.randn(1, LATENT_DIM, device=device)
    
    interpolated_images = []
    
    with torch.no_grad():
        for i in range(num_steps):
            # ì„ í˜• ë³´ê°„
            alpha = i / (num_steps - 1)
            z_interp = (1 - alpha) * z1 + alpha * z2
            
            # ì´ë¯¸ì§€ ìƒì„±
            fake_image = generator(z_interp)
            fake_image = (fake_image + 1) / 2  # ì •ê·œí™” í•´ì œ
            
            interpolated_images.append(fake_image.cpu())
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, num_steps, figsize=(20, 3))
    
    for i, img in enumerate(interpolated_images):
        axes[i].imshow(img.squeeze().permute(1, 2, 0))
        axes[i].set_title(f'Step {i+1}')
        axes[i].axis('off')
    
    plt.suptitle('ì ì¬ ê³µê°„ ë³´ê°„ - ë¶€ë“œëŸ¬ìš´ ì–¼êµ´ ë³€í™”', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

def explore_latent_directions(generator, device):
    """
    ì ì¬ ê³µê°„ì˜ ì˜ë¯¸ìˆëŠ” ë°©í–¥ íƒìƒ‰
    
    íŠ¹ì • ë°©í–¥ìœ¼ë¡œ ì´ë™í–ˆì„ ë•Œ ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ê´€ì°°
    """
    generator.eval()
    
    # ê¸°ì¤€ ì ì¬ ë²¡í„°
    base_z = torch.randn(1, LATENT_DIM, device=device)
    
    # ëœë¤ ë°©í–¥ ë²¡í„°
    direction = torch.randn(1, LATENT_DIM, device=device)
    direction = direction / direction.norm()  # ì •ê·œí™”
    
    scales = [-2, -1, 0, 1, 2]  # ì´ë™ ì •ë„
    
    with torch.no_grad():
        fig, axes = plt.subplots(1, len(scales), figsize=(15, 3))
        
        for i, scale in enumerate(scales):
            # ë°©í–¥ìœ¼ë¡œ ì´ë™
            z_moved = base_z + scale * direction
            
            # ì´ë¯¸ì§€ ìƒì„±
            fake_image = generator(z_moved)
            fake_image = (fake_image + 1) / 2
            
            axes[i].imshow(fake_image.cpu().squeeze().permute(1, 2, 0))
            axes[i].set_title(f'Scale: {scale}')
            axes[i].axis('off')
        
        plt.suptitle('ì ì¬ ê³µê°„ ë°©í–¥ íƒìƒ‰', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì ì¬ ê³µê°„ íƒìƒ‰
if best_generator_state is not None:
    generator.load_state_dict(best_generator_state)

print("ğŸ”„ ì ì¬ ê³µê°„ ë³´ê°„:")
interpolate_latent_space(generator, device)

print("\nğŸ§­ ì ì¬ ê³µê°„ ë°©í–¥ íƒìƒ‰:")
explore_latent_directions(generator, device)

# ============================================================================
# 10. ëª¨ë“œ ë¶•ê´´ ë¶„ì„
# ============================================================================

print(f"\nğŸ” ëª¨ë“œ ë¶•ê´´ (Mode Collapse) ë¶„ì„")

def analyze_mode_collapse(generator, device, num_samples=100):
    """
    ëª¨ë“œ ë¶•ê´´ ë¶„ì„
    
    ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì˜ ë‹¤ì–‘ì„±ì„ ì¸¡ì •í•˜ì—¬
    ëª¨ë“œ ë¶•ê´´ ì—¬ë¶€ í™•ì¸
    """
    generator.eval()
    
    generated_images = []
    
    with torch.no_grad():
        for _ in range(num_samples):
            z = torch.randn(1, LATENT_DIM, device=device)
            fake_image = generator(z)
            fake_image = (fake_image + 1) / 2
            generated_images.append(fake_image.cpu())
    
    # ì´ë¯¸ì§€ë“¤ì„ í…ì„œë¡œ ê²°í•©
    all_images = torch.cat(generated_images, dim=0)
    
    # ë‹¤ì–‘ì„± ì¸¡ì •
    # 1. í”½ì…€ ê°’ì˜ í‘œì¤€í¸ì°¨
    pixel_diversity = all_images.std(dim=0).mean().item()
    
    # 2. ì´ë¯¸ì§€ ê°„ í‰ê·  ê±°ë¦¬
    flattened = all_images.view(num_samples, -1)
    distances = []
    for i in range(min(50, num_samples)):  # ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ 50ê°œë§Œ
        for j in range(i+1, min(50, num_samples)):
            dist = torch.norm(flattened[i] - flattened[j]).item()
            distances.append(dist)
    
    avg_distance = np.mean(distances) if distances else 0
    
    print(f"ğŸ“Š ë‹¤ì–‘ì„± ë¶„ì„:")
    print(f"   í”½ì…€ ë‹¤ì–‘ì„±: {pixel_diversity:.4f}")
    print(f"   í‰ê·  ì´ë¯¸ì§€ ê±°ë¦¬: {avg_distance:.4f}")
    
    # ë‹¤ì–‘ì„± ê¸°ì¤€
    if pixel_diversity < 0.1:
        print(f"   âš ï¸ ë‚®ì€ ë‹¤ì–‘ì„± - ëª¨ë“œ ë¶•ê´´ ì˜ì‹¬")
    elif pixel_diversity > 0.3:
        print(f"   âœ… ë†’ì€ ë‹¤ì–‘ì„± - ê±´ê°•í•œ ìƒì„±")
    else:
        print(f"   ğŸ“Š ë³´í†µ ë‹¤ì–‘ì„±")
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ ì‹œê°í™”
    sample_grid = make_grid(all_images[:16], nrow=4, padding=2)
    
    plt.figure(figsize=(10, 10))
    plt.imshow(sample_grid.permute(1, 2, 0))
    plt.title(f'ë‹¤ì–‘ì„± ë¶„ì„ - ìƒì„±ëœ ìƒ˜í”Œë“¤\ní”½ì…€ ë‹¤ì–‘ì„±: {pixel_diversity:.4f}', 
              fontsize=14, fontweight='bold')
    plt.axis('off')
    plt.show()
    
    return pixel_diversity, avg_distance

diversity_score, distance_score = analyze_mode_collapse(generator, device)

# ============================================================================
# 11. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ì‹¤ìš©ì  í™œìš©
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. GANì˜ í•µì‹¬ ê°œë…ê³¼ ì ëŒ€ì  í•™ìŠµ ì›ë¦¬")
print(f"   2. DCGAN êµ¬ì¡° (ìƒì„±ì + íŒë³„ì) êµ¬í˜„")
print(f"   3. GAN í›ˆë ¨ì˜ ë¶ˆì•ˆì •ì„±ê³¼ í•´ê²° ë°©ë²•")
print(f"   4. ìƒì„± í’ˆì§ˆ í‰ê°€ (Inception Score)")
print(f"   5. ì ì¬ ê³µê°„ íƒìƒ‰ê³¼ ë³´ê°„")
print(f"   6. ëª¨ë“œ ë¶•ê´´ ë¶„ì„ ë° ë‹¤ì–‘ì„± ì¸¡ì •")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
print(f"   - ìµœê³  IS ì ìˆ˜: {best_is_score:.4f}")
print(f"   - ìƒì„±ì íŒŒë¼ë¯¸í„°: {gen_params['total_params']:,}ê°œ")
print(f"   - íŒë³„ì íŒŒë¼ë¯¸í„°: {disc_params['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   - ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.4f}")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ì ëŒ€ì  í•™ìŠµ: ë‘ ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìŸì„ í†µí•œ í•™ìŠµ")
print(f"   2. ìƒì„±ì: ë…¸ì´ì¦ˆì—ì„œ í˜„ì‹¤ì ì¸ ì´ë¯¸ì§€ ìƒì„±")
print(f"   3. íŒë³„ì: ì§„ì§œì™€ ê°€ì§œ ì´ë¯¸ì§€ êµ¬ë³„")
print(f"   4. ê· í˜•ì˜ ì¤‘ìš”ì„±: ìƒì„±ìì™€ íŒë³„ìì˜ ì ì ˆí•œ ê· í˜•")
print(f"   5. ì ì¬ ê³µê°„: ì˜ë¯¸ìˆëŠ” í‘œí˜„ í•™ìŠµ")

print(f"\nğŸ” GANì˜ ì¥ë‹¨ì :")
print(f"   ì¥ì :")
print(f"   - ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± ê°€ëŠ¥")
print(f"   - ëª…ì‹œì  í™•ë¥  ëª¨ë¸ ë¶ˆí•„ìš”")
print(f"   - ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼")
print(f"   - ì°½ì˜ì  ì½˜í…ì¸  ìƒì„±")
print(f"   ë‹¨ì :")
print(f"   - í›ˆë ¨ ë¶ˆì•ˆì •ì„±")
print(f"   - ëª¨ë“œ ë¶•ê´´ ë¬¸ì œ")
print(f"   - í‰ê°€ ë©”íŠ¸ë¦­ì˜ ì–´ë ¤ì›€")
print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ì„±")

print(f"\nğŸš€ ì‹¤ìš©ì  í™œìš© ë¶„ì•¼:")
print(f"   1. ì˜ˆìˆ  ë° ë””ìì¸: ì°½ì‘ ë„êµ¬, ìŠ¤íƒ€ì¼ ë³€í™˜")
print(f"   2. ì—”í„°í…Œì¸ë¨¼íŠ¸: ê²Œì„ ìºë¦­í„°, ì˜í™” íŠ¹ìˆ˜íš¨ê³¼")
print(f"   3. íŒ¨ì…˜: ì˜ìƒ ë””ìì¸, ê°€ìƒ ëª¨ë¸")
print(f"   4. ë°ì´í„° ì¦ê°•: í›ˆë ¨ ë°ì´í„° ë¶€ì¡± í•´ê²°")
print(f"   5. ì˜ë£Œ: ì˜ë£Œ ì˜ìƒ ìƒì„±, í”„ë¼ì´ë²„ì‹œ ë³´í˜¸")
print(f"   6. ë³´ì•ˆ: ë”¥í˜ì´í¬ íƒì§€, ìƒì²´ì¸ì‹ ê°•í™”")

print(f"\nğŸ”§ GAN ê°œì„  ê¸°ë²•:")
print(f"   1. WGAN: Wasserstein ê±°ë¦¬ë¡œ ì•ˆì •ì  í•™ìŠµ")
print(f"   2. Progressive GAN: ì ì§„ì  í•´ìƒë„ ì¦ê°€")
print(f"   3. StyleGAN: ìŠ¤íƒ€ì¼ ê¸°ë°˜ ìƒì„±")
print(f"   4. BigGAN: ëŒ€ê·œëª¨ ê³ í•´ìƒë„ ìƒì„±")
print(f"   5. Conditional GAN: ì¡°ê±´ë¶€ ìƒì„±")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   - 08_transformer_nlp.py: Transformerë¡œ ìì—°ì–´ ì²˜ë¦¬")
print(f"   - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ í˜„ëŒ€ NLPì˜ í•µì‹¬")
print(f"   - ê¸°ê³„ ë²ˆì—­ê³¼ ì–¸ì–´ ëª¨ë¸ë§")

print(f"\nğŸ”§ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. ì¡°ê±´ë¶€ GAN: íŠ¹ì • ì†ì„±ì„ ê°€ì§„ ì–¼êµ´ ìƒì„±")
print(f"   2. CycleGAN: ë„ë©”ì¸ ê°„ ë³€í™˜")
print(f"   3. StyleGAN: ê³ í’ˆì§ˆ ì–¼êµ´ ìƒì„±")
print(f"   4. ì‹¤ì œ CelebA ë°ì´í„°ì…‹ ì‚¬ìš©")
print(f"   5. FID ì ìˆ˜ë¡œ ë” ì •í™•í•œ í‰ê°€")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ GAN ì´ë¯¸ì§€ ìƒì„± íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œ Transformer NLPë¥¼ ë°°ì›Œë³´ì„¸ìš”!")
print(f"=" * 60)
import os
from PIL import Image
import requests
import zipfile
from scipy import linalg
import warnings
warnings.filterwarnings('ignore')

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.data_utils import download_and_extract
from utils.visualization import plot_training_curves
from utils.model_utils import count_parameters, save_checkpoint

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 7: GAN ì´ë¯¸ì§€ ìƒì„±")
print("=" * 60)#
 ============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}")

# GAN í›ˆë ¨ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
BATCH_SIZE = 64        # GANì€ í° ë°°ì¹˜ê°€ ì•ˆì •ì  í•™ìŠµì— ë„ì›€
LEARNING_RATE_G = 0.0002  # ìƒì„±ì í•™ìŠµë¥ 
LEARNING_RATE_D = 0.0002  # íŒë³„ì í•™ìŠµë¥ 
BETA1 = 0.5            # Adam ì˜µí‹°ë§ˆì´ì € ë² íƒ€1 (GANì— ìµœì í™”)
EPOCHS = 100           # GANì€ ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í•„ìš”
RANDOM_SEED = 42
LATENT_DIM = 100       # ì ì¬ ë²¡í„° ì°¨ì›
IMG_SIZE = 64          # ìƒì„±í•  ì´ë¯¸ì§€ í¬ê¸°
NUM_CHANNELS = 3       # RGB ì±„ë„
SAMPLE_INTERVAL = 500  # ìƒ˜í”Œ ìƒì„± ê°„ê²©

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)

print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   ìƒì„±ì í•™ìŠµë¥ : {LEARNING_RATE_G}")
print(f"   íŒë³„ì í•™ìŠµë¥ : {LEARNING_RATE_D}")
print(f"   ì—í¬í¬: {EPOCHS}")
print(f"   ì ì¬ ì°¨ì›: {LATENT_DIM}")
print(f"   ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}x{IMG_SIZE}")

# ============================================================================
# 2. CelebA ë°ì´í„°ì…‹ ì¤€ë¹„
# ============================================================================

print(f"\nğŸ“ CelebA ë°ì´í„°ì…‹ ì¤€ë¹„")

class CelebADataset(Dataset):
    """
    CelebA ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    
    CelebAëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ”
    ê³µì‹ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, root_dir, transform=None, use_sample_data=True):
        self.root_dir = root_dir
        self.transform = transform
        self.use_sample_data = use_sample_data
        
        if use_sample_data:
            # êµìœ¡ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            self.create_sample_data()
        else:
            # ì‹¤ì œ CelebA ë°ì´í„° ë¡œë“œ
            self.load_celeba_data()
    
    def create_sample_data(self):
        """
        êµìœ¡ìš© ìƒ˜í”Œ ì–¼êµ´ ë°ì´í„° ìƒì„±
        
        ì‹¤ì œ CelebA ëŒ€ì‹  í•©ì„±ëœ ì–¼êµ´ íŒ¨í„´ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì§„ì§œ CelebA ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        print("ğŸ­ êµìœ¡ìš© ìƒ˜í”Œ ì–¼êµ´ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ìƒ˜í”Œ ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.root_dir, exist_ok=True)
        
        # ë‹¤ì–‘í•œ ì–¼êµ´ íŒ¨í„´ ìƒì„±
        self.sample_images = []
        
        for i in range(1000):  # 1000ê°œ ìƒ˜í”Œ ìƒì„±
            # ê¸°ë³¸ ì–¼êµ´ í˜•íƒœ ìƒì„±
            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
            
            # ì–¼êµ´ ìœ¤ê³½ (íƒ€ì›)
            center_x, center_y = IMG_SIZE // 2, IMG_SIZE // 2
            
            # í”¼ë¶€ìƒ‰ (ë‹¤ì–‘í•œ í†¤)
            skin_color = np.random.randint(180, 255, 3)
            
            # ì–¼êµ´ ì˜ì—­ ì±„ìš°ê¸°
            y, x = np.ogrid[:IMG_SIZE, :IMG_SIZE]
            mask = ((x - center_x) / 20) ** 2 + ((y - center_y) / 25) ** 2 <= 1
            img[mask] = skin_color
            
            # ëˆˆ (ê²€ì€ìƒ‰ íƒ€ì›)
            eye_y = center_y - 8
            left_eye_x, right_eye_x = center_x - 8, center_x + 8
            
            # ì™¼ìª½ ëˆˆ
            eye_mask_l = ((x - left_eye_x) / 3) ** 2 + ((y - eye_y) / 2) ** 2 <= 1
            img[eye_mask_l] = [0, 0, 0]
            
            # ì˜¤ë¥¸ìª½ ëˆˆ
            eye_mask_r = ((x - right_eye_x) / 3) ** 2 + ((y - eye_y) / 2) ** 2 <= 1
            img[eye_mask_r] = [0, 0, 0]
            
            # ì… (ë¹¨ê°„ìƒ‰)
            mouth_y = center_y + 8
            mouth_mask = ((x - center_x) / 6) ** 2 + ((y - mouth_y) / 2) ** 2 <= 1
            img[mouth_mask] = [200, 50, 50]
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ì›€)
            noise = np.random.normal(0, 10, img.shape).astype(np.int16)
            img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)
            
            self.sample_images.append(img)
        
        print(f"âœ… {len(self.sample_images)}ê°œ ìƒ˜í”Œ ì–¼êµ´ ìƒì„± ì™„ë£Œ")
    
    def load_celeba_data(self):
        """
        ì‹¤ì œ CelebA ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì‚¬ìš© ì‹œ)
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CelebA ë°ì´í„° ë¡œë“œ
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
        self.create_sample_data()
    
    def __len__(self):
        return len(self.sample_images)
    
    def __getitem__(self, idx):
        image = self.sample_images[idx]
        
        # PIL Imageë¡œ ë³€í™˜
        image = Image.fromarray(image)
        
        if self.transform:
            image = self.transform(image)
        
        return image

# ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
])

# ë°ì´í„°ì…‹ ìƒì„±
dataset = CelebADataset(
    root_dir='./data/celeba_sample',
    transform=transform,
    use_sample_data=True
)

# ë°ì´í„° ë¡œë” ìƒì„±
dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    drop_last=True  # ë§ˆì§€ë§‰ ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì œê±°
)

print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}ê°œ")
print(f"   ë°°ì¹˜ ìˆ˜: {len(dataloader)}")

# ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”
print(f"\nğŸ–¼ï¸  ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”")

sample_batch = next(iter(dataloader))
sample_grid = make_grid(sample_batch[:16], nrow=4, normalize=True, padding=2)

plt.figure(figsize=(10, 10))
plt.imshow(sample_grid.permute(1, 2, 0))
plt.title("CelebA ìƒ˜í”Œ ë°ì´í„° (êµìœ¡ìš©)")
plt.axis('off')
plt.show()

# ============================================================================
# 3. GAN ëª¨ë¸ ì •ì˜
# ============================================================================

print(f"\nğŸ§  GAN ëª¨ë¸ ì •ì˜")

class Generator(nn.Module):
    """
    DCGAN ìŠ¤íƒ€ì¼ ìƒì„±ì
    
    êµ¬ì¡°:
    - ì…ë ¥: ì ì¬ ë²¡í„° z (latent_dim,)
    - ì¶œë ¥: ì´ë¯¸ì§€ (3, 64, 64)
    - ì „ì¹˜ í•©ì„±ê³±(Transposed Convolution)ìœ¼ë¡œ ì—…ìƒ˜í”Œë§
    - ë°°ì¹˜ ì •ê·œí™”ì™€ ReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
    
    ì™œ ì´ëŸ° êµ¬ì¡°ì¸ê°€?
    1. ì „ì¹˜ í•©ì„±ê³±: ì €í•´ìƒë„ â†’ ê³ í•´ìƒë„ ë³€í™˜
    2. ë°°ì¹˜ ì •ê·œí™”: ì•ˆì •ì ì¸ í•™ìŠµ
    3. ReLU: ì–‘ìˆ˜ í™œì„±í™”ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€
    4. Tanh ì¶œë ¥: [-1, 1] ë²”ìœ„ (ì •ê·œí™”ëœ ì´ë¯¸ì§€ì™€ ë§¤ì¹­)
    """
    
    def __init__(self, latent_dim=100, img_channels=3, feature_dim=64):
        super(Generator, self).__init__()
        
        self.latent_dim = latent_dim
        
        # ì´ˆê¸° ì™„ì „ì—°ê²°ì¸µ (ì ì¬ ë²¡í„° â†’ íŠ¹ì„± ë§µ)
        self.fc = nn.Linear(latent_dim, feature_dim * 8 * 4 * 4)
        
        # ì „ì¹˜ í•©ì„±ê³± ë ˆì´ì–´ë“¤ (ì—…ìƒ˜í”Œë§)
        self.conv_layers = nn.Sequential(
            # 4x4 â†’ 8x8
            nn.ConvTranspose2d(feature_dim * 8, feature_dim * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 4),
            nn.ReLU(True),
            
            # 8x8 â†’ 16x16  
            nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 2),
            nn.ReLU(True),
            
            # 16x16 â†’ 32x32
            nn.ConvTranspose2d(feature_dim * 2, feature_dim, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(True),
            
            # 32x32 â†’ 64x64
            nn.ConvTranspose2d(feature_dim, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()  # [-1, 1] ë²”ìœ„ ì¶œë ¥
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """
        ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (DCGAN ë…¼ë¬¸ ê¸°ì¤€)
        
        ì™œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ê°€ ì¤‘ìš”í•œê°€?
        1. ì•ˆì •ì ì¸ í•™ìŠµ: ì ì ˆí•œ ì´ˆê¸°ê°’ìœ¼ë¡œ ìˆ˜ë ´ ë³´ì¥
        2. ê·¸ë˜ë””ì–¸íŠ¸ íë¦„: ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì€ ê°’ ë°©ì§€
        3. ëŒ€ì¹­ì„± ê¹¨ê¸°: ëª¨ë“  ë‰´ëŸ°ì´ ê°™ì€ ê°’ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì•ˆë¨
        """
        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):
            nn.init.normal_(module.weight.data, 0.0, 0.02)
        elif isinstance(module, nn.BatchNorm2d):
            nn.init.normal_(module.weight.data, 1.0, 0.02)
            nn.init.constant_(module.bias.data, 0)
    
    def forward(self, z):
        # ì™„ì „ì—°ê²°ì¸µ
        x = self.fc(z)
        
        # 4D í…ì„œë¡œ reshape
        x = x.view(x.size(0), -1, 4, 4)
        
        # ì „ì¹˜ í•©ì„±ê³±ìœ¼ë¡œ ì—…ìƒ˜í”Œë§
        x = self.conv_layers(x)
        
        return x

class Discriminator(nn.Module):
    """
    DCGAN ìŠ¤íƒ€ì¼ íŒë³„ì
    
    êµ¬ì¡°:
    - ì…ë ¥: ì´ë¯¸ì§€ (3, 64, 64)
    - ì¶œë ¥: ì‹¤ì œ/ê°€ì§œ í™•ë¥  (1,)
    - í•©ì„±ê³±ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§
    - LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
    
    ì™œ ì´ëŸ° êµ¬ì¡°ì¸ê°€?
    1. í•©ì„±ê³±: ì´ë¯¸ì§€ì˜ ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ
    2. LeakyReLU: ìŒìˆ˜ ê·¸ë˜ë””ì–¸íŠ¸ë„ ì¼ë¶€ ì „ë‹¬
    3. ë°°ì¹˜ ì •ê·œí™”: ì•ˆì •ì ì¸ í•™ìŠµ (ì²« ì¸µ ì œì™¸)
    4. ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥: [0, 1] í™•ë¥ ê°’
    """
    
    def __init__(self, img_channels=3, feature_dim=64):
        super(Discriminator, self).__init__()
        
        self.conv_layers = nn.Sequential(
            # 64x64 â†’ 32x32
            nn.Conv2d(img_channels, feature_dim, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 32x32 â†’ 16x16
            nn.Conv2d(feature_dim, feature_dim * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 2),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 16x16 â†’ 8x8
            nn.Conv2d(feature_dim * 2, feature_dim * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 4),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 8x8 â†’ 4x4
            nn.Conv2d(feature_dim * 4, feature_dim * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 8),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 4x4 â†’ 1x1
            nn.Conv2d(feature_dim * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Conv2d):
            nn.init.normal_(module.weight.data, 0.0, 0.02)
        elif isinstance(module, nn.BatchNorm2d):
            nn.init.normal_(module.weight.data, 1.0, 0.02)
            nn.init.constant_(module.bias.data, 0)
    
    def forward(self, x):
        x = self.conv_layers(x)
        return x.view(x.size(0), -1)  # (batch_size, 1)

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
generator = Generator(
    latent_dim=LATENT_DIM,
    img_channels=NUM_CHANNELS,
    feature_dim=64
).to(device)

discriminator = Discriminator(
    img_channels=NUM_CHANNELS,
    feature_dim=64
).to(device)

print(f"âœ… GAN ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
print(f"\nğŸ“‹ ìƒì„±ì êµ¬ì¡°:")
print(generator)

print(f"\nğŸ“‹ íŒë³„ì êµ¬ì¡°:")
print(discriminator)

# ëª¨ë¸ ë³µì¡ë„ ë¶„ì„
print(f"\nğŸ“Š ëª¨ë¸ ë³µì¡ë„:")
gen_params = count_parameters(generator, detailed=False)
disc_params = count_parameters(discriminator, detailed=False)

print(f"   ìƒì„±ì: {gen_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")
print(f"   íŒë³„ì: {disc_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")
print(f"   ì´í•©: {gen_params['total_params'] + disc_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")

# ============================================================================
# 4. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
# ============================================================================

print(f"\nâš™ï¸  ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")

# GAN ì†ì‹¤ í•¨ìˆ˜
# ì™œ Binary Cross Entropyì¸ê°€?
# 1. ì´ì§„ ë¶„ë¥˜: ì‹¤ì œ(1) vs ê°€ì§œ(0) êµ¬ë¶„
# 2. í™•ë¥  ì¶œë ¥: ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ê³¼ ë§¤ì¹­
# 3. ì•ˆì •ì  í•™ìŠµ: ì˜ ì •ì˜ëœ ê·¸ë˜ë””ì–¸íŠ¸
criterion = nn.BCELoss()

# ì˜µí‹°ë§ˆì´ì € (ìƒì„±ìì™€ íŒë³„ì ê°ê°)
# ì™œ Adamì„ ì‚¬ìš©í•˜ëŠ”ê°€?
# 1. ì ì‘ì  í•™ìŠµë¥ : ê° íŒŒë¼ë¯¸í„°ë³„ ìµœì í™”
# 2. ëª¨ë©˜í…€ íš¨ê³¼: ì§„ë™ ê°ì†Œ
# 3. GANì—ì„œ ê²€ì¦ëœ ì„±ëŠ¥
optimizer_G = optim.Adam(
    generator.parameters(),
    lr=LEARNING_RATE_G,
    betas=(BETA1, 0.999)  # DCGAN ë…¼ë¬¸ ê¶Œì¥ê°’
)

optimizer_D = optim.Adam(
    discriminator.parameters(),
    lr=LEARNING_RATE_D,
    betas=(BETA1, 0.999)
)

print(f"   ì†ì‹¤ í•¨ìˆ˜: {criterion.__class__.__name__}")
print(f"   ìƒì„±ì ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE_G}, beta1={BETA1})")
print(f"   íŒë³„ì ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE_D}, beta1={BETA1})")

# ê³ ì •ëœ ì ì¬ ë²¡í„° (ìƒì„± ê³¼ì • ì‹œê°í™”ìš©)
fixed_noise = torch.randn(16, LATENT_DIM, device=device)

print(f"\nğŸ² ê³ ì • ì ì¬ ë²¡í„° ìƒì„±: {fixed_noise.shape}")

# ============================================================================
# 5. GAN í›ˆë ¨ í•¨ìˆ˜
# ============================================================================

def train_gan_epoch(generator, discriminator, dataloader, optimizer_G, optimizer_D, 
                   criterion, device, epoch):
    """
    GAN í•œ ì—í¬í¬ í›ˆë ¨
    
    GAN í›ˆë ¨ ê³¼ì •:
    1. íŒë³„ì í›ˆë ¨: ì‹¤ì œ ì´ë¯¸ì§€ëŠ” 1, ê°€ì§œ ì´ë¯¸ì§€ëŠ” 0ìœ¼ë¡œ ë¶„ë¥˜
    2. ìƒì„±ì í›ˆë ¨: íŒë³„ìê°€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ 1ë¡œ ë¶„ë¥˜í•˜ë„ë¡ ì†ì„
    
    Args:
        generator: ìƒì„±ì ëª¨ë¸
        discriminator: íŒë³„ì ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        optimizer_G: ìƒì„±ì ì˜µí‹°ë§ˆì´ì €
        optimizer_D: íŒë³„ì ì˜µí‹°ë§ˆì´ì €
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ì—°ì‚° ì¥ì¹˜
        epoch: í˜„ì¬ ì—í¬í¬
    
    Returns:
        tuple: (ìƒì„±ì ì†ì‹¤, íŒë³„ì ì†ì‹¤)
    """
    
    generator.train()
    discriminator.train()
    
    running_loss_G = 0.0
    running_loss_D = 0.0
    num_batches = 0
    
    # ì‹¤ì œ/ê°€ì§œ ë¼ë²¨
    real_label = 1.0
    fake_label = 0.0
    
    pbar = tqdm(dataloader, desc=f"ì—í¬í¬ {epoch+1}")
    
    for batch_idx, real_images in enumerate(pbar):
        batch_size = real_images.size(0)
        real_images = real_images.to(device)
        
        # ====================================================================
        # 1. íŒë³„ì í›ˆë ¨
        # ====================================================================
        
        optimizer_D.zero_grad()
        
        # 1-1. ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ì ì†ì‹¤
        real_labels = torch.full((batch_size, 1), real_label, device=device)
        real_output = discriminator(real_images)
        loss_D_real = criterion(real_output, real_labels)
        
        # 1-2. ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ì ì†ì‹¤
        noise = torch.randn(batch_size, LATENT_DIM, device=device)
        fake_images = generator(noise)
        fake_labels = torch.full((batch_size, 1), fake_label, device=device)
        fake_output = discriminator(fake_images.detach())  # detachë¡œ ìƒì„±ì ê·¸ë˜ë””ì–¸íŠ¸ ì°¨ë‹¨
        loss_D_fake = criterion(fake_output, fake_labels)
        
        # 1-3. íŒë³„ì ì´ ì†ì‹¤
        loss_D = loss_D_real + loss_D_fake
        loss_D.backward()
        optimizer_D.step()
        
        # ====================================================================
        # 2. ìƒì„±ì í›ˆë ¨
        # ====================================================================
        
        optimizer_G.zero_grad()
        
        # ìƒì„±ìëŠ” íŒë³„ìê°€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì‹¤ì œë¡œ ë¶„ë¥˜í•˜ë„ë¡ ì†ì„
        fake_output = discriminator(fake_images)
        loss_G = criterion(fake_output, real_labels)  # ê°€ì§œë¥¼ ì‹¤ì œë¡œ ì†ì´ë ¤ í•¨
        loss_G.backward()
        optimizer_G.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss_G += loss_G.item()
        running_loss_D += loss_D.item()
        num_batches += 1
        
        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        pbar.set_postfix({
            'D_Loss': f'{loss_D.item():.4f}',
            'G_Loss': f'{loss_G.item():.4f}',
            'D_Real': f'{real_output.mean().item():.3f}',
            'D_Fake': f'{fake_output.mean().item():.3f}'
        })
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ìƒì„± ì´ë¯¸ì§€ ì €ì¥
        if batch_idx % SAMPLE_INTERVAL == 0:
            save_generated_samples(generator, fixed_noise, epoch, batch_idx)
    
    avg_loss_G = running_loss_G / num_batches
    avg_loss_D = running_loss_D / num_batches
    
    return avg_loss_G, avg_loss_D

def save_generated_samples(generator, fixed_noise, epoch, batch_idx):
    """ìƒì„±ëœ ìƒ˜í”Œ ì´ë¯¸ì§€ ì €ì¥"""
    generator.eval()
    
    with torch.no_grad():
        fake_images = generator(fixed_noise)
        
        # ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ìƒì„±
        grid = make_grid(fake_images, nrow=4, normalize=True, padding=2)
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('./generated_samples', exist_ok=True)
        
        # ì´ë¯¸ì§€ ì €ì¥
        save_image(grid, f'./generated_samples/epoch_{epoch+1}_batch_{batch_idx}.png')
    
    generator.train()

# ============================================================================
# 6. GAN í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ GAN í›ˆë ¨ ì‹œì‘")

# í›ˆë ¨ ê¸°ë¡
G_losses = []
D_losses = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì  (ìƒì„±ì ê¸°ì¤€)
best_G_loss = float('inf')
best_generator_state = None

start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í•œ ì—í¬í¬ í›ˆë ¨
    loss_G, loss_D = train_gan_epoch(
        generator, discriminator, dataloader,
        optimizer_G, optimizer_D, criterion, device, epoch
    )
    
    # ê¸°ë¡ ì €ì¥
    G_losses.append(loss_G)
    D_losses.append(loss_D)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   ìƒì„±ì ì†ì‹¤: {loss_G:.4f}")
    print(f"   íŒë³„ì ì†ì‹¤: {loss_D:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if loss_G < best_G_loss:
        best_G_loss = loss_G
        best_generator_state = copy.deepcopy(generator.state_dict())
        print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ìƒì„±ì ì†ì‹¤: {loss_G:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(
            generator, optimizer_G, epoch, loss_G, 0,
            save_path="./checkpoints/gan_generator_best.pth"
        )
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ìƒì„± ê²°ê³¼ ì‹œê°í™”
    if (epoch + 1) % 10 == 0:
        visualize_generation_progress(generator, fixed_noise, epoch)

training_time = time.time() - start_time
print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  ìƒì„±ì ì†ì‹¤: {best_G_loss:.4f}")

# ============================================================================
# 7. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# ì†ì‹¤ ê³¡ì„ 
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(G_losses, label='ìƒì„±ì ì†ì‹¤', color='blue')
plt.plot(D_losses, label='íŒë³„ì ì†ì‹¤', color='red')
plt.xlabel('ì—í¬í¬')
plt.ylabel('ì†ì‹¤')
plt.title('GAN í›ˆë ¨ ì†ì‹¤')
plt.legend()
plt.grid(True, alpha=0.3)

# ì†ì‹¤ ë¹„ìœ¨
plt.subplot(1, 2, 2)
loss_ratio = np.array(G_losses) / np.array(D_losses)
plt.plot(loss_ratio, color='green')
plt.xlabel('ì—í¬í¬')
plt.ylabel('ìƒì„±ì ì†ì‹¤ / íŒë³„ì ì†ì‹¤')
plt.title('ì†ì‹¤ ë¹„ìœ¨ (ê· í˜• ì§€í‘œ)')
plt.grid(True, alpha=0.3)
plt.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='ê· í˜•ì ')
plt.legend()

plt.tight_layout()
plt.show()

def visualize_generation_progress(generator, fixed_noise, epoch):
    """ìƒì„± ê³¼ì • ì‹œê°í™”"""
    generator.eval()
    
    with torch.no_grad():
        fake_images = generator(fixed_noise)
        
        # ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ìƒì„±
        grid = make_grid(fake_images, nrow=4, normalize=True, padding=2)
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 10))
        plt.imshow(grid.permute(1, 2, 0).cpu())
        plt.title(f'ìƒì„±ëœ ì–¼êµ´ ì´ë¯¸ì§€ - ì—í¬í¬ {epoch+1}')
        plt.axis('off')
        plt.show()
    
    generator.train()

# ìµœì¢… ìƒì„± ê²°ê³¼
print(f"\nğŸ¨ ìµœì¢… ìƒì„± ê²°ê³¼")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
if best_generator_state is not None:
    generator.load_state_dict(best_generator_state)

visualize_generation_progress(generator, fixed_noise, EPOCHS-1)# =
===========================================================================
# 8. ì ì¬ ê³µê°„ íƒìƒ‰
# ============================================================================

print(f"\nğŸŒŒ ì ì¬ ê³µê°„ íƒìƒ‰")

def explore_latent_space(generator, device, num_samples=8):
    """
    ì ì¬ ê³µê°„ íƒìƒ‰ ë° ì‹œê°í™”
    
    ì ì¬ ê³µê°„ì˜ íŠ¹ì„±:
    1. ì—°ì†ì„±: ë¹„ìŠ·í•œ ë²¡í„°ëŠ” ë¹„ìŠ·í•œ ì´ë¯¸ì§€ ìƒì„±
    2. ì˜ë¯¸ì  êµ¬ì¡°: íŠ¹ì • ë°©í–¥ì´ íŠ¹ì • ì†ì„± ë³€í™”
    3. ë³´ê°„ ê°€ëŠ¥ì„±: ë‘ ì  ì‚¬ì´ ë³´ê°„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë³€í™”
    """
    
    generator.eval()
    
    print(f"ğŸ² 1. ëœë¤ ìƒ˜í”Œ ìƒì„±")
    
    # ëœë¤ ì ì¬ ë²¡í„° ìƒì„±
    random_noise = torch.randn(num_samples, LATENT_DIM, device=device)
    
    with torch.no_grad():
        random_images = generator(random_noise)
        
        # ì‹œê°í™”
        grid = make_grid(random_images, nrow=4, normalize=True, padding=2)
        plt.figure(figsize=(10, 6))
        plt.imshow(grid.permute(1, 2, 0).cpu())
        plt.title('ëœë¤ ì ì¬ ë²¡í„°ë¡œ ìƒì„±ëœ ì–¼êµ´ë“¤')
        plt.axis('off')
        plt.show()
    
    print(f"ğŸ”„ 2. ì ì¬ ê³µê°„ ë³´ê°„ (Interpolation)")
    
    # ë‘ ëœë¤ ì  ì„ íƒ
    z1 = torch.randn(1, LATENT_DIM, device=device)
    z2 = torch.randn(1, LATENT_DIM, device=device)
    
    # ë³´ê°„ ê³„ìˆ˜
    alphas = torch.linspace(0, 1, 8, device=device)
    
    interpolated_images = []
    
    with torch.no_grad():
        for alpha in alphas:
            # ì„ í˜• ë³´ê°„
            z_interp = (1 - alpha) * z1 + alpha * z2
            img = generator(z_interp)
            interpolated_images.append(img)
        
        # ë³´ê°„ ê²°ê³¼ ì‹œê°í™”
        interpolated_tensor = torch.cat(interpolated_images, dim=0)
        grid = make_grid(interpolated_tensor, nrow=8, normalize=True, padding=2)
        
        plt.figure(figsize=(16, 4))
        plt.imshow(grid.permute(1, 2, 0).cpu())
        plt.title('ì ì¬ ê³µê°„ ë³´ê°„: ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ ë³€í™”')
        plt.axis('off')
        plt.show()
    
    print(f"ğŸ¯ 3. íŠ¹ì • ë°©í–¥ íƒìƒ‰")
    
    # ê¸°ì¤€ì  ì„ íƒ
    base_z = torch.randn(1, LATENT_DIM, device=device)
    
    # ëœë¤ ë°©í–¥ ë²¡í„°
    direction = torch.randn(1, LATENT_DIM, device=device)
    direction = direction / direction.norm()  # ì •ê·œí™”
    
    # ë°©í–¥ì„ ë”°ë¼ ì´ë™
    scales = torch.linspace(-3, 3, 7, device=device)
    
    direction_images = []
    
    with torch.no_grad():
        for scale in scales:
            z_moved = base_z + scale * direction
            img = generator(z_moved)
            direction_images.append(img)
        
        # ë°©í–¥ íƒìƒ‰ ê²°ê³¼ ì‹œê°í™”
        direction_tensor = torch.cat(direction_images, dim=0)
        grid = make_grid(direction_tensor, nrow=7, normalize=True, padding=2)
        
        plt.figure(figsize=(14, 4))
        plt.imshow(grid.permute(1, 2, 0).cpu())
        plt.title('íŠ¹ì • ë°©í–¥ì„ ë”°ë¥¸ ì ì¬ ê³µê°„ íƒìƒ‰')
        plt.axis('off')
        plt.show()
    
    generator.train()

explore_latent_space(generator, device)

# ============================================================================
# 9. GAN í’ˆì§ˆ í‰ê°€
# ============================================================================

print(f"\nğŸ“Š GAN ìƒì„± í’ˆì§ˆ í‰ê°€")

def calculate_inception_score(generator, device, num_samples=1000, batch_size=32):
    """
    Inception Score (IS) ê³„ì‚°
    
    ISëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë©”íŠ¸ë¦­:
    1. ë‹¤ì–‘ì„±: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì´ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ì— ë¶„í¬
    2. ì„ ëª…ë„: ê° ì´ë¯¸ì§€ê°€ íŠ¹ì • í´ë˜ìŠ¤ì— ëª…í™•íˆ ë¶„ë¥˜
    
    ë†’ì€ IS = ë‹¤ì–‘í•˜ë©´ì„œë„ ì„ ëª…í•œ ì´ë¯¸ì§€ ìƒì„±
    """
    
    print("ğŸ¯ Inception Score ê³„ì‚° ì¤‘...")
    
    try:
        # ì‚¬ì „ í›ˆë ¨ëœ Inception ëª¨ë¸ ë¡œë“œ
        from torchvision.models import inception_v3
        inception_model = inception_v3(pretrained=True, transform_input=False)
        inception_model.eval()
        inception_model.to(device)
        
        generator.eval()
        
        # ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì˜ ì˜ˆì¸¡ í™•ë¥  ìˆ˜ì§‘
        all_preds = []
        
        with torch.no_grad():
            for i in range(0, num_samples, batch_size):
                current_batch_size = min(batch_size, num_samples - i)
                
                # ì ì¬ ë²¡í„° ìƒì„±
                z = torch.randn(current_batch_size, LATENT_DIM, device=device)
                
                # ì´ë¯¸ì§€ ìƒì„±
                fake_images = generator(z)
                
                # Inception ëª¨ë¸ ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (299x299)
                fake_images_resized = F.interpolate(
                    fake_images, size=(299, 299), mode='bilinear', align_corners=False
                )
                
                # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
                pred = inception_model(fake_images_resized)
                pred = F.softmax(pred, dim=1)
                
                all_preds.append(pred.cpu())
        
        # ì „ì²´ ì˜ˆì¸¡ í™•ë¥  ê²°í•©
        all_preds = torch.cat(all_preds, dim=0)
        
        # IS ê³„ì‚°
        # IS = exp(E[KL(p(y|x) || p(y))])
        py = all_preds.mean(dim=0)  # p(y)
        kl_div = all_preds * (torch.log(all_preds) - torch.log(py))
        kl_div = kl_div.sum(dim=1)
        is_score = torch.exp(kl_div.mean()).item()
        
        print(f"âœ… Inception Score: {is_score:.2f}")
        
        generator.train()
        return is_score
        
    except Exception as e:
        print(f"âŒ IS ê³„ì‚° ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ torchvision ë²„ì „ì„ í™•ì¸í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

def analyze_mode_collapse(generator, device, num_samples=100):
    """
    ëª¨ë“œ ë¶•ê´´(Mode Collapse) ë¶„ì„
    
    ëª¨ë“œ ë¶•ê´´: ìƒì„±ìê°€ ë‹¤ì–‘ì„±ì„ ìƒê³  ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë§Œ ìƒì„±í•˜ëŠ” í˜„ìƒ
    
    ë¶„ì„ ë°©ë²•:
    1. ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    2. í‰ê·  ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ëª¨ë“œ ë¶•ê´´ ì˜ì‹¬
    """
    
    print("ğŸ” ëª¨ë“œ ë¶•ê´´ ë¶„ì„ ì¤‘...")
    
    generator.eval()
    
    # ì—¬ëŸ¬ ì´ë¯¸ì§€ ìƒì„±
    generated_images = []
    
    with torch.no_grad():
        for i in range(num_samples):
            z = torch.randn(1, LATENT_DIM, device=device)
            img = generator(z)
            generated_images.append(img.cpu())
    
    # ì´ë¯¸ì§€ë“¤ì„ í‰íƒ„í™”í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜
    flattened_images = [img.view(-1) for img in generated_images]
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    
    for i in range(len(flattened_images)):
        for j in range(i+1, len(flattened_images)):
            img1, img2 = flattened_images[i], flattened_images[j]
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = F.cosine_similarity(img1.unsqueeze(0), img2.unsqueeze(0))
            similarities.append(similarity.item())
    
    avg_similarity = np.mean(similarities)
    std_similarity = np.std(similarities)
    
    print(f"ğŸ“Š ëª¨ë“œ ë¶•ê´´ ë¶„ì„ ê²°ê³¼:")
    print(f"   í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
    print(f"   ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨: {std_similarity:.4f}")
    
    if avg_similarity > 0.8:
        print(f"   âš ï¸  ë†’ì€ ìœ ì‚¬ë„ - ëª¨ë“œ ë¶•ê´´ ì˜ì‹¬")
        print(f"      í•´ê²° ë°©ë²•: í•™ìŠµë¥  ì¡°ì •, ì •ê·œí™” ê¸°ë²• ì ìš©")
    elif avg_similarity < 0.3:
        print(f"   âœ… ë‚®ì€ ìœ ì‚¬ë„ - ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ìƒì„±")
    else:
        print(f"   ğŸ“Š ë³´í†µ ìˆ˜ì¤€ì˜ ë‹¤ì–‘ì„±")
    
    # ìœ ì‚¬ë„ ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    plt.hist(similarities, bins=30, alpha=0.7, edgecolor='black')
    plt.axvline(avg_similarity, color='red', linestyle='--', 
                label=f'í‰ê· : {avg_similarity:.3f}')
    plt.xlabel('ì½”ì‚¬ì¸ ìœ ì‚¬ë„')
    plt.ylabel('ë¹ˆë„')
    plt.title('ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ë¶„í¬')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    generator.train()
    
    return avg_similarity, std_similarity

# í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
is_score = calculate_inception_score(generator, device, num_samples=500)
similarity_stats = analyze_mode_collapse(generator, device, num_samples=50)

# ============================================================================
# 10. GAN ê°œì„  ê¸°ë²• ì†Œê°œ
# ============================================================================

print(f"\nğŸš€ GAN ê°œì„  ê¸°ë²• ì†Œê°œ")

def explain_gan_improvements():
    """
    GANì˜ ì£¼ìš” ê°œì„  ê¸°ë²•ë“¤ ì„¤ëª…
    """
    
    print(f"\nğŸ“š 1. í›ˆë ¨ ì•ˆì •í™” ê¸°ë²•")
    print(f"   ğŸ¯ Spectral Normalization:")
    print(f"      - íŒë³„ìì˜ ë¦½ì‹œì¸  ìƒìˆ˜ ì œí•œ")
    print(f"      - ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€")
    print(f"      - ë” ì•ˆì •ì ì¸ í›ˆë ¨")
    
    print(f"\n   ğŸ¯ Progressive Growing:")
    print(f"      - ë‚®ì€ í•´ìƒë„ë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì  ì¦ê°€")
    print(f"      - ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± ê°€ëŠ¥")
    print(f"      - ì•ˆì •ì ì¸ í•™ìŠµ ê³¼ì •")
    
    print(f"\n   ğŸ¯ Self-Attention:")
    print(f"      - ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§")
    print(f"      - ë” ì¼ê´€ëœ ì´ë¯¸ì§€ ìƒì„±")
    print(f"      - ì„¸ë¶€ ë””í…Œì¼ í–¥ìƒ")
    
    print(f"\nğŸ“š 2. ì†ì‹¤ í•¨ìˆ˜ ê°œì„ ")
    print(f"   ğŸ¯ Wasserstein GAN (WGAN):")
    print(f"      - Earth Mover's Distance ì‚¬ìš©")
    print(f"      - ë” ì˜ë¯¸ìˆëŠ” ì†ì‹¤ê°’")
    print(f"      - ëª¨ë“œ ë¶•ê´´ ì™„í™”")
    
    print(f"\n   ğŸ¯ Least Squares GAN (LSGAN):")
    print(f"      - MSE ì†ì‹¤ ì‚¬ìš©")
    print(f"      - ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ì™„í™”")
    print(f"      - ë” ì•ˆì •ì ì¸ í›ˆë ¨")
    
    print(f"\nğŸ“š 3. ì•„í‚¤í…ì²˜ ê°œì„ ")
    print(f"   ğŸ¯ StyleGAN:")
    print(f"      - ìŠ¤íƒ€ì¼ ê¸°ë°˜ ìƒì„±")
    print(f"      - ì„¸ë°€í•œ ì œì–´ ê°€ëŠ¥")
    print(f"      - ìµœê³  í’ˆì§ˆì˜ ì–¼êµ´ ìƒì„±")
    
    print(f"\n   ğŸ¯ BigGAN:")
    print(f"      - ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ë°ì´í„°")
    print(f"      - í´ë˜ìŠ¤ ì¡°ê±´ë¶€ ìƒì„±")
    print(f"      - ImageNet ìˆ˜ì¤€ í’ˆì§ˆ")
    
    print(f"\nğŸ“š 4. ì¡°ê±´ë¶€ ìƒì„±")
    print(f"   ğŸ¯ Conditional GAN (cGAN):")
    print(f"      - ë¼ë²¨ ì¡°ê±´ë¶€ ìƒì„±")
    print(f"      - ì›í•˜ëŠ” í´ë˜ìŠ¤ ì´ë¯¸ì§€ ìƒì„±")
    print(f"      - ì œì–´ ê°€ëŠ¥í•œ ìƒì„±")
    
    print(f"\n   ğŸ¯ Pix2Pix:")
    print(f"      - ì´ë¯¸ì§€ ê°„ ë³€í™˜")
    print(f"      - ìŠ¤ì¼€ì¹˜ â†’ ì‚¬ì§„ ë³€í™˜")
    print(f"      - ë‹¤ì–‘í•œ ì‘ìš© ê°€ëŠ¥")

explain_gan_improvements()

# ============================================================================
# 11. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. GANì˜ í•µì‹¬ ê°œë…ê³¼ ì ëŒ€ì  í•™ìŠµ ì›ë¦¬")
print(f"   2. DCGAN êµ¬ì¡°ë¡œ ì–¼êµ´ ì´ë¯¸ì§€ ìƒì„±")
print(f"   3. ìƒì„±ìì™€ íŒë³„ìì˜ ê· í˜•ì¡íŒ í›ˆë ¨")
print(f"   4. ì ì¬ ê³µê°„ íƒìƒ‰ê³¼ ë³´ê°„ ê¸°ë²•")
print(f"   5. ìƒì„± í’ˆì§ˆ í‰ê°€ (IS, ëª¨ë“œ ë¶•ê´´ ë¶„ì„)")
print(f"   6. GAN ê°œì„  ê¸°ë²•ë“¤ í•™ìŠµ")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
if is_score:
    print(f"   - Inception Score: {is_score:.2f}")
if similarity_stats:
    print(f"   - í‰ê·  ì´ë¯¸ì§€ ìœ ì‚¬ë„: {similarity_stats[0]:.4f}")
print(f"   - ìƒì„±ì íŒŒë¼ë¯¸í„°: {gen_params['total_params']:,}ê°œ")
print(f"   - íŒë³„ì íŒŒë¼ë¯¸í„°: {disc_params['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ì ëŒ€ì  í•™ìŠµ: ë‘ ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìŸì„ í†µí•œ ê°œì„ ")
print(f"   2. ìƒì„± ëª¨ë¸ë§: ë°ì´í„° ë¶„í¬ í•™ìŠµê³¼ ìƒ˜í”Œë§")
print(f"   3. ì ì¬ ê³µê°„: ì—°ì†ì ì´ê³  ì˜ë¯¸ìˆëŠ” í‘œí˜„ ê³µê°„")
print(f"   4. í›ˆë ¨ ê· í˜•: ìƒì„±ìì™€ íŒë³„ìì˜ ì ì ˆí•œ ê· í˜•")
print(f"   5. í’ˆì§ˆ í‰ê°€: ì •ëŸ‰ì  ë©”íŠ¸ë¦­ê³¼ ì •ì„±ì  í‰ê°€")

print(f"\nğŸ” GANì˜ ì¥ë‹¨ì :")
print(f"   ì¥ì :")
print(f"   - ì„ ëª…í•œ ì´ë¯¸ì§€ ìƒì„± (VAE ëŒ€ë¹„)")
print(f"   - ë‹¤ì–‘í•œ ì‘ìš© ê°€ëŠ¥ (ì´ë¯¸ì§€ ë³€í™˜, ìŠ¤íƒ€ì¼ ì „ì´)")
print(f"   - ì°½ì‘ ë„êµ¬ë¡œì„œì˜ ê°€ì¹˜")
print(f"   - ë°ì´í„° ì¦ê°• íš¨ê³¼")
print(f"   ë‹¨ì :")
print(f"   - ë¶ˆì•ˆì •í•œ í›ˆë ¨ (ëª¨ë“œ ë¶•ê´´, ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤)")
print(f"   - í‰ê°€ì˜ ì–´ë ¤ì›€ (ì •ëŸ‰ì  ë©”íŠ¸ë¦­ ë¶€ì¡±)")
print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ì„±")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   - 08_transformer_nlp.py: Transformerë¡œ ìì—°ì–´ ì²˜ë¦¬")
print(f"   - Multi30k ë°ì´í„°ì…‹ìœ¼ë¡œ ê¸°ê³„ ë²ˆì—­")
print(f"   - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ì…€í”„ ì–´í…ì…˜")

print(f"\nğŸ”§ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. ì¡°ê±´ë¶€ GAN: íŠ¹ì • ì†ì„±ì„ ê°€ì§„ ì–¼êµ´ ìƒì„±")
print(f"   2. StyleGAN: ìŠ¤íƒ€ì¼ ê¸°ë°˜ ê³ í’ˆì§ˆ ìƒì„±")
print(f"   3. CycleGAN: ë„ë©”ì¸ ê°„ ì´ë¯¸ì§€ ë³€í™˜")
print(f"   4. Pix2Pix: ìŠ¤ì¼€ì¹˜ì—ì„œ ì‚¬ì§„ìœ¼ë¡œ ë³€í™˜")
print(f"   5. 3D GAN: 3ì°¨ì› ê°ì²´ ìƒì„±")

print(f"\nğŸ¯ ì‹¤ì œ ì‘ìš© ë¶„ì•¼:")
print(f"   - ì—”í„°í…Œì¸ë¨¼íŠ¸: ê°€ìƒ ìºë¦­í„°, ê²Œì„ ì—ì…‹ ìƒì„±")
print(f"   - íŒ¨ì…˜: ì˜ë¥˜ ë””ìì¸, ê°€ìƒ í”¼íŒ…")
print(f"   - ì˜ë£Œ: ì˜ë£Œ ì˜ìƒ ë°ì´í„° ì¦ê°•")
print(f"   - ì˜ˆìˆ : ë””ì§€í„¸ ì•„íŠ¸, ì°½ì‘ ë„êµ¬")
print(f"   - ë°ì´í„° í”„ë¼ì´ë²„ì‹œ: í•©ì„± ë°ì´í„° ìƒì„±")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ GAN ì´ë¯¸ì§€ ìƒì„± íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œ Transformerë¥¼ ë°°ì›Œë³´ì„¸ìš”!")
print(f"=" * 60)