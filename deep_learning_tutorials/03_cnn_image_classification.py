"""
ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 3: CNN ì´ë¯¸ì§€ ë¶„ë¥˜

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” CIFAR-10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 
í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì˜ í•µì‹¬ ê°œë…ê³¼ êµ¬í˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
1. í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì˜ êµ¬ì¡°ì™€ ì›ë¦¬ ì´í•´
2. í•©ì„±ê³±ì¸µ, í’€ë§ì¸µ, ì™„ì „ì—°ê²°ì¸µì˜ ì—­í• 
3. íŠ¹ì„± ë§µ(Feature Map) ì‹œê°í™” ë° í•´ì„
4. ë°ì´í„° ì¦ê°•ì˜ ê³ ê¸‰ ê¸°ë²•
5. ì „ì´ í•™ìŠµ(Transfer Learning) ê°œë…
6. ëª¨ë¸ ì•™ìƒë¸” ê¸°ë²•

ë°ì´í„°ì…‹ ì„ íƒ ì´ìœ  - CIFAR-10:
- 32x32 í”½ì…€ì˜ ì»¬ëŸ¬ ì´ë¯¸ì§€ (RGB 3ì±„ë„)
- 10ê°œ í´ë˜ìŠ¤: ë¹„í–‰ê¸°, ìë™ì°¨, ìƒˆ, ê³ ì–‘ì´, ì‚¬ìŠ´, ê°œ, ê°œêµ¬ë¦¬, ë§, ë°°, íŠ¸ëŸ­
- 50,000ê°œ í›ˆë ¨ + 10,000ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
- CNNì˜ íš¨ê³¼ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ëŠ” ìµœì ì˜ ë°ì´í„°ì…‹
- ì»¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ì±„ë„ ì°¨ì› ì²˜ë¦¬ í•™ìŠµ ê°€ëŠ¥
- ì ë‹¹í•œ ë³µì¡ë„ë¡œ CNN ê°œë… í•™ìŠµì— ì´ìƒì 
- ê°ì²´ì˜ í˜•íƒœ, ìƒ‰ìƒ, í…ìŠ¤ì²˜ ë“± ë‹¤ì–‘í•œ íŠ¹ì§• í¬í•¨
- ì‹¤ì œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì˜ ì¶•ì†ŒíŒìœ¼ë¡œ ì‹¤ìš©ì  í•™ìŠµ ê°€ëŠ¥

ì™œ ì´ì œ CNNì„ ì‚¬ìš©í•˜ëŠ”ê°€?
1. ì´ë¯¸ì§€ì˜ ê³µê°„ì  êµ¬ì¡° ë³´ì¡´: ì™„ì „ì—°ê²°ì¸µì€ ê³µê°„ ì •ë³´ ì†ì‹¤
2. ì§€ì—­ì  íŠ¹ì§• ì¶”ì¶œ: ì—£ì§€, ì½”ë„ˆ, í…ìŠ¤ì²˜ ë“± êµ­ì†Œ íŒ¨í„´ ê°ì§€
3. í‰í–‰ì´ë™ ë¶ˆë³€ì„±: ê°ì²´ ìœ„ì¹˜ì— ê´€ê³„ì—†ì´ ì¸ì‹ ê°€ëŠ¥
4. íŒŒë¼ë¯¸í„° ê³µìœ : ê°™ì€ í•„í„°ë¡œ ì „ì²´ ì´ë¯¸ì§€ ìŠ¤ìº”í•˜ì—¬ íš¨ìœ¨ì„± í™•ë³´
5. ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ: ì €ìˆ˜ì¤€â†’ê³ ìˆ˜ì¤€ íŠ¹ì§•ìœ¼ë¡œ ì ì§„ì  ì¶”ìƒí™”
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import time
import copy

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.data_utils import explore_dataset, visualize_samples
from utils.visualization import (plot_training_curves, plot_confusion_matrix, 
                               plot_model_predictions, plot_feature_maps)
from utils.model_utils import count_parameters, save_checkpoint, evaluate_model, compare_models

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 3: CNN ì´ë¯¸ì§€ ë¶„ë¥˜")
print("=" * 60)

# ============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}")

# CNNì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ì»¬ëŸ¬ ì´ë¯¸ì§€ì™€ ë” ë³µì¡í•œ íŒ¨í„´ìœ¼ë¡œ ì¸í•´ ì¡°ì •ëœ ì„¤ì •
BATCH_SIZE = 128       # CNNì€ ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°
LEARNING_RATE = 0.001  # Adam ì˜µí‹°ë§ˆì´ì €ì— ì í•©í•œ í•™ìŠµë¥ 
EPOCHS = 50            # CNNì€ ìˆ˜ë ´ì´ ëŠë ¤ ë” ë§ì€ ì—í¬í¬ í•„ìš”
RANDOM_SEED = 42
VALIDATION_SPLIT = 0.1

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)

print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   ì—í¬í¬: {EPOCHS}")

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°•
# ============================================================================

print(f"\nğŸ“ CIFAR-10 ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

# CIFAR-10ì„ ìœ„í•œ ê³ ê¸‰ ë°ì´í„° ì¦ê°•
# ì™œ ì´ëŸ° ì¦ê°• ê¸°ë²•ë“¤ì´ í•„ìš”í•œê°€?

# í›ˆë ¨ìš© ë³€í™˜ (ê°•ë ¥í•œ ë°ì´í„° ì¦ê°•)
train_transform = transforms.Compose([
    # RandomCrop: 32x32ì—ì„œ 4í”½ì…€ íŒ¨ë”© í›„ ë¬´ì‘ìœ„ í¬ë¡­
    # ì´ìœ : ê°ì²´ì˜ ìœ„ì¹˜ ë³€í™”ì— ëŒ€í•œ ê°•ê±´ì„± í™•ë³´
    transforms.RandomCrop(32, padding=4),
    
    # RandomHorizontalFlip: 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „
    # ì´ìœ : ëŒ€ë¶€ë¶„ì˜ ê°ì²´ëŠ” ì¢Œìš° ëŒ€ì¹­ì  íŠ¹ì„±ì„ ê°€ì§
    transforms.RandomHorizontalFlip(p=0.5),
    
    # ColorJitter: ë°ê¸°, ëŒ€ë¹„, ì±„ë„, ìƒ‰ì¡° ë¬´ì‘ìœ„ ë³€ê²½
    # ì´ìœ : ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ê³¼ ì¹´ë©”ë¼ ì„¤ì •ì— ëŒ€í•œ ê°•ê±´ì„±
    transforms.ColorJitter(
        brightness=0.2,    # ë°ê¸° Â±20% ë³€í™”
        contrast=0.2,      # ëŒ€ë¹„ Â±20% ë³€í™”
        saturation=0.2,    # ì±„ë„ Â±20% ë³€í™”
        hue=0.1           # ìƒ‰ì¡° Â±10% ë³€í™”
    ),
    
    # RandomRotation: Â±15ë„ íšŒì „
    # ì´ìœ : ì´¬ì˜ ê°ë„ ë³€í™”ì— ëŒ€í•œ ê°•ê±´ì„±
    transforms.RandomRotation(degrees=15),
    
    transforms.ToTensor(),
    
    # CIFAR-10ì˜ ì±„ë„ë³„ ì •ê·œí™”
    # RGB ê° ì±„ë„ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”
    # ì´ ê°’ë“¤ì€ ì „ì²´ CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ ê³„ì‚°ëœ í†µê³„ê°’
    transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],  # RGB ì±„ë„ë³„ í‰ê· 
        std=[0.2023, 0.1994, 0.2010]    # RGB ì±„ë„ë³„ í‘œì¤€í¸ì°¨
    )
])

# í…ŒìŠ¤íŠ¸ìš© ë³€í™˜ (ì¦ê°• ì—†ìŒ, ì •ê·œí™”ë§Œ)
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2023, 0.1994, 0.2010]
    )
])

# ë°ì´í„°ì…‹ ë¡œë“œ
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=True, 
    download=True, 
    transform=train_transform
)

test_dataset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=False, 
    download=True, 
    transform=test_transform
)

print(f"âœ… CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")

# ============================================================================
# 3. ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
# ============================================================================

print(f"\nğŸ” CIFAR-10 ë°ì´í„°ì…‹ íƒìƒ‰")

# CIFAR-10 í´ë˜ìŠ¤ ì´ë¦„
class_names = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

# ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ í™•ì¸
explore_dataset(train_dataset, "CIFAR-10 í›ˆë ¨ ë°ì´í„°ì…‹", show_samples=3)

# ì›ë³¸ ë°ì´í„° ì‹œê°í™” (ì¦ê°• ì „)
print(f"\nğŸ–¼ï¸  CIFAR-10 ì›ë³¸ ìƒ˜í”Œ ì‹œê°í™”")

# ì¦ê°• ì—†ëŠ” ë³€í™˜ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° í™•ì¸
original_transform = transforms.Compose([transforms.ToTensor()])
original_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, transform=original_transform
)

visualize_samples(
    original_dataset, 
    num_samples=20, 
    num_cols=5,
    title="CIFAR-10 ì›ë³¸ ë°ì´í„° ìƒ˜í”Œ",
    class_names=class_names
)

# CIFAR-10 vs ì´ì „ ë°ì´í„°ì…‹ ë¹„êµ
print(f"\nğŸ“ˆ CIFAR-10ì˜ íŠ¹ì§• ë° ë„ì „ê³¼ì œ:")
print(f"   1. ì»¬ëŸ¬ ì´ë¯¸ì§€ (3ì±„ë„): RGB ì •ë³´ í™œìš© í•„ìš”")
print(f"   2. ë‚®ì€ í•´ìƒë„ (32x32): ì œí•œëœ í”½ì…€ë¡œ ê°ì²´ ì¸ì‹")
print(f"   3. í´ë˜ìŠ¤ ë‚´ ë‹¤ì–‘ì„±: ê°™ì€ í´ë˜ìŠ¤ë„ ë‹¤ì–‘í•œ í˜•íƒœ/ìƒ‰ìƒ")
print(f"   4. í´ë˜ìŠ¤ ê°„ ìœ ì‚¬ì„±: ê°œì™€ ê³ ì–‘ì´, ìë™ì°¨ì™€ íŠ¸ëŸ­ ë“±")
print(f"   5. ë°°ê²½ ë³µì¡ì„±: ë‹¨ìˆœí•œ ë°°ê²½ì´ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ ì¥ë©´")
print(f"   â†’ CNNì˜ ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥ì´ í•„ìˆ˜ì ")

# ============================================================================
# 4. ë°ì´í„° ë¡œë” ìƒì„±
# ============================================================================

print(f"\nğŸ“¦ ë°ì´í„° ë¶„í•  ë° ë¡œë” ìƒì„±")

# í›ˆë ¨/ê²€ì¦ ë¶„í• 
train_size = int((1 - VALIDATION_SPLIT) * len(train_dataset))
val_size = len(train_dataset) - train_size

train_subset, val_subset = torch.utils.data.random_split(
    train_dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(RANDOM_SEED)
)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
print(f"   í›ˆë ¨: {len(train_subset):,}ê°œ")
print(f"   ê²€ì¦: {len(val_subset):,}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸: {len(test_dataset):,}ê°œ")

# ============================================================================
# 5. CNN ëª¨ë¸ ì •ì˜
# ============================================================================

print(f"\nğŸ§  CNN ëª¨ë¸ ì •ì˜")

class SimpleCNN(nn.Module):
    """
    ê¸°ë³¸ CNN ëª¨ë¸
    
    êµ¬ì¡°:
    - Conv Block 1: Conv2d(3â†’32) â†’ BatchNorm â†’ ReLU â†’ MaxPool
    - Conv Block 2: Conv2d(32â†’64) â†’ BatchNorm â†’ ReLU â†’ MaxPool  
    - Conv Block 3: Conv2d(64â†’128) â†’ BatchNorm â†’ ReLU â†’ MaxPool
    - Classifier: Flatten â†’ FC(128*4*4â†’512) â†’ Dropout â†’ FC(512â†’10)
    
    ì™œ ì´ëŸ° êµ¬ì¡°ì¸ê°€?
    1. ì ì§„ì  ì±„ë„ ì¦ê°€: ì €ìˆ˜ì¤€â†’ê³ ìˆ˜ì¤€ íŠ¹ì§•ìœ¼ë¡œ ë³µì¡ë„ ì¦ê°€
    2. ê³µê°„ ì°¨ì› ê°ì†Œ: MaxPoolë¡œ ê³„ì‚°ëŸ‰ ì¤„ì´ê³  ìˆ˜ìš© ì˜ì—­ í™•ëŒ€
    3. ë°°ì¹˜ ì •ê·œí™”: ê° ì¸µì˜ ì…ë ¥ ë¶„í¬ ì•ˆì •í™”
    4. ë“œë¡­ì•„ì›ƒ: ê³¼ì í•© ë°©ì§€
    """
    
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        
        # ì²« ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡
        # ì…ë ¥: (3, 32, 32) â†’ ì¶œë ¥: (32, 16, 16)
        self.conv1 = nn.Conv2d(
            in_channels=3,      # RGB 3ì±„ë„
            out_channels=32,    # 32ê°œ íŠ¹ì„± ë§µ ìƒì„±
            kernel_size=3,      # 3x3 í•„í„° (ê°€ì¥ ì¼ë°˜ì )
            padding=1          # íŒ¨ë”©ìœ¼ë¡œ í¬ê¸° ìœ ì§€
        )
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # í¬ê¸° ì ˆë°˜ìœ¼ë¡œ ì¶•ì†Œ
        
        # ë‘ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡
        # ì…ë ¥: (32, 16, 16) â†’ ì¶œë ¥: (64, 8, 8)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ì„¸ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡
        # ì…ë ¥: (64, 8, 8) â†’ ì¶œë ¥: (128, 4, 4)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ë¶„ë¥˜ê¸° (Classifier)
        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # íŠ¹ì„± ë§µì„ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹¨
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(512, num_classes)
    
    def forward(self, x):
        # ì²« ë²ˆì§¸ ë¸”ë¡: Conv â†’ BN â†’ ReLU â†’ Pool
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))
        
        # íŠ¹ì„± ë§µì„ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°
        # (batch_size, 128, 4, 4) â†’ (batch_size, 128*4*4)
        x = x.view(x.size(0), -1)
        
        # ë¶„ë¥˜ê¸°
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

class AdvancedCNN(nn.Module):
    """
    ê³ ê¸‰ CNN ëª¨ë¸ (ResNet ìŠ¤íƒ€ì¼ì˜ ì”ì°¨ ì—°ê²° í¬í•¨)
    
    ê°œì„ ì‚¬í•­:
    1. ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ (5ê°œ í•©ì„±ê³± ë¸”ë¡)
    2. ì”ì°¨ ì—°ê²° (Residual Connection)
    3. ì ì‘ì  í‰ê·  í’€ë§ (Adaptive Average Pooling)
    4. ë” ì •êµí•œ ì •ê·œí™”
    """
    
    def __init__(self, num_classes=10):
        super(AdvancedCNN, self).__init__()
        
        # ì´ˆê¸° í•©ì„±ê³±ì¸µ
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        
        # í•©ì„±ê³± ë¸”ë¡ë“¤
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        
        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(128)
        
        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn5 = nn.BatchNorm2d(256)
        
        # í’€ë§ì¸µë“¤
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ì ì‘ì  í‰ê·  í’€ë§ (ì…ë ¥ í¬ê¸°ì— ê´€ê³„ì—†ì´ ê³ ì • ì¶œë ¥ í¬ê¸°)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # ë¶„ë¥˜ê¸°
        self.fc1 = nn.Linear(256, 512)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(256, num_classes)
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ 1x1 í•©ì„±ê³±
        self.shortcut1 = nn.Conv2d(64, 128, kernel_size=1)
        self.shortcut2 = nn.Conv2d(128, 256, kernel_size=1)
    
    def forward(self, x):
        # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ
        x = F.relu(self.bn1(self.conv1(x)))
        
        # ì²« ë²ˆì§¸ ë¸”ë¡ (ì”ì°¨ ì—°ê²°)
        identity = x
        x = F.relu(self.bn2(self.conv2(x)))
        x = x + identity  # ì”ì°¨ ì—°ê²°
        x = self.pool(x)
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡ (ì±„ë„ ìˆ˜ ì¦ê°€)
        identity = self.shortcut1(x)
        x = F.relu(self.bn3(self.conv3(x)))
        x = x + identity
        x = self.pool(x)
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡
        identity = x
        x = F.relu(self.bn4(self.conv4(x)))
        x = x + identity
        
        # ë„¤ ë²ˆì§¸ ë¸”ë¡ (ì±„ë„ ìˆ˜ ì¦ê°€)
        identity = self.shortcut2(x)
        x = F.relu(self.bn5(self.conv5(x)))
        x = x + identity
        x = self.pool(x)
        
        # ì ì‘ì  í’€ë§
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        
        # ë¶„ë¥˜ê¸°
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        
        return x

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
simple_cnn = SimpleCNN(num_classes=10).to(device)
advanced_cnn = AdvancedCNN(num_classes=10).to(device)

print(f"âœ… CNN ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
print(f"\nğŸ“‹ ê°„ë‹¨í•œ CNN êµ¬ì¡°:")
print(simple_cnn)

# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
print(f"\nğŸ“Š ëª¨ë¸ ë³µì¡ë„ ë¹„êµ:")
simple_params = count_parameters(simple_cnn, detailed=False)
advanced_params = count_parameters(advanced_cnn, detailed=False)

print(f"   ê°„ë‹¨í•œ CNN: {simple_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")
print(f"   ê³ ê¸‰ CNN: {advanced_params['total_params']:,}ê°œ íŒŒë¼ë¯¸í„°")

# ============================================================================
# 6. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
# ============================================================================

print(f"\nâš™ï¸  ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")

# ì†ì‹¤ í•¨ìˆ˜
criterion = nn.CrossEntropyLoss()

# ì˜µí‹°ë§ˆì´ì € (ê³ ê¸‰ CNNìš©)
optimizer = optim.Adam(
    advanced_cnn.parameters(), 
    lr=LEARNING_RATE,
    weight_decay=1e-4
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì½”ì‚¬ì¸ ì–´ë‹ë§)
# ì™œ ì½”ì‚¬ì¸ ì–´ë‹ë§ì¸ê°€?
# 1. ë¶€ë“œëŸ¬ìš´ í•™ìŠµë¥  ê°ì†Œë¡œ ì•ˆì •ì ì¸ ìˆ˜ë ´
# 2. ì£¼ê¸°ì  ì¬ì‹œì‘ìœ¼ë¡œ ì§€ì—­ ìµœì†Ÿê°’ íƒˆì¶œ
# 3. CNN í›ˆë ¨ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ ì…ì¦
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=EPOCHS,      # ì „ì²´ ì—í¬í¬ ìˆ˜
    eta_min=1e-6       # ìµœì†Œ í•™ìŠµë¥ 
)

print(f"   ì†ì‹¤ í•¨ìˆ˜: {criterion.__class__.__name__}")
print(f"   ì˜µí‹°ë§ˆì´ì €: {optimizer.__class__.__name__}")
print(f"   ìŠ¤ì¼€ì¤„ëŸ¬: CosineAnnealingLR")

# ============================================================================
# 7. í›ˆë ¨ í•¨ìˆ˜ ì •ì˜
# ============================================================================

def train_epoch_cnn(model, train_loader, criterion, optimizer, device, epoch):
    """CNNì„ ìœ„í•œ í›ˆë ¨ í•¨ìˆ˜"""
    model.train()
    
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    pbar = tqdm(train_loader, desc=f"ì—í¬í¬ {epoch+1} í›ˆë ¨")
    
    for batch_idx, (data, targets) in enumerate(pbar):
        data, targets = data.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (CNNì—ì„œ ì¤‘ìš”)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss += loss.item()
        predictions = torch.argmax(outputs, dim=1)
        correct_predictions += (predictions == targets).sum().item()
        total_samples += targets.size(0)
        
        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        if batch_idx % 50 == 0:  # 50ë°°ì¹˜ë§ˆë‹¤ ì—…ë°ì´íŠ¸
            current_accuracy = correct_predictions / total_samples
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_accuracy:.4f}',
                'LR': f'{current_lr:.6f}'
            })
    
    avg_loss = running_loss / len(train_loader)
    accuracy = correct_predictions / total_samples
    
    return avg_loss, accuracy

def validate_epoch_cnn(model, val_loader, criterion, device):
    """CNNì„ ìœ„í•œ ê²€ì¦ í•¨ìˆ˜"""
    model.eval()
    
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="ê²€ì¦")
        
        for data, targets in pbar:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            running_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            correct_predictions += (predictions == targets).sum().item()
            total_samples += targets.size(0)
            
            current_accuracy = correct_predictions / total_samples
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_accuracy:.4f}'
            })
    
    avg_loss = running_loss / len(val_loader)
    accuracy = correct_predictions / total_samples
    
    return avg_loss, accuracy

# ============================================================================
# 8. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ CNN ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

# í›ˆë ¨ ê¸°ë¡
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []
learning_rates = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì 
best_val_accuracy = 0.0
best_model_state = None
patience = 10
patience_counter = 0

start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í˜„ì¬ í•™ìŠµë¥  ê¸°ë¡
    current_lr = optimizer.param_groups[0]['lr']
    learning_rates.append(current_lr)
    
    # í›ˆë ¨
    train_loss, train_acc = train_epoch_cnn(
        advanced_cnn, train_loader, criterion, optimizer, device, epoch
    )
    
    # ê²€ì¦
    val_loss, val_acc = validate_epoch_cnn(
        advanced_cnn, val_loader, criterion, device
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler.step()
    
    # ê¸°ë¡ ì €ì¥
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   í›ˆë ¨ - ì†ì‹¤: {train_loss:.4f}, ì •í™•ë„: {train_acc:.4f}")
    print(f"   ê²€ì¦ - ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_acc:.4f}")
    print(f"   í•™ìŠµë¥ : {current_lr:.6f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if val_acc > best_val_accuracy:
        best_val_accuracy = val_acc
        best_model_state = copy.deepcopy(advanced_cnn.state_dict())
        patience_counter = 0
        print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(
            advanced_cnn, optimizer, epoch, val_loss, val_acc,
            save_path="./checkpoints/cifar10_cnn_best_model.pth"
        )
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"   â° ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
            break

training_time = time.time() - start_time
print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f}")

# ============================================================================
# 9. íŠ¹ì„± ë§µ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ” CNN íŠ¹ì„± ë§µ ì‹œê°í™”")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
if best_model_state is not None:
    advanced_cnn.load_state_dict(best_model_state)

# ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ íŠ¹ì„± ë§µ ì‹œê°í™”
sample_batch = next(iter(test_loader))
sample_image = sample_batch[0][0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€

print(f"ğŸ“Š ì²« ë²ˆì§¸ í•©ì„±ê³±ì¸µ íŠ¹ì„± ë§µ:")
plot_feature_maps(
    model=advanced_cnn,
    input_tensor=sample_image,
    layer_name="conv1",
    num_maps=16,
    title="ì²« ë²ˆì§¸ í•©ì„±ê³±ì¸µ íŠ¹ì„± ë§µ (ì €ìˆ˜ì¤€ íŠ¹ì§•)"
)

print(f"ğŸ“Š ì„¸ ë²ˆì§¸ í•©ì„±ê³±ì¸µ íŠ¹ì„± ë§µ:")
plot_feature_maps(
    model=advanced_cnn,
    input_tensor=sample_image,
    layer_name="conv3",
    num_maps=16,
    title="ì„¸ ë²ˆì§¸ í•©ì„±ê³±ì¸µ íŠ¹ì„± ë§µ (ì¤‘ê°„ ìˆ˜ì¤€ íŠ¹ì§•)"
)

# ============================================================================
# 10. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# í›ˆë ¨ ê³¡ì„ 
plot_training_curves(
    train_losses=train_losses,
    val_losses=val_losses,
    train_accuracies=train_accuracies,
    val_accuracies=val_accuracies,
    title="CIFAR-10 CNN ë¶„ë¥˜ - í›ˆë ¨ ê³¼ì •"
)

# ============================================================================
# 11. ëª¨ë¸ ë¹„êµ ì‹¤í—˜
# ============================================================================

print(f"\nğŸ† CNN vs MLP ì„±ëŠ¥ ë¹„êµ")

# ë¹„êµë¥¼ ìœ„í•œ MLP ëª¨ë¸ (ì´ì „ íŠœí† ë¦¬ì–¼ ìŠ¤íƒ€ì¼)
class MLP_for_CIFAR(nn.Module):
    def __init__(self):
        super(MLP_for_CIFAR, self).__init__()
        self.fc1 = nn.Linear(32 * 32 * 3, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)  # í‰íƒ„í™”
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)
        return x

# MLP ëª¨ë¸ ë¹ ë¥¸ í›ˆë ¨ (ë¹„êµìš©)
mlp_model = MLP_for_CIFAR().to(device)
mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=LEARNING_RATE)

print(f"ğŸ“Š MLP ëª¨ë¸ ë¹ ë¥¸ í›ˆë ¨ ì¤‘... (5 ì—í¬í¬)")
for epoch in range(5):
    mlp_model.train()
    for batch_idx, (data, targets) in enumerate(train_loader):
        if batch_idx > 100:  # ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ ì œí•œ
            break
        data, targets = data.to(device), targets.to(device)
        mlp_optimizer.zero_grad()
        outputs = mlp_model(data)
        loss = criterion(outputs, targets)
        loss.backward()
        mlp_optimizer.step()

# ëª¨ë¸ ë¹„êµ
models_to_compare = {
    "ê³ ê¸‰ CNN (ì”ì°¨ ì—°ê²°)": advanced_cnn,
    "ê°„ë‹¨í•œ CNN": simple_cnn,
    "MLP (ì™„ì „ì—°ê²°ì¸µë§Œ)": mlp_model
}

comparison_results = compare_models(
    models=models_to_compare,
    test_dataloader=test_loader,
    criterion=criterion,
    device=device
)

# ============================================================================
# 12. ìµœì¢… í‰ê°€ ë° ì‹œê°í™”
# ============================================================================

print(f"\nğŸ¯ ìµœì¢… CNN ëª¨ë¸ í‰ê°€")

# ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€
final_results = evaluate_model(
    model=advanced_cnn,
    dataloader=test_loader,
    criterion=criterion,
    device=device,
    num_classes=10
)

# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
print(f"\nğŸ–¼ï¸  CNN ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”")
plot_model_predictions(
    model=advanced_cnn,
    dataloader=test_loader,
    class_names=class_names,
    num_samples=16,
    device=device,
    title="CIFAR-10 CNN ë¶„ë¥˜ - ì˜ˆì¸¡ ê²°ê³¼"
)

# í˜¼ë™ í–‰ë ¬ ìƒì„±
print(f"\nğŸ“Š í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")

all_predictions = []
all_targets = []

advanced_cnn.eval()
with torch.no_grad():
    for data, targets in tqdm(test_loader, desc="ì˜ˆì¸¡ ìˆ˜ì§‘"):
        data, targets = data.to(device), targets.to(device)
        outputs = advanced_cnn(data)
        predictions = torch.argmax(outputs, dim=1)
        
        all_predictions.extend(predictions.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())

plot_confusion_matrix(
    y_true=np.array(all_targets),
    y_pred=np.array(all_predictions),
    class_names=class_names,
    title="CIFAR-10 CNN ë¶„ë¥˜ - í˜¼ë™ í–‰ë ¬"
)

# ============================================================================
# 13. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. CNNì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ (Conv, Pool, FC) ì´í•´")
print(f"   2. CIFAR-10 ì»¬ëŸ¬ ì´ë¯¸ì§€ ë¶„ë¥˜ êµ¬í˜„")
print(f"   3. íŠ¹ì„± ë§µ ì‹œê°í™”ë¡œ CNN ë™ì‘ ì›ë¦¬ í™•ì¸")
print(f"   4. ì”ì°¨ ì—°ê²°ê³¼ ê³ ê¸‰ CNN ê¸°ë²• ì ìš©")
print(f"   5. CNN vs MLP ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
print(f"   - CNN ìµœê³  ì •í™•ë„: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)")
print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {advanced_params['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ê³µê°„ì  êµ¬ì¡° ë³´ì¡´: CNNì´ ì´ë¯¸ì§€ì— ì í•©í•œ ì´ìœ ")
print(f"   2. ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ: ì €ìˆ˜ì¤€â†’ê³ ìˆ˜ì¤€ íŠ¹ì§• ì¶”ì¶œ")
print(f"   3. íŒŒë¼ë¯¸í„° ê³µìœ : íš¨ìœ¨ì ì¸ íŠ¹ì§• ê°ì§€")
print(f"   4. í‰í–‰ì´ë™ ë¶ˆë³€ì„±: ìœ„ì¹˜ì— ê´€ê³„ì—†ëŠ” ê°ì²´ ì¸ì‹")
print(f"   5. ë°ì´í„° ì¦ê°•ì˜ ì¤‘ìš”ì„±: ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")

print(f"\nğŸ” CNN vs MLP ë¹„êµ ë¶„ì„:")
print(f"   - CNN: ê³µê°„ì  êµ¬ì¡° í™œìš©ìœ¼ë¡œ ë” ë†’ì€ ì„±ëŠ¥")
print(f"   - MLP: ê³µê°„ ì •ë³´ ì†ì‹¤ë¡œ ì œí•œì  ì„±ëŠ¥")
print(f"   - íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±: CNNì´ ë” ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥")
print(f"   - í•´ì„ ê°€ëŠ¥ì„±: CNNì˜ íŠ¹ì„± ë§µìœ¼ë¡œ í•™ìŠµ ê³¼ì • ì‹œê°í™” ê°€ëŠ¥")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   - 04_rnn_text_classification.py: ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬")
print(f"   - IMDB ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„")
print(f"   - ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ê¸°ë²• í•™ìŠµ")

print(f"\nğŸ”§ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. ì „ì´ í•™ìŠµ: ì‚¬ì „ í›ˆë ¨ëœ ResNet, VGG í™œìš©")
print(f"   2. ë‹¤ì–‘í•œ CNN ì•„í‚¤í…ì²˜: DenseNet, EfficientNet")
print(f"   3. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜: ì¤‘ìš”í•œ ì˜ì—­ì— ì§‘ì¤‘")
print(f"   4. ëª¨ë¸ ì••ì¶•: Pruning, Quantization")
print(f"   5. ì•™ìƒë¸” ê¸°ë²•: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ CNN ì´ë¯¸ì§€ ë¶„ë¥˜ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œ RNNì„ ë°°ì›Œë³´ì„¸ìš”!")
print(f"=" * 60)