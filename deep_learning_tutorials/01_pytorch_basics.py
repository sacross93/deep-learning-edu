"""
ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 1: PyTorch ê¸°ì´ˆ

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” PyTorchì˜ ê¸°ë³¸ ê°œë…ê³¼ MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ 
ê°„ë‹¨í•œ ì‹ ê²½ë§ êµ¬í˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.

í•™ìŠµ ëª©í‘œ:
1. PyTorch í…ì„œ ì—°ì‚°ì˜ ê¸°ë³¸ ì´í•´
2. ìë™ ë¯¸ë¶„(autograd) ì‹œìŠ¤í…œ ì´í•´  
3. ë°ì´í„° ë¡œë”ì™€ ë°ì´í„°ì…‹ ì‚¬ìš©ë²•
4. ê¸°ë³¸ ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„
5. í›ˆë ¨ ë£¨í”„ êµ¬í˜„

ë°ì´í„°ì…‹ ì„ íƒ ì´ìœ  - MNIST:
- 28x28 í”½ì…€ì˜ í‘ë°± ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€ (0-9)
- 60,000ê°œ í›ˆë ¨ + 10,000ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
- ë”¥ëŸ¬ë‹ ì…ë¬¸ìë¥¼ ìœ„í•œ "Hello World" ë°ì´í„°ì…‹
- ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ PyTorch ê¸°ë³¸ ê°œë… í•™ìŠµì— ìµœì 
- ë¹ ë¥¸ í›ˆë ¨ ì‹œê°„ìœ¼ë¡œ ì¦‰ê°ì ì¸ í”¼ë“œë°± ê°€ëŠ¥
- ì‹œê°í™”ê°€ ì‰¬ì›Œ ê²°ê³¼ í•´ì„ì´ ì§ê´€ì 
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import time

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from utils.data_utils import explore_dataset, visualize_samples, get_data_statistics
from utils.visualization import plot_training_curves, plot_confusion_matrix, plot_model_predictions
from utils.model_utils import count_parameters, save_checkpoint, evaluate_model

print("ğŸš€ ë”¥ëŸ¬ë‹ ê°•ì˜ ì‹œë¦¬ì¦ˆ 1: PyTorch ê¸°ì´ˆ")
print("=" * 60)

# ============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ============================================================================

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}")

if device.type == 'cuda':
    print(f"   GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
    print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
# ì´ ê°’ë“¤ì€ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ê°ê°ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤
BATCH_SIZE = 64        # ë°°ì¹˜ í¬ê¸°: í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜
                       # ì‘ìœ¼ë©´ ë©”ëª¨ë¦¬ ì ˆì•½, í¬ë©´ ì•ˆì •ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸
LEARNING_RATE = 0.001  # í•™ìŠµë¥ : ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ í¬ê¸°
                       # ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµ ì†ë„ ì €í•˜
EPOCHS = 10            # ì—í¬í¬: ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí• ì§€
RANDOM_SEED = 42       # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ëœë¤ ì‹œë“œ

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)

print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   ì—í¬í¬: {EPOCHS}")

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”©
# ============================================================================

print(f"\nğŸ“ MNIST ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

# ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
# ì™œ ì´ëŸ° ì „ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?
transform = transforms.Compose([
    # ToTensor(): PIL Imageë‚˜ numpy arrayë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜
    # ë™ì‹œì— [0, 255] ë²”ìœ„ì˜ í”½ì…€ê°’ì„ [0.0, 1.0] ë²”ìœ„ë¡œ ì •ê·œí™”
    # ì´ìœ : ì‹ ê²½ë§ì€ ì‘ì€ ê°’ì—ì„œ ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë¨
    transforms.ToTensor(),
    
    # Normalize(): í‰ê·  0.1307, í‘œì¤€í¸ì°¨ 0.3081ë¡œ ì •ê·œí™”
    # ì´ ê°’ë“¤ì€ MNIST ë°ì´í„°ì…‹ì˜ ì‹¤ì œ í†µê³„ê°’
    # ì´ìœ : ì •ê·œí™”ëœ ë°ì´í„°ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ë°œ ë¬¸ì œë¥¼ ì™„í™”í•˜ê³ 
    #       ë” ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•¨
    transforms.Normalize((0.1307,), (0.3081,))
])

# í›ˆë ¨ ë°ì´í„°ì…‹ ë¡œë“œ
# download=True: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
# train=True: í›ˆë ¨ìš© ë°ì´í„°ì…‹ (60,000ê°œ)
train_dataset = torchvision.datasets.MNIST(
    root='./data', 
    train=True, 
    download=True, 
    transform=transform
)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ  
# train=False: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ (10,000ê°œ)
test_dataset = torchvision.datasets.MNIST(
    root='./data', 
    train=False, 
    download=True, 
    transform=transform
)

print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")

# ============================================================================
# 3. ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
# ============================================================================

print(f"\nğŸ” ë°ì´í„°ì…‹ íƒìƒ‰")

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¡œ ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ í™•ì¸
explore_dataset(train_dataset, "MNIST í›ˆë ¨ ë°ì´í„°ì…‹", show_samples=3)
explore_dataset(test_dataset, "MNIST í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹", show_samples=3)

# MNIST í´ë˜ìŠ¤ ì´ë¦„ (0-9 ìˆ«ì)
class_names = [str(i) for i in range(10)]

# ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”
print(f"\nğŸ–¼ï¸  í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ ì‹œê°í™”")
visualize_samples(
    train_dataset, 
    num_samples=12, 
    num_cols=4,
    title="MNIST í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ",
    class_names=class_names
)

# ============================================================================
# 4. ë°ì´í„° ë¡œë” ìƒì„±
# ============================================================================

print(f"\nğŸ“¦ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")

# ë°ì´í„° ë¡œë” ìƒì„±
# ì™œ ë°ì´í„° ë¡œë”ê°€ í•„ìš”í•œê°€?
# 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•Šê³  ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
# 2. ë³‘ë ¬ ì²˜ë¦¬: num_workersë¡œ ë°ì´í„° ë¡œë”©ì„ ë³‘ë ¬í™”í•˜ì—¬ ì†ë„ í–¥ìƒ
# 3. ì…”í”Œë§: ë§¤ ì—í¬í¬ë§ˆë‹¤ ë°ì´í„° ìˆœì„œë¥¼ ë°”ê¿” ëª¨ë¸ì´ ìˆœì„œì— ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ í•¨

train_loader = DataLoader(
    train_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=True,      # í›ˆë ¨ ì‹œì—ëŠ” ì…”í”Œë§ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
    num_workers=2      # ë³‘ë ¬ ë°ì´í„° ë¡œë”© (CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •)
)

test_loader = DataLoader(
    test_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=False,     # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì…”í”Œë§ ë¶ˆí•„ìš”
    num_workers=2
)

print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ")
print(f"   í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
print(f"   í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}")

# ë°ì´í„° ë¡œë” ë™ì‘ í™•ì¸
sample_batch = next(iter(train_loader))
sample_images, sample_labels = sample_batch
print(f"   ë°°ì¹˜ ì´ë¯¸ì§€ í¬ê¸°: {sample_images.shape}")  # [batch_size, channels, height, width]
print(f"   ë°°ì¹˜ ë¼ë²¨ í¬ê¸°: {sample_labels.shape}")    # [batch_size]

# ============================================================================
# 5. ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
# ============================================================================

print(f"\nğŸ§  ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜")

class SimpleNN(nn.Module):
    """
    ê°„ë‹¨í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (Multi-Layer Perceptron, MLP)
    
    êµ¬ì¡°:
    - ì…ë ¥ì¸µ: 28x28 = 784ê°œ ë‰´ëŸ° (MNIST ì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹¨)
    - ì€ë‹‰ì¸µ 1: 128ê°œ ë‰´ëŸ° + ReLU í™œì„±í™” í•¨ìˆ˜
    - ì€ë‹‰ì¸µ 2: 64ê°œ ë‰´ëŸ° + ReLU í™œì„±í™” í•¨ìˆ˜  
    - ì¶œë ¥ì¸µ: 10ê°œ ë‰´ëŸ° (0-9 ìˆ«ì í´ë˜ìŠ¤)
    
    ì™œ ì´ëŸ° êµ¬ì¡°ë¥¼ ì„ íƒí–ˆëŠ”ê°€?
    1. ì™„ì „ì—°ê²°ì¸µ(Linear): ëª¨ë“  ì…ë ¥ì´ ëª¨ë“  ì¶œë ¥ì— ì—°ê²°ë˜ì–´ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
    2. ReLU í™œì„±í™”: ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ ì™„í™”, ê³„ì‚° íš¨ìœ¨ì„±
    3. ì ì§„ì  ì°¨ì› ì¶•ì†Œ: 784 â†’ 128 â†’ 64 â†’ 10ìœ¼ë¡œ ì •ë³´ë¥¼ ì••ì¶•í•˜ë©° íŠ¹ì§• ì¶”ì¶œ
    """
    
    def __init__(self):
        super(SimpleNN, self).__init__()
        
        # ì™„ì „ì—°ê²°ì¸µ ì •ì˜
        # nn.Linear(ì…ë ¥_í¬ê¸°, ì¶œë ¥_í¬ê¸°)
        self.fc1 = nn.Linear(28 * 28, 128)  # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
        self.fc2 = nn.Linear(128, 64)       # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ  
        self.fc3 = nn.Linear(64, 10)        # ì¶œë ¥ì¸µ
        
        # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ (ê³¼ì í•© ë°©ì§€)
        # í›ˆë ¨ ì‹œ ì¼ì • ë¹„ìœ¨ì˜ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë¹„í™œì„±í™”
        # ì´ìœ : ëª¨ë¸ì´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€
        self.dropout = nn.Dropout(0.2)  # 20% ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë¹„í™œì„±í™”
    
    def forward(self, x):
        """
        ìˆœì „íŒŒ(forward pass) ì •ì˜
        
        Args:
            x: ì…ë ¥ í…ì„œ [batch_size, 1, 28, 28]
        
        Returns:
            ì¶œë ¥ í…ì„œ [batch_size, 10] (ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ë¡œì§“)
        """
        
        # 1. ì…ë ¥ ì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°
        # [batch_size, 1, 28, 28] â†’ [batch_size, 784]
        # ì´ìœ : ì™„ì „ì—°ê²°ì¸µì€ 1ì°¨ì› ì…ë ¥ì„ ë°›ê¸° ë•Œë¬¸
        x = x.view(x.size(0), -1)  # -1ì€ ìë™ìœ¼ë¡œ í¬ê¸° ê³„ì‚°
        
        # 2. ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ + ReLU í™œì„±í™”
        # ReLU(x) = max(0, x): ìŒìˆ˜ëŠ” 0, ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ
        # ì´ìœ : ë¹„ì„ í˜•ì„± ë„ì…ìœ¼ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
        x = F.relu(self.fc1(x))
        
        # 3. ë“œë¡­ì•„ì›ƒ ì ìš© (í›ˆë ¨ ì‹œì—ë§Œ)
        x = self.dropout(x)
        
        # 4. ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ + ReLU í™œì„±í™”
        x = F.relu(self.fc2(x))
        
        # 5. ë“œë¡­ì•„ì›ƒ ì ìš©
        x = self.dropout(x)
        
        # 6. ì¶œë ¥ì¸µ (í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ)
        # ì´ìœ : CrossEntropyLossê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•˜ë¯€ë¡œ
        x = self.fc3(x)
        
        return x

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¥ì¹˜ë¡œ ì´ë™
model = SimpleNN().to(device)

print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
print(f"\nğŸ“‹ ëª¨ë¸ êµ¬ì¡°:")
print(model)

# ìš°ë¦¬ê°€ ë§Œë“  ìœ í‹¸ë¦¬í‹°ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„
param_info = count_parameters(model, detailed=True)

# ============================================================================
# 6. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
# ============================================================================

print(f"\nâš™ï¸  ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")

# ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss
# ì™œ CrossEntropyLossë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?
# 1. ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì— ìµœì í™”ë¨
# 2. ì†Œí”„íŠ¸ë§¥ìŠ¤ + ìŒì˜ ë¡œê·¸ ìš°ë„ë¥¼ ê²°í•©í•œ íš¨ìœ¨ì ì¸ êµ¬í˜„
# 3. í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ì—¬ ì§ê´€ì ì¸ í•´ì„ ê°€ëŠ¥
# 4. ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì˜ ì „íŒŒë˜ì–´ ì•ˆì •ì ì¸ í•™ìŠµ
criterion = nn.CrossEntropyLoss()

# ì˜µí‹°ë§ˆì´ì €: Adam
# ì™œ Adamì„ ì‚¬ìš©í•˜ëŠ”ê°€?
# 1. ì ì‘ì  í•™ìŠµë¥ : ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ë‹¤ë¥¸ í•™ìŠµë¥  ì ìš©
# 2. ëª¨ë©˜í…€ íš¨ê³¼: ì´ì „ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§„ë™ ê°ì†Œ
# 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€
# 4. ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print(f"   ì†ì‹¤ í•¨ìˆ˜: {criterion.__class__.__name__}")
print(f"   ì˜µí‹°ë§ˆì´ì €: {optimizer.__class__.__name__}")
print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")

# ============================================================================
# 7. í›ˆë ¨ í•¨ìˆ˜ ì •ì˜
# ============================================================================

def train_epoch(model, train_loader, criterion, optimizer, device):
    """
    í•œ ì—í¬ï¿½ï¿½ ë™ì•ˆ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
    
    Args:
        model: í›ˆë ¨í•  ëª¨ë¸
        train_loader: í›ˆë ¨ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ì—°ì‚° ì¥ì¹˜
    
    Returns:
        tuple: (í‰ê·  ì†ì‹¤, ì •í™•ë„)
    
    í›ˆë ¨ ê³¼ì •ì˜ ê° ë‹¨ê³„:
    1. ìˆœì „íŒŒ: ì…ë ¥ â†’ ì˜ˆì¸¡
    2. ì†ì‹¤ ê³„ì‚°: ì˜ˆì¸¡ vs ì‹¤ì œ
    3. ì—­ì „íŒŒ: ì†ì‹¤ â†’ ê·¸ë˜ë””ì–¸íŠ¸
    4. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: ê·¸ë˜ë””ì–¸íŠ¸ â†’ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜
    """
    
    model.train()  # í›ˆë ¨ ëª¨ë“œ ì„¤ì • (ë“œë¡­ì•„ì›ƒ, ë°°ì¹˜ ì •ê·œí™” í™œì„±í™”)
    
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
    pbar = tqdm(train_loader, desc="í›ˆë ¨ ì¤‘")
    
    for batch_idx, (data, targets) in enumerate(pbar):
        # ë°ì´í„°ë¥¼ ì¥ì¹˜ë¡œ ì´ë™
        data, targets = data.to(device), targets.to(device)
        
        # 1. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        # ì´ìœ : PyTorchëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ë¯€ë¡œ ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì´ˆê¸°í™” í•„ìš”
        optimizer.zero_grad()
        
        # 2. ìˆœì „íŒŒ (Forward pass)
        outputs = model(data)
        
        # 3. ì†ì‹¤ ê³„ì‚°
        loss = criterion(outputs, targets)
        
        # 4. ì—­ì „íŒŒ (Backward pass)
        # ì†ì‹¤ì— ëŒ€í•œ ê° íŒŒë¼ë¯¸í„°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        loss.backward()
        
        # 5. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        # ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss += loss.item()
        
        # ì •í™•ë„ ê³„ì‚°
        predictions = torch.argmax(outputs, dim=1)
        correct_predictions += (predictions == targets).sum().item()
        total_samples += targets.size(0)
        
        # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
        current_accuracy = correct_predictions / total_samples
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{current_accuracy:.4f}'
        })
    
    # ì—í¬í¬ í‰ê·  ê³„ì‚°
    avg_loss = running_loss / len(train_loader)
    accuracy = correct_predictions / total_samples
    
    return avg_loss, accuracy

def validate_epoch(model, val_loader, criterion, device):
    """
    ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ì—°ì‚° ì¥ì¹˜
    
    Returns:
        tuple: (í‰ê·  ì†ì‹¤, ì •í™•ë„)
    
    ê²€ì¦ê³¼ í›ˆë ¨ì˜ ì°¨ì´ì :
    1. model.eval(): ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™”, ë°°ì¹˜ ì •ê·œí™” ê³ ì •
    2. torch.no_grad(): ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    3. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì—†ìŒ: ìˆœìˆ˜í•˜ê²Œ ì„±ëŠ¥ ì¸¡ì •ë§Œ
    """
    
    model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •
    
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="ê²€ì¦ ì¤‘")
        
        for data, targets in pbar:
            data, targets = data.to(device), targets.to(device)
            
            # ìˆœì „íŒŒë§Œ ìˆ˜í–‰
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            running_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            correct_predictions += (predictions == targets).sum().item()
            total_samples += targets.size(0)
            
            # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
            current_accuracy = correct_predictions / total_samples
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_accuracy:.4f}'
            })
    
    avg_loss = running_loss / len(val_loader)
    accuracy = correct_predictions / total_samples
    
    return avg_loss, accuracy

# ============================================================================
# 8. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ============================================================================

print(f"\nğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
print(f"   ì´ ì—í¬í¬: {EPOCHS}")
print(f"   ë°°ì¹˜ë‹¹ ìƒ˜í”Œ ìˆ˜: {BATCH_SIZE}")
print(f"   ì´ í›ˆë ¨ ë°°ì¹˜: {len(train_loader)}")

# í›ˆë ¨ ê¸°ë¡ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# ìµœê³  ì„±ëŠ¥ ì¶”ì 
best_val_accuracy = 0.0
best_model_state = None

# í›ˆë ¨ ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()

for epoch in range(EPOCHS):
    print(f"\nğŸ“… ì—í¬í¬ {epoch+1}/{EPOCHS}")
    
    # í›ˆë ¨
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    
    # ê²€ì¦
    val_loss, val_acc = validate_epoch(model, test_loader, criterion, device)
    
    # ê¸°ë¡ ì €ì¥
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   í›ˆë ¨ - ì†ì‹¤: {train_loss:.4f}, ì •í™•ë„: {train_acc:.4f}")
    print(f"   ê²€ì¦ - ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_acc:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if val_acc > best_val_accuracy:
        best_val_accuracy = val_acc
        best_model_state = model.state_dict().copy()
        print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(
            model, optimizer, epoch, val_loss, val_acc,
            save_path="./checkpoints/mnist_best_model.pth",
            additional_info={
                'train_loss': train_loss,
                'train_accuracy': train_acc,
                'hyperparameters': {
                    'batch_size': BATCH_SIZE,
                    'learning_rate': LEARNING_RATE,
                    'epochs': EPOCHS
                }
            }
        )

# í›ˆë ¨ ì™„ë£Œ
training_time = time.time() - start_time
print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"   ì´ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
print(f"   ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f}")

# ============================================================================
# 9. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”")

# í›ˆë ¨ ê³¡ì„  ê·¸ë¦¬ê¸°
plot_training_curves(
    train_losses=train_losses,
    val_losses=val_losses,
    train_accuracies=train_accuracies,
    val_accuracies=val_accuracies,
    title="MNIST ë¶„ë¥˜ - í›ˆë ¨ ê³¼ì •",
    save_path="./results/mnist_training_curves.png"
)

# ============================================================================
# 10. ìµœì¢… ëª¨ë¸ í‰ê°€
# ============================================================================

print(f"\nğŸ¯ ìµœì¢… ëª¨ë¸ í‰ê°€")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€
final_results = evaluate_model(
    model=model,
    dataloader=test_loader,
    criterion=criterion,
    device=device,
    num_classes=10
)

# ============================================================================
# 11. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
# ============================================================================

print(f"\nğŸ–¼ï¸  ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”")

# ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
plot_model_predictions(
    model=model,
    dataloader=test_loader,
    class_names=class_names,
    num_samples=12,
    device=device,
    title="MNIST ë¶„ë¥˜ - ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼"
)

# í˜¼ë™ í–‰ë ¬ ìƒì„±ì„ ìœ„í•œ ì „ì²´ ì˜ˆì¸¡ ìˆ˜ì§‘
print(f"\nğŸ“Š í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")

all_predictions = []
all_targets = []

model.eval()
with torch.no_grad():
    for data, targets in tqdm(test_loader, desc="ì˜ˆì¸¡ ìˆ˜ì§‘"):
        data, targets = data.to(device), targets.to(device)
        outputs = model(data)
        predictions = torch.argmax(outputs, dim=1)
        
        all_predictions.extend(predictions.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
plot_confusion_matrix(
    y_true=np.array(all_targets),
    y_pred=np.array(all_predictions),
    class_names=class_names,
    title="MNIST ë¶„ë¥˜ - í˜¼ë™ í–‰ë ¬",
    save_path="./results/mnist_confusion_matrix.png"
)

# ============================================================================
# 12. í•™ìŠµ ë‚´ìš© ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
# ============================================================================

print(f"\nğŸ“ í•™ìŠµ ë‚´ìš© ìš”ì•½")
print(f"=" * 60)

print(f"âœ… ì™„ë£Œí•œ ë‚´ìš©:")
print(f"   1. PyTorch í…ì„œì™€ ìë™ ë¯¸ë¶„ ì‹œìŠ¤í…œ ì´í•´")
print(f"   2. MNIST ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬")
print(f"   3. ê°„ë‹¨í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  êµ¬í˜„")
print(f"   4. í›ˆë ¨ ë£¨í”„ì™€ ê²€ì¦ ê³¼ì • êµ¬í˜„")
print(f"   5. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”")

print(f"\nğŸ“Š ìµœì¢… ì„±ê³¼:")
print(f"   - ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)")
print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {param_info['total_params']:,}ê°œ")
print(f"   - í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")

print(f"\nğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
print(f"   1. ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„± (ì •ê·œí™”, í…ì„œ ë³€í™˜)")
print(f"   2. ì ì ˆí•œ ëª¨ë¸ êµ¬ì¡° ì„¤ê³„ (ì€ë‹‰ì¸µ í¬ê¸°, í™œì„±í™” í•¨ìˆ˜)")
print(f"   3. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„ íƒì˜ ì˜í–¥")
print(f"   4. ê³¼ì í•© ë°©ì§€ ê¸°ë²• (ë“œë¡­ì•„ì›ƒ)")
print(f"   5. í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§ì˜ ì¤‘ìš”ì„±")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"   - 02_neural_networks.py: ë” ë³µì¡í•œ ì‹ ê²½ë§ê³¼ ì •ê·œí™” ê¸°ë²•")
print(f"   - Fashion-MNISTë¡œ ë” ì–´ë ¤ìš´ ë¶„ë¥˜ ë¬¸ì œ ë„ì „")
print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¸°ë²• í•™ìŠµ")

print(f"\nğŸ”§ ê°œì„  ì‹¤í—˜ ì•„ì´ë””ì–´:")
print(f"   1. í•™ìŠµë¥  ë³€ê²½ (0.01, 0.0001)")
print(f"   2. ì€ë‹‰ì¸µ í¬ê¸° ì¡°ì • (256, 512)")
print(f"   3. ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë³€ê²½ (0.1, 0.5)")
print(f"   4. ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì € ì‹œë„ (SGD, RMSprop)")
print(f"   5. ë°°ì¹˜ í¬ê¸° ë³€ê²½ (32, 128)")

print(f"\n" + "=" * 60)
print(f"ğŸ‰ PyTorch ê¸°ì´ˆ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
print(f"   ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œ ë” ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë°°ì›Œë³´ì„¸ìš”!")
print(f"=" * 60)