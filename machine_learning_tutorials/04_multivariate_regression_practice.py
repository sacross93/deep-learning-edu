"""
ë‹¤ë³€ëŸ‰ íšŒê·€ ì‹¤ìŠµ

ì´ ì‹¤ìŠµì—ì„œëŠ” Boston Housing ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ë³€ëŸ‰ ì„ í˜• íšŒê·€ë¥¼ êµ¬í˜„í•˜ê³ 
íšŒê·€ ëª¨ë¸ì˜ í›ˆë ¨, ì˜ˆì¸¡, ì„±ëŠ¥ í‰ê°€, íŒŒë¼ë¯¸í„° í•´ì„ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def load_and_explore_data():
    """
    1. Boston Housing ë°ì´í„°ì…‹ ë¡œë”© ë° íƒìƒ‰
    """
    print("=" * 60)
    print("1. Boston Housing ë°ì´í„°ì…‹ ë¡œë”© ë° íƒìƒ‰")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë”©
    boston = load_boston()
    X = pd.DataFrame(boston.data, columns=boston.feature_names)
    y = pd.Series(boston.target, name='MEDV')
    
    print("ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´:")
    print(f"- ìƒ˜í”Œ ìˆ˜: {X.shape[0]}")
    print(f"- íŠ¹ì„± ìˆ˜: {X.shape[1]}")
    print(f"- íƒ€ê²Ÿ ë³€ìˆ˜: ì£¼íƒ ê°€ê²© (ë‹¨ìœ„: $1000)")
    
    print("\níŠ¹ì„± ì„¤ëª…:")
    feature_descriptions = {
        'CRIM': 'ë²”ì£„ìœ¨',
        'ZN': '25,000 sq.ft. ì´ìƒ ì£¼ê±°ì§€ì—­ ë¹„ìœ¨',
        'INDUS': 'ë¹„ì†Œë§¤ ìƒì—…ì§€ì—­ ë¹„ìœ¨',
        'CHAS': 'ì°°ìŠ¤ê°• ì¸ì ‘ ì—¬ë¶€ (1: ì¸ì ‘, 0: ë¹„ì¸ì ‘)',
        'NOX': 'ì¼ì‚°í™”ì§ˆì†Œ ë†ë„',
        'RM': 'í‰ê·  ë°© ê°œìˆ˜',
        'AGE': '1940ë…„ ì´ì „ ê±´ì¶• ì£¼íƒ ë¹„ìœ¨',
        'DIS': 'ê³ ìš©ì„¼í„°ê¹Œì§€ ê±°ë¦¬',
        'RAD': 'ê³ ì†ë„ë¡œ ì ‘ê·¼ì„±',
        'TAX': 'ì¬ì‚°ì„¸ìœ¨',
        'PTRATIO': 'í•™ìƒ-êµì‚¬ ë¹„ìœ¨',
        'B': 'í‘ì¸ ê±°ì£¼ ë¹„ìœ¨',
        'LSTAT': 'ì €ì†Œë“ì¸µ ë¹„ìœ¨'
    }
    
    for feature, desc in feature_descriptions.items():
        print(f"- {feature}: {desc}")
    
    print(f"\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(X.head())
    
    print(f"\níƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
    print(y.describe())
    
    return X, y

def visualize_data_distribution(X, y):
    """
    2. ë°ì´í„° ë¶„í¬ ë° ìƒê´€ê´€ê³„ ì‹œê°í™”
    """
    print("\n" + "=" * 60)
    print("2. ë°ì´í„° ë¶„í¬ ë° ìƒê´€ê´€ê³„ ì‹œê°í™”")
    print("=" * 60)
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 3, 1)
    plt.hist(y, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    plt.title('Target Variable Distribution (MEDV)')
    plt.xlabel('House Price ($1000)')
    plt.ylabel('Frequency')
    
    # ì£¼ìš” íŠ¹ì„±ë“¤ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ê´€ê³„
    important_features = ['RM', 'LSTAT', 'PTRATIO', 'NOX']
    
    for i, feature in enumerate(important_features, 2):
        plt.subplot(2, 3, i)
        plt.scatter(X[feature], y, alpha=0.6, color='coral')
        plt.xlabel(feature)
        plt.ylabel('MEDV')
        plt.title(f'{feature} vs MEDV')
        
        # ìƒê´€ê³„ìˆ˜ í‘œì‹œ
        corr = np.corrcoef(X[feature], y)[0, 1]
        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', 
                transform=plt.gca().transAxes, 
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    plt.figure(figsize=(12, 10))
    correlation_matrix = pd.concat([X, y], axis=1).corr()
    
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                center=0, square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
    plt.title('Feature Correlation Matrix')
    plt.tight_layout()
    plt.show()
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ìˆœìœ„
    target_corr = correlation_matrix['MEDV'].abs().sort_values(ascending=False)
    print("íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
    for feature, corr in target_corr.items():
        if feature != 'MEDV':
            print(f"- {feature}: {corr:.3f}")

def preprocess_data(X, y):
    """
    3. ë°ì´í„° ì „ì²˜ë¦¬
    """
    print("\n" + "=" * 60)
    print("3. ë°ì´í„° ì „ì²˜ë¦¬")
    print("=" * 60)
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]} ìƒ˜í”Œ")
    
    # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("\níŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ (í‘œì¤€í™”)")
    print("ìŠ¤ì¼€ì¼ë§ ì „í›„ ë¹„êµ:")
    print(f"- ì›ë³¸ ë°ì´í„° ë²”ìœ„: [{X_train.min().min():.2f}, {X_train.max().max():.2f}]")
    print(f"- ìŠ¤ì¼€ì¼ë§ í›„ ë²”ìœ„: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
    
    return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler

def train_basic_regression(X_train_scaled, X_test_scaled, y_train, y_test):
    """
    4. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ í›ˆë ¨
    """
    print("\n" + "=" * 60)
    print("4. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ í›ˆë ¨")
    print("=" * 60)
    
    # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)
    
    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    
    # ì„±ëŠ¥ í‰ê°€
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print("ê¸°ë³¸ ì„ í˜• íšŒê·€ ì„±ëŠ¥:")
    print(f"- í›ˆë ¨ MSE: {train_mse:.3f}")
    print(f"- í…ŒìŠ¤íŠ¸ MSE: {test_mse:.3f}")
    print(f"- í›ˆë ¨ RÂ²: {train_r2:.3f}")
    print(f"- í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.3f}")
    
    # ê³¼ì í•© ì—¬ë¶€ í™•ì¸
    if train_r2 - test_r2 > 0.1:
        print("âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âœ… ì ì ˆí•œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
    
    return model, y_test_pred

def analyze_coefficients(model, feature_names, scaler):
    """
    5. íšŒê·€ ê³„ìˆ˜ ë¶„ì„ ë° í•´ì„
    """
    print("\n" + "=" * 60)
    print("5. íšŒê·€ ê³„ìˆ˜ ë¶„ì„ ë° í•´ì„")
    print("=" * 60)
    
    # íšŒê·€ ê³„ìˆ˜ ì¶”ì¶œ
    coefficients = model.coef_
    intercept = model.intercept_
    
    print(f"ì ˆí¸ (Intercept): {intercept:.3f}")
    print("\níšŒê·€ ê³„ìˆ˜ (Coefficients):")
    
    # ê³„ìˆ˜ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬
    coef_df = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coefficients,
        'Abs_Coefficient': np.abs(coefficients)
    }).sort_values('Abs_Coefficient', ascending=False)
    
    print(coef_df.to_string(index=False))
    
    # ê³„ìˆ˜ ì‹œê°í™”
    plt.figure(figsize=(12, 8))
    
    # ê³„ìˆ˜ í¬ê¸°ë³„ ì •ë ¬
    sorted_idx = np.argsort(np.abs(coefficients))
    
    plt.subplot(1, 2, 1)
    colors = ['red' if c < 0 else 'blue' for c in coefficients[sorted_idx]]
    plt.barh(range(len(coefficients)), coefficients[sorted_idx], color=colors, alpha=0.7)
    plt.yticks(range(len(coefficients)), [feature_names[i] for i in sorted_idx])
    plt.xlabel('Coefficient Value')
    plt.title('Regression Coefficients')
    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)
    
    # ê³„ìˆ˜ ì ˆëŒ“ê°’
    plt.subplot(1, 2, 2)
    plt.barh(range(len(coefficients)), np.abs(coefficients[sorted_idx]), 
             color='green', alpha=0.7)
    plt.yticks(range(len(coefficients)), [feature_names[i] for i in sorted_idx])
    plt.xlabel('|Coefficient Value|')
    plt.title('Absolute Coefficient Values')
    
    plt.tight_layout()
    plt.show()
    
    # ê³„ìˆ˜ í•´ì„
    print("\nê³„ìˆ˜ í•´ì„:")
    print("ì–‘ìˆ˜ ê³„ìˆ˜: í•´ë‹¹ íŠ¹ì„±ì´ ì¦ê°€í•˜ë©´ ì£¼íƒ ê°€ê²©ì´ ì¦ê°€")
    print("ìŒìˆ˜ ê³„ìˆ˜: í•´ë‹¹ íŠ¹ì„±ì´ ì¦ê°€í•˜ë©´ ì£¼íƒ ê°€ê²©ì´ ê°ì†Œ")
    print("ì ˆëŒ“ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ íŠ¹ì„±ì˜ ì˜í–¥ë ¥ì´ í¼")
    
    # ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” íŠ¹ì„±ë“¤
    top_features = coef_df.head(5)
    print(f"\nê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” íŠ¹ì„± Top 5:")
    for _, row in top_features.iterrows():
        direction = "ì¦ê°€" if row['Coefficient'] > 0 else "ê°ì†Œ"
        print(f"- {row['Feature']}: {row['Coefficient']:.3f} ({direction} íš¨ê³¼)")

def compare_regularization_methods(X_train_scaled, X_test_scaled, y_train, y_test, feature_names):
    """
    6. ì •ê·œí™” ê¸°ë²• ë¹„êµ (Ridge, Lasso, Elastic Net)
    """
    print("\n" + "=" * 60)
    print("6. ì •ê·œí™” ê¸°ë²• ë¹„êµ")
    print("=" * 60)
    
    # ë‹¤ì–‘í•œ ì •ê·œí™” ëª¨ë¸
    models = {
        'Linear': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=0.1),
        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)
    }
    
    results = {}
    
    for name, model in models.items():
        # ëª¨ë¸ í›ˆë ¨
        model.fit(X_train_scaled, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_test_scaled)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # êµì°¨ê²€ì¦
        cv_scores = cross_val_score(model, X_train_scaled, y_train, 
                                   cv=5, scoring='r2')
        
        results[name] = {
            'model': model,
            'mse': mse,
            'r2': r2,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"{name}:")
        print(f"  - Test MSE: {mse:.3f}")
        print(f"  - Test RÂ²: {r2:.3f}")
        print(f"  - CV RÂ² (meanÂ±std): {cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")
    
    # ê³„ìˆ˜ ë¹„êµ ì‹œê°í™”
    plt.figure(figsize=(15, 10))
    
    for i, (name, result) in enumerate(results.items(), 1):
        plt.subplot(2, 2, i)
        model = result['model']
        
        if hasattr(model, 'coef_'):
            coefficients = model.coef_
            plt.barh(range(len(coefficients)), coefficients, alpha=0.7)
            plt.yticks(range(len(coefficients)), feature_names)
            plt.xlabel('Coefficient Value')
            plt.title(f'{name} Coefficients')
            plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.show()
    
    # ì„±ëŠ¥ ë¹„êµ í‘œ
    comparison_df = pd.DataFrame({
        'Model': list(results.keys()),
        'Test_MSE': [results[name]['mse'] for name in results.keys()],
        'Test_R2': [results[name]['r2'] for name in results.keys()],
        'CV_R2_Mean': [results[name]['cv_mean'] for name in results.keys()],
        'CV_R2_Std': [results[name]['cv_std'] for name in results.keys()]
    })
    
    print("\nëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
    print(comparison_df.to_string(index=False))
    
    return results

def polynomial_features_experiment(X_train, X_test, y_train, y_test):
    """
    7. ë‹¤í•­ì‹ íŠ¹ì„± ì‹¤í—˜
    """
    print("\n" + "=" * 60)
    print("7. ë‹¤í•­ì‹ íŠ¹ì„± ì‹¤í—˜")
    print("=" * 60)
    
    degrees = [1, 2, 3]
    poly_results = {}
    
    for degree in degrees:
        print(f"\në‹¤í•­ì‹ ì°¨ìˆ˜: {degree}")
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„± (ë‹¤í•­ì‹ íŠ¹ì„± + ìŠ¤ì¼€ì¼ë§ + íšŒê·€)
        pipeline = Pipeline([
            ('poly', PolynomialFeatures(degree=degree, include_bias=False)),
            ('scaler', StandardScaler()),
            ('regressor', LinearRegression())
        ])
        
        # í›ˆë ¨
        pipeline.fit(X_train, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_train_pred = pipeline.predict(X_train)
        y_test_pred = pipeline.predict(X_test)
        
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        
        # íŠ¹ì„± ê°œìˆ˜
        n_features = pipeline.named_steps['poly'].transform(X_train).shape[1]
        
        poly_results[degree] = {
            'train_r2': train_r2,
            'test_r2': test_r2,
            'test_mse': test_mse,
            'n_features': n_features,
            'overfitting': train_r2 - test_r2
        }
        
        print(f"  - íŠ¹ì„± ê°œìˆ˜: {n_features}")
        print(f"  - í›ˆë ¨ RÂ²: {train_r2:.3f}")
        print(f"  - í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.3f}")
        print(f"  - ê³¼ì í•© ì •ë„: {train_r2 - test_r2:.3f}")
        
        if train_r2 - test_r2 > 0.2:
            print("  âš ï¸  ì‹¬ê°í•œ ê³¼ì í•©!")
        elif train_r2 - test_r2 > 0.1:
            print("  âš ï¸  ê³¼ì í•© ì£¼ì˜")
        else:
            print("  âœ… ì ì ˆí•œ ë³µì¡ë„")
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    degrees_list = list(poly_results.keys())
    train_r2_list = [poly_results[d]['train_r2'] for d in degrees_list]
    test_r2_list = [poly_results[d]['test_r2'] for d in degrees_list]
    
    plt.plot(degrees_list, train_r2_list, 'o-', label='Training RÂ²', color='blue')
    plt.plot(degrees_list, test_r2_list, 'o-', label='Test RÂ²', color='red')
    plt.xlabel('Polynomial Degree')
    plt.ylabel('RÂ² Score')
    plt.title('Model Performance vs Polynomial Degree')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    n_features_list = [poly_results[d]['n_features'] for d in degrees_list]
    plt.bar(degrees_list, n_features_list, alpha=0.7, color='green')
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Number of Features')
    plt.title('Feature Count vs Polynomial Degree')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return poly_results

def residual_analysis(y_test, y_test_pred):
    """
    8. ì”ì°¨ ë¶„ì„
    """
    print("\n" + "=" * 60)
    print("8. ì”ì°¨ ë¶„ì„")
    print("=" * 60)
    
    residuals = y_test - y_test_pred
    
    plt.figure(figsize=(15, 10))
    
    # ì”ì°¨ vs ì˜ˆì¸¡ê°’
    plt.subplot(2, 3, 1)
    plt.scatter(y_test_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='red', linestyle='--')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residuals vs Predicted Values')
    
    # ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
    plt.subplot(2, 3, 2)
    plt.hist(residuals, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.title('Residuals Distribution')
    
    # Q-Q í”Œë¡¯ (ì •ê·œì„± ê²€ì •)
    from scipy import stats
    plt.subplot(2, 3, 3)
    stats.probplot(residuals, dist="norm", plot=plt)
    plt.title('Q-Q Plot (Normality Test)')
    
    # ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’
    plt.subplot(2, 3, 4)
    plt.scatter(y_test, y_test_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
             'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Actual vs Predicted Values')
    
    # ì”ì°¨ì˜ ì ˆëŒ“ê°’
    plt.subplot(2, 3, 5)
    plt.scatter(y_test_pred, np.abs(residuals), alpha=0.6)
    plt.xlabel('Predicted Values')
    plt.ylabel('|Residuals|')
    plt.title('Absolute Residuals vs Predicted')
    
    # ì”ì°¨ í†µê³„
    plt.subplot(2, 3, 6)
    residual_stats = {
        'Mean': np.mean(residuals),
        'Std': np.std(residuals),
        'Min': np.min(residuals),
        'Max': np.max(residuals),
        'Skewness': stats.skew(residuals),
        'Kurtosis': stats.kurtosis(residuals)
    }
    
    stats_text = '\n'.join([f'{k}: {v:.3f}' for k, v in residual_stats.items()])
    plt.text(0.1, 0.5, stats_text, transform=plt.gca().transAxes, 
             fontsize=12, verticalalignment='center',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    plt.axis('off')
    plt.title('Residual Statistics')
    
    plt.tight_layout()
    plt.show()
    
    # ì”ì°¨ ë¶„ì„ ê²°ê³¼
    print("ì”ì°¨ ë¶„ì„ ê²°ê³¼:")
    print(f"- ì”ì°¨ í‰ê· : {np.mean(residuals):.3f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)")
    print(f"- ì”ì°¨ í‘œì¤€í¸ì°¨: {np.std(residuals):.3f}")
    print(f"- ì”ì°¨ ë²”ìœ„: [{np.min(residuals):.3f}, {np.max(residuals):.3f}]")
    
    # ê°€ì • ê²€ì¦
    print("\nì„ í˜• íšŒê·€ ê°€ì • ê²€ì¦:")
    
    # 1. ì„ í˜•ì„± (ì”ì°¨ vs ì˜ˆì¸¡ê°’ì˜ íŒ¨í„´)
    if np.abs(np.corrcoef(y_test_pred, residuals)[0, 1]) < 0.1:
        print("âœ… ì„ í˜•ì„±: ì”ì°¨ì™€ ì˜ˆì¸¡ê°’ ê°„ ìƒê´€ê´€ê³„ê°€ ë‚®ìŒ")
    else:
        print("âš ï¸  ì„ í˜•ì„±: ë¹„ì„ í˜• íŒ¨í„´ì´ ê°ì§€ë¨")
    
    # 2. ì •ê·œì„± (Shapiro-Wilk ê²€ì •)
    _, p_value = stats.shapiro(residuals)
    if p_value > 0.05:
        print("âœ… ì •ê·œì„±: ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ (p > 0.05)")
    else:
        print("âš ï¸  ì •ê·œì„±: ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ (p â‰¤ 0.05)")
    
    # 3. ë“±ë¶„ì‚°ì„± (Breusch-Pagan ê²€ì • ê·¼ì‚¬)
    residual_variance = np.var(residuals)
    if residual_variance < np.var(y_test) * 0.5:
        print("âœ… ë“±ë¶„ì‚°ì„±: ì”ì°¨ì˜ ë¶„ì‚°ì´ ì ì ˆí•¨")
    else:
        print("âš ï¸  ë“±ë¶„ì‚°ì„±: ì´ë¶„ì‚°ì„± ê°€ëŠ¥ì„± ìˆìŒ")

def feature_importance_analysis(X, y, feature_names):
    """
    9. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    """
    print("\n" + "=" * 60)
    print("9. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
    print("=" * 60)
    
    # ë‹¨ë³€ëŸ‰ ìƒê´€ê´€ê³„
    correlations = []
    for feature in feature_names:
        corr = np.corrcoef(X[feature], y)[0, 1]
        correlations.append(abs(corr))
    
    # ìˆœì—´ ì¤‘ìš”ë„ (ê°„ë‹¨í•œ ë²„ì „)
    from sklearn.model_selection import cross_val_score
    
    # ê¸°ì¤€ ëª¨ë¸ ì„±ëŠ¥
    base_model = LinearRegression()
    base_scores = cross_val_score(base_model, X, y, cv=5, scoring='r2')
    base_score = base_scores.mean()
    
    permutation_importance = []
    
    for i, feature in enumerate(feature_names):
        # í•´ë‹¹ íŠ¹ì„±ì„ ì…”í”Œ
        X_shuffled = X.copy()
        X_shuffled.iloc[:, i] = np.random.permutation(X_shuffled.iloc[:, i])
        
        # ì„±ëŠ¥ ì¸¡ì •
        shuffled_scores = cross_val_score(base_model, X_shuffled, y, cv=5, scoring='r2')
        importance = base_score - shuffled_scores.mean()
        permutation_importance.append(max(0, importance))  # ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ
    
    # ê²°ê³¼ ì •ë¦¬
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Correlation': correlations,
        'Permutation_Importance': permutation_importance
    })
    
    # ì •ê·œí™” (0-1 ìŠ¤ì¼€ì¼)
    importance_df['Correlation_Norm'] = importance_df['Correlation'] / importance_df['Correlation'].max()
    importance_df['Permutation_Norm'] = importance_df['Permutation_Importance'] / importance_df['Permutation_Importance'].max()
    
    # ì¢…í•© ì ìˆ˜ (í‰ê· )
    importance_df['Combined_Score'] = (importance_df['Correlation_Norm'] + importance_df['Permutation_Norm']) / 2
    
    # ì •ë ¬
    importance_df = importance_df.sort_values('Combined_Score', ascending=False)
    
    print("íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ê²°ê³¼:")
    print(importance_df.to_string(index=False))
    
    # ì‹œê°í™”
    plt.figure(figsize=(12, 8))
    
    plt.subplot(1, 2, 1)
    plt.barh(range(len(importance_df)), importance_df['Correlation'], alpha=0.7, color='blue')
    plt.yticks(range(len(importance_df)), importance_df['Feature'])
    plt.xlabel('Absolute Correlation')
    plt.title('Feature Importance: Correlation')
    
    plt.subplot(1, 2, 2)
    plt.barh(range(len(importance_df)), importance_df['Permutation_Importance'], alpha=0.7, color='green')
    plt.yticks(range(len(importance_df)), importance_df['Feature'])
    plt.xlabel('Permutation Importance')
    plt.title('Feature Importance: Permutation')
    
    plt.tight_layout()
    plt.show()
    
    return importance_df

def main():
    """
    ë©”ì¸ ì‹¤ìŠµ í•¨ìˆ˜
    """
    print("ë‹¤ë³€ëŸ‰ íšŒê·€ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("Boston Housing ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•´ë³´ê² ìŠµë‹ˆë‹¤.")
    
    # 1. ë°ì´í„° ë¡œë”© ë° íƒìƒ‰
    X, y = load_and_explore_data()
    
    # 2. ë°ì´í„° ì‹œê°í™”
    visualize_data_distribution(X, y)
    
    # 3. ë°ì´í„° ì „ì²˜ë¦¬
    X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = preprocess_data(X, y)
    
    # 4. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ í›ˆë ¨
    model, y_test_pred = train_basic_regression(X_train_scaled, X_test_scaled, y_train, y_test)
    
    # 5. íšŒê·€ ê³„ìˆ˜ ë¶„ì„
    analyze_coefficients(model, X.columns.tolist(), scaler)
    
    # 6. ì •ê·œí™” ê¸°ë²• ë¹„êµ
    regularization_results = compare_regularization_methods(
        X_train_scaled, X_test_scaled, y_train, y_test, X.columns.tolist()
    )
    
    # 7. ë‹¤í•­ì‹ íŠ¹ì„± ì‹¤í—˜
    poly_results = polynomial_features_experiment(X_train, X_test, y_train, y_test)
    
    # 8. ì”ì°¨ ë¶„ì„
    residual_analysis(y_test, y_test_pred)
    
    # 9. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    importance_df = feature_importance_analysis(X, y, X.columns.tolist())
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ì‹¤ìŠµ ìš”ì•½")
    print("=" * 60)
    
    print("âœ… ì™„ë£Œëœ ì‹¤ìŠµ ë‚´ìš©:")
    print("1. Boston Housing ë°ì´í„°ì…‹ íƒìƒ‰")
    print("2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°í™”")
    print("3. ê¸°ë³¸ ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬í˜„")
    print("4. íšŒê·€ ê³„ìˆ˜ í•´ì„")
    print("5. ì •ê·œí™” ê¸°ë²• ë¹„êµ (Ridge, Lasso, Elastic Net)")
    print("6. ë‹¤í•­ì‹ íŠ¹ì„±ì„ í†µí•œ ë³µì¡ë„ ì¡°ì ˆ")
    print("7. ì”ì°¨ ë¶„ì„ì„ í†µí•œ ëª¨ë¸ ì§„ë‹¨")
    print("8. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
    
    print("\nğŸ¯ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:")
    print("- ë‹¤ë³€ëŸ‰ íšŒê·€ëŠ” ì—¬ëŸ¬ íŠ¹ì„±ì„ ì‚¬ìš©í•œ ì—°ì†í˜• ì˜ˆì¸¡")
    print("- íšŒê·€ ê³„ìˆ˜ë¥¼ í†µí•´ ê° íŠ¹ì„±ì˜ ì˜í–¥ë„ í•´ì„ ê°€ëŠ¥")
    print("- ì •ê·œí™” ê¸°ë²•ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€")
    print("- ì”ì°¨ ë¶„ì„ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì • ê²€ì¦")
    print("- íŠ¹ì„± ì¤‘ìš”ë„ë¡œ ëª¨ë¸ í•´ì„ë ¥ í–¥ìƒ")

if __name__ == "__main__":
    main()