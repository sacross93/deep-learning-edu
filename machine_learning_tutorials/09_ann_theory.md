# 인공신경망 완전 이론 가이드

## 1. 개요 및 핵심 개념

### 인공신경망의 정의
인공신경망(Artificial Neural Network, ANN)은 생물학적 뉴런의 구조와 기능을 모방하여 복잡한 비선형 패턴을 학습할 수 있는 머신러닝 모델입니다. 전통적인 선형 모델로는 해결하기 어려운 복잡한 분류 및 회귀 문제를 해결할 수 있습니다.

### 핵심 구성 요소
- **뉴런(노드)**: 입력을 받아 가중합을 계산하고 활성화 함수를 통해 출력을 생성
- **가중치(Weight)**: 입력 신호의 중요도를 나타내는 매개변수
- **편향(Bias)**: 뉴런의 활성화 임계값을 조정하는 매개변수
- **활성화 함수**: 비선형성을 도입하여 복잡한 패턴 학습을 가능하게 하는 함수

### 생물학적 뉴런과의 비교
- **수상돌기**: 입력 신호 수신 → 입력층
- **세포체**: 신호 처리 → 가중합 계산
- **축삭**: 출력 신호 전달 → 활성화 함수 출력
- **시냅스**: 뉴런 간 연결 → 가중치

## 2. 동작 원리

### 퍼셉트론 (Perceptron)
가장 단순한 형태의 인공신경망으로, 선형 분류 문제를 해결할 수 있습니다.

**수학적 표현:**
```
y = f(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)
```

**동작 과정:**
1. 입력값과 가중치의 가중합 계산
2. 편향 추가
3. 활성화 함수 적용
4. 출력 생성

**한계점:**
- XOR 문제와 같은 비선형 분류 불가능
- 단일 결정 경계만 생성 가능

### 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)
여러 층의 뉴런으로 구성된 신경망으로, 비선형 문제를 해결할 수 있습니다.

**구조:**
- **입력층**: 데이터를 받는 층
- **은닉층**: 특징을 추출하고 변환하는 층 (1개 이상)
- **출력층**: 최종 예측값을 생성하는 층

**순전파 (Forward Propagation):**
1. 입력층에서 첫 번째 은닉층으로 신호 전달
2. 각 층에서 가중합 계산 및 활성화 함수 적용
3. 최종 출력층까지 신호 전파

### 역전파 알고리즘 (Backpropagation)
신경망의 가중치를 학습하는 핵심 알고리즘입니다.

**기본 원리:**
- 연쇄 법칙(Chain Rule)을 사용한 그래디언트 계산
- 출력층에서 입력층 방향으로 오차 역전파
- 각 가중치에 대한 손실 함수의 편미분 계산

**단계별 과정:**
1. **순전파**: 입력 → 출력 예측값 계산
2. **손실 계산**: 예측값과 실제값 간의 오차 측정
3. **역전파**: 출력층부터 입력층까지 그래디언트 계산
4. **가중치 업데이트**: 계산된 그래디언트로 가중치 조정

**수학적 표현:**
```
∂L/∂w = ∂L/∂y × ∂y/∂z × ∂z/∂w
```

## 3. 파라미터 구성

### 활성화 함수 (Activation Functions)

#### 1. 시그모이드 (Sigmoid)
```
σ(x) = 1 / (1 + e^(-x))
```
- **범위**: (0, 1)
- **특징**: S자 곡선, 확률 해석 가능
- **단점**: 그래디언트 소실 문제, 출력이 0 중심이 아님

#### 2. 하이퍼볼릭 탄젠트 (Tanh)
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```
- **범위**: (-1, 1)
- **특징**: 0 중심 출력, 시그모이드보다 강한 그래디언트
- **단점**: 여전히 그래디언트 소실 문제 존재

#### 3. ReLU (Rectified Linear Unit)
```
ReLU(x) = max(0, x)
```
- **범위**: [0, ∞)
- **장점**: 계산 효율적, 그래디언트 소실 완화
- **단점**: 죽은 뉴런 문제 (음수 입력 시 그래디언트 0)

#### 4. Leaky ReLU
```
LeakyReLU(x) = max(αx, x), α는 작은 양수
```
- **특징**: 음수 영역에서도 작은 그래디언트 유지
- **장점**: 죽은 뉴런 문제 해결

#### 5. 소프트맥스 (Softmax) - 출력층용
```
softmax(x_i) = e^(x_i) / Σ(e^(x_j))
```
- **용도**: 다중 클래스 분류의 출력층
- **특징**: 모든 출력의 합이 1 (확률 분포)

### 손실 함수 (Loss Functions)

#### 1. 평균 제곱 오차 (Mean Squared Error, MSE)
```
MSE = (1/n) × Σ(y_true - y_pred)²
```
- **용도**: 회귀 문제
- **특징**: 큰 오차에 더 큰 페널티

#### 2. 교차 엔트로피 (Cross-Entropy)
```
CE = -Σ(y_true × log(y_pred))
```
- **용도**: 분류 문제
- **이진 분류**: Binary Cross-Entropy
- **다중 분류**: Categorical Cross-Entropy

#### 3. 희소 교차 엔트로피 (Sparse Categorical Cross-Entropy)
- **용도**: 레이블이 정수로 인코딩된 다중 분류
- **장점**: 원-핫 인코딩 불필요

### 최적화 알고리즘 (Optimization Algorithms)

#### 1. 경사 하강법 (Gradient Descent)
```
w = w - η × ∇L(w)
```
- **η**: 학습률 (Learning Rate)
- **특징**: 전체 데이터셋 사용
- **단점**: 느린 수렴, 지역 최솟값 문제

#### 2. 확률적 경사 하강법 (SGD)
- **특징**: 한 번에 하나의 샘플 사용
- **장점**: 빠른 업데이트, 지역 최솟값 탈출 가능
- **단점**: 노이즈가 많은 업데이트

#### 3. 미니배치 경사 하강법
- **특징**: 작은 배치 단위로 업데이트
- **장점**: SGD와 배치 GD의 장점 결합
- **일반적 배치 크기**: 32, 64, 128, 256

#### 4. Adam (Adaptive Moment Estimation)
- **특징**: 모멘텀과 적응적 학습률 결합
- **장점**: 빠른 수렴, 하이퍼파라미터 튜닝 부담 적음
- **기본 설정**: β₁=0.9, β₂=0.999, ε=1e-8

#### 5. RMSprop
- **특징**: 적응적 학습률 조정
- **장점**: 진동하는 그래디언트 문제 해결

## 4. 성능 평가 방법

### 분류 문제 평가 지표

#### 1. 정확도 (Accuracy)
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

#### 2. 정밀도 (Precision)
```
Precision = TP / (TP + FP)
```

#### 3. 재현율 (Recall)
```
Recall = TP / (TP + FN)
```

#### 4. F1-Score
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

### 회귀 문제 평가 지표

#### 1. 평균 절대 오차 (MAE)
```
MAE = (1/n) × Σ|y_true - y_pred|
```

#### 2. 평균 제곱근 오차 (RMSE)
```
RMSE = √[(1/n) × Σ(y_true - y_pred)²]
```

#### 3. 결정 계수 (R²)
```
R² = 1 - (SS_res / SS_tot)
```

### 과적합 탐지 방법
- **검증 손실 모니터링**: 훈련 손실은 감소하지만 검증 손실이 증가
- **조기 종료**: 검증 성능이 개선되지 않으면 훈련 중단
- **학습 곡선 분석**: 훈련/검증 성능 차이 관찰

## 5. 다른 알고리즘과의 비교

### 선형 모델과의 비교

#### 유사점:
- 가중치 기반 예측
- 그래디언트 기반 학습
- 수치적 최적화 사용

#### 차이점:
- **비선형성**: 신경망은 활성화 함수로 비선형 패턴 학습
- **특징 학습**: 신경망은 자동으로 특징 추출
- **복잡도**: 신경망은 더 많은 매개변수와 계산 필요

### 의사결정나무와의 비교

#### 유사점:
- 비선형 패턴 학습 가능
- 복잡한 결정 경계 생성

#### 차이점:
- **해석성**: 의사결정나무가 더 해석하기 쉬움
- **연속성**: 신경망은 연속적 함수, 트리는 불연속적
- **과적합**: 신경망은 정규화 기법 필요

### SVM과의 비교

#### 유사점:
- 비선형 분류 가능 (커널 트릭 vs 은닉층)
- 고차원 데이터 처리 가능

#### 차이점:
- **확장성**: 신경망이 대용량 데이터에 더 적합
- **확률 출력**: 신경망은 확률 해석 가능
- **특징 학습**: 신경망은 end-to-end 학습

## 6. 적용 사례 및 한계

### 적용 분야
- **이미지 인식**: 손글씨 인식, 얼굴 인식, 의료 영상 분석
- **자연어 처리**: 감정 분석, 기계 번역, 텍스트 분류
- **음성 인식**: 음성-텍스트 변환, 화자 인식
- **게임 AI**: 바둑, 체스, 비디오 게임 AI
- **추천 시스템**: 개인화된 콘텐츠 추천
- **금융**: 신용 평가, 사기 탐지, 알고리즘 트레이딩

### 장점
- **범용성**: 다양한 문제에 적용 가능
- **비선형 학습**: 복잡한 패턴 인식 가능
- **자동 특징 추출**: 수동 특징 엔지니어링 불필요
- **확장성**: 대용량 데이터 처리 가능
- **근사 능력**: 임의의 연속 함수 근사 가능 (Universal Approximation Theorem)

### 한계점
- **해석성 부족**: "블랙박스" 모델로 결정 과정 불투명
- **데이터 의존성**: 대량의 훈련 데이터 필요
- **계산 비용**: 훈련과 추론에 많은 계산 자원 필요
- **하이퍼파라미터 민감성**: 네트워크 구조와 학습 설정에 민감
- **과적합 위험**: 복잡한 모델로 인한 일반화 성능 저하 가능
- **지역 최솟값**: 전역 최적해 보장 어려움

### 사용 시 주의사항
- **데이터 전처리**: 정규화, 표준화 필수
- **배치 정규화**: 내부 공변량 이동 문제 해결
- **드롭아웃**: 과적합 방지를 위한 정규화 기법
- **조기 종료**: 검증 성능 기반 훈련 중단
- **학습률 스케줄링**: 학습 과정에서 학습률 조정
- **가중치 초기화**: 적절한 초기값 설정 중요

## 7. 용어 사전

### 기본 용어
- **뉴런/노드**: 신경망의 기본 처리 단위
- **시냅스**: 뉴런 간의 연결 (가중치로 표현)
- **층(Layer)**: 같은 레벨의 뉴런들의 집합
- **깊이(Depth)**: 신경망의 층 수
- **너비(Width)**: 각 층의 뉴런 수

### 학습 관련 용어
- **에포크(Epoch)**: 전체 훈련 데이터를 한 번 학습하는 과정
- **배치(Batch)**: 한 번에 처리하는 데이터 샘플 수
- **이터레이션**: 한 번의 가중치 업데이트 과정
- **학습률(Learning Rate)**: 가중치 업데이트 크기 조절 매개변수
- **모멘텀**: 이전 그래디언트 정보를 활용한 가속 기법

### 정규화 용어
- **드롭아웃**: 훈련 시 일부 뉴런을 무작위로 비활성화
- **배치 정규화**: 각 층의 입력을 정규화하여 학습 안정화
- **가중치 감쇠**: L1/L2 정규화를 통한 가중치 크기 제한
- **조기 종료**: 과적합 방지를 위한 훈련 중단 기법

### 성능 관련 용어
- **그래디언트 소실**: 깊은 네트워크에서 그래디언트가 0에 가까워지는 문제
- **그래디언트 폭발**: 그래디언트가 너무 커져서 학습이 불안정해지는 문제
- **죽은 뉴런**: ReLU 사용 시 항상 0을 출력하는 뉴런
- **내부 공변량 이동**: 층별 입력 분포가 훈련 중 변하는 현상

### 수식 기호 설명
- **w**: 가중치 (weight)
- **b**: 편향 (bias)
- **x**: 입력값
- **y**: 출력값
- **η**: 학습률 (learning rate)
- **σ**: 활성화 함수 (주로 시그모이드)
- **L**: 손실 함수 (loss function)
- **∇**: 그래디언트 (gradient)

### 개념 간 연관성
```
입력 데이터 → [가중합 계산] → [활성화 함수] → [다음 층으로 전달]
     ↓
[손실 함수 계산] → [역전파] → [그래디언트 계산] → [가중치 업데이트]
     ↓
[성능 평가] → [하이퍼파라미터 조정] → [모델 개선]
```

이러한 구조를 통해 인공신경망은 복잡한 비선형 패턴을 학습하고, 다양한 실세계 문제를 해결할 수 있는 강력한 도구가 됩니다.