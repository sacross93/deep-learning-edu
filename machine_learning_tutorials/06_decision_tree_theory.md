# 의사결정나무 완전 이론 가이드

## 1. 개요 및 핵심 개념

### 의사결정나무란?
의사결정나무(Decision Tree)는 데이터를 분류하거나 예측하기 위해 트리 구조를 사용하는 지도학습 알고리즘입니다. 나무를 거꾸로 뒤집어 놓은 형태로, 루트 노드에서 시작하여 리프 노드까지 일련의 질문(분할 조건)을 통해 최종 결정을 내립니다.

### 핵심 구성 요소
- **루트 노드(Root Node)**: 전체 데이터셋을 포함하는 최상위 노드
- **내부 노드(Internal Node)**: 분할 조건을 가지는 중간 노드
- **리프 노드(Leaf Node)**: 최종 예측값을 가지는 말단 노드
- **분할(Split)**: 데이터를 두 개 이상의 하위 집합으로 나누는 조건
- **가지(Branch)**: 노드 간의 연결선

### 의사결정나무의 장점
- **해석 가능성**: 인간이 이해하기 쉬운 규칙 형태
- **비모수적**: 데이터 분포에 대한 가정이 불필요
- **혼합 데이터 처리**: 수치형과 범주형 데이터 동시 처리 가능
- **특성 선택**: 중요한 특성을 자동으로 선택
- **결측값 처리**: 결측값이 있어도 학습 가능

### 의사결정나무의 단점
- **과적합**: 복잡한 트리는 훈련 데이터에 과도하게 맞춤
- **불안정성**: 데이터 변화에 민감하여 트리 구조가 크게 달라질 수 있음
- **편향**: 더 많은 수준을 가진 특성을 선호하는 경향
- **선형 관계**: 선형 관계를 표현하기 어려움

## 2. 동작 원리

### 트리 구축 과정
의사결정나무는 **탐욕적 알고리즘(Greedy Algorithm)**을 사용하여 구축됩니다:

1. **루트 노드 생성**: 전체 훈련 데이터로 시작
2. **최적 분할 찾기**: 모든 가능한 분할 중 최적의 분할 선택
3. **노드 분할**: 선택된 조건에 따라 데이터를 하위 노드로 분할
4. **재귀적 반복**: 각 하위 노드에 대해 2-3단계 반복
5. **종료 조건**: 더 이상 분할할 수 없거나 종료 조건 만족 시 중단

### 분할 기준 선택
각 노드에서 최적의 분할을 선택하기 위해 **불순도(Impurity)** 감소를 최대화합니다:

**정보 이득(Information Gain) = 부모 노드 불순도 - 가중 평균 자식 노드 불순도**

### 종료 조건
- 노드의 모든 샘플이 같은 클래스에 속함
- 더 이상 분할할 특성이 없음
- 최대 깊이에 도달
- 노드의 샘플 수가 최소 임계값 미만
- 정보 이득이 최소 임계값 미만

## 3. 분할 기준 (불순도 측정)

### 1. 엔트로피 (Entropy)
정보 이론에서 유래한 불확실성의 측정 지표입니다.

**수식**: `Entropy(S) = -Σ(i=1 to c) p_i * log₂(p_i)`

- S: 샘플 집합
- c: 클래스 개수  
- p_i: 클래스 i의 비율

**특성**:
- 값의 범위: 0 ~ log₂(c)
- 모든 샘플이 같은 클래스: 엔트로피 = 0 (완전 순수)
- 클래스가 균등 분포: 엔트로피 = log₂(c) (최대 불순도)

### 2. 지니 불순도 (Gini Impurity)
한 노드에서 무작위로 선택한 샘플이 잘못 분류될 확률입니다.

**수식**: `Gini(S) = 1 - Σ(i=1 to c) p_i²`

**특성**:
- 값의 범위: 0 ~ (1 - 1/c)
- 계산이 엔트로피보다 빠름 (로그 연산 불필요)
- CART 알고리즘의 기본 분할 기준

### 3. 분류 오차 (Classification Error)
가장 빈번한 클래스로 분류했을 때의 오차율입니다.

**수식**: `Error(S) = 1 - max(p_i)`

**특성**:
- 가장 단순한 측정 방법
- 실제로는 잘 사용되지 않음 (민감도가 낮음)

### 분할 기준 비교
```
예시: [9개 양성, 5개 음성] 노드

엔트로피 = -(9/14)log₂(9/14) - (5/14)log₂(5/14) ≈ 0.940
지니 = 1 - (9/14)² - (5/14)² ≈ 0.459
오차 = 1 - 9/14 ≈ 0.357
```

## 4. 정보 이득과 이득 비율

### 정보 이득 (Information Gain)
분할 전후의 엔트로피 감소량을 측정합니다.

**수식**: `IG(S,A) = Entropy(S) - Σ(v∈Values(A)) |Sv|/|S| * Entropy(Sv)`

- A: 분할 속성
- Values(A): 속성 A의 가능한 값들
- Sv: 속성 A의 값이 v인 샘플들

### 이득 비율 (Gain Ratio)
정보 이득을 분할 정보로 나누어 정규화한 값입니다.

**수식**: `GainRatio(S,A) = IG(S,A) / SplitInfo(S,A)`

**분할 정보**: `SplitInfo(S,A) = -Σ(v∈Values(A)) |Sv|/|S| * log₂(|Sv|/|S|)`

**목적**: 많은 값을 가진 속성에 대한 편향을 줄임

## 5. 과적합 방지를 위한 가지치기

### 과적합 문제
- **증상**: 훈련 데이터에는 완벽하지만 테스트 데이터에서 성능 저하
- **원인**: 트리가 너무 복잡하여 노이즈까지 학습
- **결과**: 일반화 능력 저하

### 1. 사전 가지치기 (Pre-pruning)
트리 구축 중에 성장을 조기에 중단하는 방법입니다.

**주요 매개변수**:
- **max_depth**: 최대 트리 깊이 제한
- **min_samples_split**: 분할을 위한 최소 샘플 수
- **min_samples_leaf**: 리프 노드의 최소 샘플 수
- **max_features**: 분할 시 고려할 최대 특성 수
- **min_impurity_decrease**: 분할을 위한 최소 불순도 감소량

### 2. 사후 가지치기 (Post-pruning)
완전한 트리를 구축한 후 불필요한 가지를 제거하는 방법입니다.

**주요 기법**:
- **비용 복잡도 가지치기**: 트리 크기와 오차의 균형점 찾기
- **축소 오차 가지치기**: 검증 데이터를 사용한 가지치기
- **최소 오차 가지치기**: 통계적 유의성 검정 기반

### 가지치기 효과
```
가지치기 전: 깊이 10, 정확도 95% (훈련), 75% (테스트)
가지치기 후: 깊이 5, 정확도 88% (훈련), 82% (테스트)
```

## 6. 다른 알고리즘과의 비교

### vs 로지스틱 회귀
**유사점**:
- 둘 다 분류 문제에 사용
- 특성 중요도 파악 가능

**차이점**:
- **해석성**: 의사결정나무가 더 직관적
- **선형성**: 로지스틱 회귀는 선형 결정 경계, 의사결정나무는 비선형
- **확률**: 로지스틱 회귀는 확률 출력, 의사결정나무는 클래스 출력
- **안정성**: 로지스틱 회귀가 더 안정적

### vs 규칙 기반 학습
**유사점**:
- IF-THEN 규칙으로 표현 가능
- 해석 가능한 모델
- 범주형 데이터 처리 용이

**차이점**:
- **구조**: 의사결정나무는 계층적, 규칙 기반은 평면적
- **순서**: 의사결정나무는 순서 중요, 규칙 기반은 독립적
- **중복**: 규칙 기반은 중복 규칙 가능

### vs K-최근접 이웃 (KNN)
**유사점**:
- 비모수적 방법
- 지역적 패턴 학습

**차이점**:
- **학습**: 의사결정나무는 명시적 학습, KNN은 지연 학습
- **해석성**: 의사결정나무가 훨씬 해석하기 쉬움
- **계산**: 의사결정나무는 예측이 빠름, KNN은 학습이 빠름

## 7. 적용 사례 및 한계

### 적용 분야
- **의료 진단**: 증상 기반 질병 진단
- **금융**: 대출 승인/거부 결정
- **마케팅**: 고객 세분화 및 타겟팅
- **제조**: 품질 관리 및 결함 탐지
- **법률**: 판례 분석 및 판결 예측

### 실제 사용 예시
```
의료 진단 트리:
├─ 체온 > 38°C?
│  ├─ Yes: 기침 있음?
│  │  ├─ Yes: 독감 (90%)
│  │  └─ No: 기타 감염 (75%)
│  └─ No: 두통 있음?
     ├─ Yes: 스트레스 (80%)
     └─ No: 정상 (95%)
```

### 한계점
- **고차원 데이터**: 차원이 높으면 성능 저하
- **연속형 관계**: 선형/곡선 관계 표현 어려움
- **불균형 데이터**: 소수 클래스 무시 경향
- **노이즈 민감성**: 이상치에 영향받기 쉬움

### 개선 방법
- **앙상블**: Random Forest, Gradient Boosting
- **특성 엔지니어링**: 도메인 지식 활용한 특성 생성
- **하이브리드**: 다른 알고리즘과 결합
- **정규화**: 가지치기 매개변수 조정

## 8. 용어 사전

### 기본 용어
- **노드(Node)**: 트리의 각 분기점 또는 끝점
- **분할(Split)**: 데이터를 나누는 조건
- **깊이(Depth)**: 루트에서 특정 노드까지의 거리
- **높이(Height)**: 트리의 최대 깊이
- **순도(Purity)**: 노드 내 클래스 동질성 정도

### 고급 용어
- **가지치기(Pruning)**: 과적합 방지를 위한 트리 단순화
- **불순도(Impurity)**: 노드 내 클래스 혼재 정도
- **정보 이득(Information Gain)**: 분할로 인한 정보량 증가
- **분할 정보(Split Information)**: 분할 자체의 정보량
- **비용 복잡도(Cost Complexity)**: 트리 크기와 오차의 균형 지표

### 수식 기호
- **S**: 샘플 집합
- **p_i**: 클래스 i의 비율
- **c**: 클래스 개수
- **A**: 분할 속성
- **IG**: 정보 이득
- **H**: 엔트로피
- **G**: 지니 불순도

### 개념 간 연관성
```
불순도 측정 → 분할 기준 선택 → 트리 구축 → 가지치기 → 최종 모델
     ↓              ↓              ↓          ↓          ↓
  엔트로피        정보 이득        재귀적      과적합      예측
  지니 불순도     이득 비율        분할       방지       성능
```

## 9. 실습 준비사항

### 필요 라이브러리
```python
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
```

### 주요 매개변수
- **criterion**: 'gini', 'entropy', 'log_loss'
- **max_depth**: 트리 최대 깊이
- **min_samples_split**: 분할 최소 샘플 수
- **min_samples_leaf**: 리프 노드 최소 샘플 수
- **random_state**: 재현 가능한 결과를 위한 시드

이제 실습을 통해 의사결정나무의 실제 동작을 확인해보겠습니다!