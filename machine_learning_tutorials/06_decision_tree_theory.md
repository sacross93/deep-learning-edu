# 의사결정나무 완전 이론 가이드

## 1. 개요 및 핵심 개념

### 의사결정나무란?
의사결정나무(Decision Tree)는 데이터를 분류하거나 예측하기 위해 트리 구조를 사용하는 지도학습 알고리즘입니다. 나무를 거꾸로 뒤집어 놓은 형태로, 루트 노드에서 시작하여 리프 노드까지 일련의 질문(분할 조건)을 통해 최종 결정을 내립니다.

### 핵심 구성 요소
- **루트 노드(Root Node)**: 전체 데이터셋을 포함하는 최상위 노드
- **내부 노드(Internal Node)**: 분할 조건을 가지는 중간 노드
- **리프 노드(Leaf Node)**: 최종 예측값을 가지는 말단 노드
- **분할(Split)**: 데이터를 두 개 이상의 하위 집합으로 나누는 조건
- **가지(Branch)**: 노드 간의 연결선

```
의사결정나무 구조 예시:
                    [루트 노드]
                   나이 > 30?
                  /          \
               Yes/            \No
                 /              \
        [내부 노드]              [내부 노드]
        소득 > 50K?             신용점수 > 700?
        /        \              /            \
    Yes/          \No       Yes/              \No
      /            \         /                 \
[리프 노드]    [리프 노드] [리프 노드]      [리프 노드]
  승인           거부        승인             거부
 (90%)         (85%)      (75%)           (95%)
```

### 의사결정나무의 장점
- **해석 가능성**: 인간이 이해하기 쉬운 규칙 형태
- **비모수적**: 데이터 분포에 대한 가정이 불필요
- **혼합 데이터 처리**: 수치형과 범주형 데이터 동시 처리 가능
- **특성 선택**: 중요한 특성을 자동으로 선택
- **결측값 처리**: 결측값이 있어도 학습 가능

### 의사결정나무의 단점
- **과적합**: 복잡한 트리는 훈련 데이터에 과도하게 맞춤
- **불안정성**: 데이터 변화에 민감하여 트리 구조가 크게 달라질 수 있음
- **편향**: 더 많은 수준을 가진 특성을 선호하는 경향
- **선형 관계**: 선형 관계를 표현하기 어려움
- **상호작용 한계**: 탐욕적 분할 특성상, 상호작용이 필요한 속성 조합을 놓치고 덜 변별적인 단일 속성을 선택할 수 있음 (XOR 문제)
- **직사각형 경계**: 각 분할 경계가 단일 속성 기준의 직사각형이라 복잡(비축정렬) 경계 표현이 제한적

## 2. 동작 원리

### 헌트(Hunt) 알고리즘
의사결정나무의 기본이 되는 **헌트 알고리즘**의 일반적 구조:

노드 t에 도달한 훈련 레코드 집합을 Dₜ라 할 때:
- **Dₜ가 하나의 클래스만 포함하면** → 리프 노드 생성 (해당 클래스 라벨 할당)
- **그렇지 않으면** → 속성 테스트로 더 작은 부분집합으로 분할 → 각 부분집합에 대해 재귀적 적용

```
헌트 알고리즘 흐름도:

시작: 전체 데이터 Dₜ
        ↓
   ┌─────────────────┐
   │ Dₜ가 순수한가?  │ ← (모든 레코드가 같은 클래스?)
   │ (단일 클래스?)  │
   └─────────────────┘
          ↓
    Yes ←─┴─→ No
     ↓         ↓
┌─────────┐  ┌──────────────────┐
│리프 노드│  │최적 분할 조건 찾기│
│생성     │  │(속성 테스트)     │
└─────────┘  └──────────────────┘
                      ↓
              ┌──────────────────┐
              │데이터를 하위집합 │
              │으로 분할         │
              └──────────────────┘
                      ↓
              ┌──────────────────┐
              │각 하위집합에     │
              │대해 재귀 호출    │ ──┐
              └──────────────────┘   │
                      ↑              │
                      └──────────────┘
```

### 트리 구축 과정
의사결정나무는 **탐욕적 알고리즘(Greedy Algorithm)**을 사용하여 구축됩니다:

1. **루트 노드 생성**: 전체 훈련 데이터로 시작
2. **최적 분할 찾기**: 모든 가능한 분할 중 최적의 분할 선택
3. **노드 분할**: 선택된 조건에 따라 데이터를 하위 노드로 분할
4. **재귀적 반복**: 각 하위 노드에 대해 2-3단계 반복
5. **종료 조건**: 더 이상 분할할 수 없거나 종료 조건 만족 시 중단

### 분류의 정의와 위치
**분류(Classification)**는 지도학습의 하위 문제로:
- **데이터**: 레코드들의 모음 (x, y)
  - x: 속성/특징/예측변수/입력
  - y: 클래스/목표/반응/출력
- **과제**: x → y를 매핑하는 모델 학습
- **지도학습 유형**:
  - 회귀: 연속형 결과 예측
  - 분류: 이산(범주형) 결과 예측

### 속성 유형별 테스트 조건 표현

#### 이진 속성
- 자연스러운 2-way 분할
```
성별 = {남, 여}
     [노드]
    성별 = ?
    /      \
  남/        \여
  /          \
[자식1]    [자식2]
```

#### 명목(Nominal) 속성
- **다분할(Multi-way)**: 고유값 개수만큼 자식 노드 생성
- **이분(Binary)**: 값 집합을 둘로 나눔 (가능한 분할 조합 수: 2^(k-1)-1)

```
자동차 유형 = {세단, SUV, 트럭, 스포츠카}

다분할:                    이분할:
    [노드]                    [노드]
  자동차유형=?              자동차유형 ∈ ?
  /  |  |  \               /            \
세단 SUV 트럭 스포츠카    {세단,SUV}    {트럭,스포츠카}
```

#### 서열(Ordinal) 속성
- 다분할 또는 이분 가능하나, **순서 보존이 중요**
- 순서를 깨는 그룹화는 부적절

```
학점 = {A, B, C, D, F}

올바른 분할:              잘못된 분할:
    [노드]                   [노드]
   학점 ≤ B?               학점 ∈ {A,C,F}?
   /      \                /              \
  /        \              /                \
{A,B}    {C,D,F}      {A,C,F}          {B,D}
                      (순서 파괴!)
```

#### 연속(Continuous) 속성
- **이분 분할**: (A < v) vs (A ≥ v) - v 후보를 모두 고려해 최적 절단점 선택
- **다분할**: 구간화 (예: <10K, [10K,25K), ..., >80K)

```
소득 (연속형)

이분 분할:                다분할 (구간화):
    [노드]                    [노드]
   소득 < 50K?              소득 구간 = ?
   /        \               /    |    |    \
  /          \             /     |    |     \
<50K        ≥50K        <25K  25-50K 50-75K >75K
```

### 분할 기준 선택
각 노드에서 최적의 분할을 선택하기 위해 **불순도(Impurity)** 감소를 최대화합니다:

**정보 이득(Information Gain) = 부모 노드 불순도 - 가중 평균 자식 노드 불순도**

### "최적 분할" 선택의 원리
- **탐욕적 접근**: 더 순수한(impure가 낮은) 자식 분포를 선호
- **이득(Gain) 계산**:
  - 분할 전 부모 불순도 P
  - 분할 후 가중 평균 불순도 M (자식들의 불순도 × 자식 비율)
  - Gain = P - M (또는 M이 최소인 분할 선택)

```
정보 이득 계산 과정:

분할 전:                    분할 후:
  [부모 노드]                [자식1]    [자식2]
  클래스 A: 60개             A: 50개     A: 10개
  클래스 B: 40개             B: 10개     B: 30개
  총 100개                   총 60개     총 40개
  불순도 P = 0.48           불순도1     불순도2
                            = 0.28      = 0.61

가중 평균 불순도 M = (60/100)×0.28 + (40/100)×0.61 = 0.412
정보 이득 = P - M = 0.48 - 0.412 = 0.068

→ 이득이 양수이므로 유효한 분할!
```

### 종료 조건
- 노드의 모든 샘플이 같은 클래스에 속함
- 더 이상 분할할 특성이 없음
- 최대 깊이에 도달
- 노드의 샘플 수가 최소 임계값 미만
- 정보 이득이 최소 임계값 미만

## 3. 분할 기준 (불순도 측정)

### 1. 엔트로피 (Entropy)
정보 이론에서 유래한 불확실성의 측정 지표입니다.

**수식**: `Entropy(S) = -Σ(i=1 to c) p_i * log₂(p_i)`

- S: 샘플 집합
- c: 클래스 개수  
- p_i: 클래스 i의 비율

**특성**:
- 값의 범위: 0 ~ log₂(c)
- 모든 샘플이 같은 클래스: 엔트로피 = 0 (완전 순수)
- 클래스가 균등 분포: 엔트로피 = log₂(c) (최대 불순도)

### 2. 지니 불순도 (Gini Impurity)
한 노드에서 무작위로 선택한 샘플이 잘못 분류될 확률입니다.

**수식**: `Gini(S) = 1 - Σ(i=1 to c) p_i²`

**특성**:
- 값의 범위: 0 ~ (1 - 1/c)
- 계산이 엔트로피보다 빠름 (로그 연산 불필요)
- CART 알고리즘의 기본 분할 기준

### 3. 분류 오차 (Classification Error)
가장 빈번한 클래스로 분류했을 때의 오차율입니다.

**수식**: `Error(S) = 1 - max(p_i)`

**특성**:
- 가장 단순한 측정 방법
- 실제로는 잘 사용되지 않음 (민감도가 낮음)

### 분할 기준 비교
```
예시: [9개 양성, 5개 음성] 노드

     [노드 상태]
    ●●●●●●●●● (양성 9개)
    ○○○○○     (음성 5개)
    
불순도 계산:
┌─────────────┬──────────┬─────────────────────────────────┐
│   측정법    │   수식   │            계산 과정            │
├─────────────┼──────────┼─────────────────────────────────┤
│  엔트로피   │ -Σpi·log₂│ -(9/14)log₂(9/14)-(5/14)log₂(5/14)│
│             │    pi    │ ≈ 0.940                         │
├─────────────┼──────────┼─────────────────────────────────┤
│ 지니 불순도 │ 1-Σpi²   │ 1-(9/14)²-(5/14)² ≈ 0.459      │
├─────────────┼──────────┼─────────────────────────────────┤
│ 분류 오차   │1-max(pi) │ 1-max(9/14,5/14) ≈ 0.357       │
└─────────────┴──────────┴─────────────────────────────────┘

불순도 크기: 엔트로피 > 지니 > 분류오차
```

## 4. 정보 이득과 이득 비율

### 정보 이득 (Information Gain)
분할 전후의 엔트로피 감소량을 측정합니다.

**수식**: `IG(S,A) = Entropy(S) - Σ(v∈Values(A)) |Sv|/|S| * Entropy(Sv)`

- A: 분할 속성
- Values(A): 속성 A의 가능한 값들
- Sv: 속성 A의 값이 v인 샘플들

### 이득 비율 (Gain Ratio) — C4.5의 보정
정보 이득을 분할 정보로 나누어 정규화한 값입니다.

**수식**: `GainRatio(S,A) = IG(S,A) / SplitInfo(S,A)`

**분할 정보**: `SplitInfo(S,A) = -Σ(v∈Values(A)) |Sv|/|S| * log₂(|Sv|/|S|)`

**목적**: 
- **분할 자체의 엔트로피(Split Info)**가 큰 경우를 패널티
- 많은 값을 가진 속성(예: 고객ID처럼 각 값이 고유)에 대한 편향을 줄임
- 자식 수가 많은 분할일수록 자식 엔트로피가 0에 가까워져 과도한 이득을 산출하는 문제 해결

**정보이득의 문제점**: 
- 자식 수가 많은 분할(예: 고객ID처럼 각 값이 고유)일수록 자식 엔트로피가 0에 가까워 과도한 이득 산출
- 분할 과대평가 문제 발생

## 5. 과적합 방지를 위한 가지치기

### 과적합 문제
- **증상**: 훈련 데이터에는 완벽하지만 테스트 데이터에서 성능 저하
- **원인**: 트리가 너무 복잡하여 노이즈까지 학습
- **결과**: 일반화 능력 저하

### 1. 사전 가지치기 (Pre-pruning)
트리 구축 중에 성장을 조기에 중단하는 방법입니다.

**주요 매개변수**:
- **max_depth**: 최대 트리 깊이 제한
- **min_samples_split**: 분할을 위한 최소 샘플 수
- **min_samples_leaf**: 리프 노드의 최소 샘플 수
- **max_features**: 분할 시 고려할 최대 특성 수
- **min_impurity_decrease**: 분할을 위한 최소 불순도 감소량

### 2. 사후 가지치기 (Post-pruning)
완전한 트리를 구축한 후 불필요한 가지를 제거하는 방법입니다.

**주요 기법**:
- **비용 복잡도 가지치기**: 트리 크기와 오차의 균형점 찾기
- **축소 오차 가지치기**: 검증 데이터를 사용한 가지치기
- **최소 오차 가지치기**: 통계적 유의성 검정 기반

### 가지치기 효과
```
가지치기 전: 깊이 10, 정확도 95% (훈련), 75% (테스트)
가지치기 후: 깊이 5, 정확도 88% (훈련), 82% (테스트)
```

## 6. 다른 알고리즘과의 비교

### vs 로지스틱 회귀
**유사점**:
- 둘 다 분류 문제에 사용
- 특성 중요도 파악 가능

**차이점**:
- **해석성**: 의사결정나무가 더 직관적
- **선형성**: 로지스틱 회귀는 선형 결정 경계, 의사결정나무는 비선형
- **확률**: 로지스틱 회귀는 확률 출력, 의사결정나무는 클래스 출력
- **안정성**: 로지스틱 회귀가 더 안정적

### vs 규칙 기반 학습
**유사점**:
- IF-THEN 규칙으로 표현 가능
- 해석 가능한 모델
- 범주형 데이터 처리 용이

**차이점**:
- **구조**: 의사결정나무는 계층적, 규칙 기반은 평면적
- **순서**: 의사결정나무는 순서 중요, 규칙 기반은 독립적
- **중복**: 규칙 기반은 중복 규칙 가능

### vs K-최근접 이웃 (KNN)
**유사점**:
- 비모수적 방법
- 지역적 패턴 학습

**차이점**:
- **학습**: 의사결정나무는 명시적 학습, KNN은 지연 학습
- **해석성**: 의사결정나무가 훨씬 해석하기 쉬움
- **계산**: 의사결정나무는 예측이 빠름, KNN은 학습이 빠름

## 7. 적용 사례 및 한계

### 적용 분야
- **의료 진단**: 증상 기반 질병 진단
- **금융**: 대출 승인/거부 결정
- **마케팅**: 고객 세분화 및 타겟팅
- **제조**: 품질 관리 및 결함 탐지
- **법률**: 판례 분석 및 판결 예측

### 실제 사용 예시
```
의료 진단 트리:
                    [환자 내원]
                   체온 > 38°C?
                  /            \
               Yes/              \No
                 /                \
          [발열 환자]            [정상 체온]
          기침 있음?             두통 있음?
          /      \              /        \
       Yes/        \No       Yes/          \No
        /           \         /             \
   [기침+발열]   [발열만]  [두통만]      [무증상]
    독감          기타감염   스트레스       정상
   (90%)         (75%)     (80%)        (95%)

규칙 형태로 표현:
IF 체온 > 38°C AND 기침 있음 THEN 독감 (90%)
IF 체온 > 38°C AND 기침 없음 THEN 기타감염 (75%)
IF 체온 ≤ 38°C AND 두통 있음 THEN 스트레스 (80%)
IF 체온 ≤ 38°C AND 두통 없음 THEN 정상 (95%)
```

### 한계점
- **고차원 데이터**: 차원이 높으면 성능 저하
- **연속형 관계**: 선형/곡선 관계 표현 어려움
- **불균형 데이터**: 소수 클래스 무시 경향
- **노이즈 민감성**: 이상치에 영향받기 쉬움

### 개선 방법
- **앙상블**: Random Forest, Gradient Boosting
- **특성 엔지니어링**: 도메인 지식 활용한 특성 생성
- **하이브리드**: 다른 알고리즘과 결합
- **정규화**: 가지치기 매개변수 조정

## 8. 용어 사전

### 기본 용어
- **노드(Node)**: 트리의 각 분기점 또는 끝점
- **분할(Split)**: 데이터를 나누는 조건
- **깊이(Depth)**: 루트에서 특정 노드까지의 거리
- **높이(Height)**: 트리의 최대 깊이
- **순도(Purity)**: 노드 내 클래스 동질성 정도

### 고급 용어
- **가지치기(Pruning)**: 과적합 방지를 위한 트리 단순화
- **불순도(Impurity)**: 노드 내 클래스 혼재 정도
- **정보 이득(Information Gain)**: 분할로 인한 정보량 증가
- **분할 정보(Split Information)**: 분할 자체의 정보량
- **비용 복잡도(Cost Complexity)**: 트리 크기와 오차의 균형 지표

### 수식 기호
- **S**: 샘플 집합
- **p_i**: 클래스 i의 비율
- **c**: 클래스 개수
- **A**: 분할 속성
- **IG**: 정보 이득
- **H**: 엔트로피
- **G**: 지니 불순도

### 개념 간 연관성
```
의사결정나무 학습 파이프라인:

┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ 불순도 측정 │───▶│분할 기준 선택│───▶│ 트리 구축   │
│             │    │             │    │             │
│ • 엔트로피  │    │ • 정보 이득 │    │ • 재귀적    │
│ • 지니불순도│    │ • 이득 비율 │    │   분할      │
│ • 분류 오차 │    │ • 지니 감소 │    │ • 헌트 알고 │
└─────────────┘    └─────────────┘    └─────────────┘
                                             │
                                             ▼
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ 최종 모델   │◀───│ 가지치기    │◀───│ 과적합 탐지 │
│             │    │             │    │             │
│ • 예측 성능 │    │ • 사전가지  │    │ • 복잡도    │
│ • 해석 가능 │    │ • 사후가지  │    │ • 일반화    │
│ • 규칙 추출 │    │ • 정규화    │    │ • 검증 성능 │
└─────────────┘    └─────────────┘    └─────────────┘
```

## 9. 실습 준비사항

### 필요 라이브러리
```python
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
```

### 주요 매개변수
- **criterion**: 'gini', 'entropy', 'log_loss'
- **max_depth**: 트리 최대 깊이
- **min_samples_split**: 분할 최소 샘플 수
- **min_samples_leaf**: 리프 노드 최소 샘플 수
- **random_state**: 재현 가능한 결과를 위한 시드

이제 실습을 통해 의사결정나무의 실제 동작을 확인해보겠습니다!
## 10. 
결정경계(Decision Boundary)

### 축 정렬 경계의 특성
의사결정나무는 **축 정렬(axis-aligned) 결정 경계**를 생성합니다:
- 각 분할이 단일 속성을 기준으로 수행됨
- 결과적으로 직사각형 형태의 영역들로 공간이 분할됨
- 2차원에서는 수직선과 수평선으로만 구성된 경계

### 경계 형성 과정
```
2차원 공간에서의 축 정렬 결정 경계:

Y축 ↑
    │
    │    R2        │    R3
    │              │
t22 ├──────────────┼──────────
    │              │
    │    R1        │    R4
    │              │
t21 ├──────────────┼──────────
    │              │
    │       R5     │
    │              │
    └──────────────┼──────────► X축
                  t11        t12

분할 순서:
1. X < t11 (수직선) → 좌측과 우측으로 분할
2. 우측에서 Y < t21 (수평선) → R4와 상단 영역
3. 상단에서 Y < t22 (수평선) → R3와 R2
4. 좌측에서 Y < t21 (수평선) → R5와 R1

결과: 5개의 직사각형 영역 (R1~R5)
각 영역은 하나의 클래스로 분류됨
```

### 다른 알고리즘과의 경계 비교
- **의사결정나무**: 축 정렬, 직사각형 경계
- **로지스틱 회귀**: 선형 경계
- **SVM**: 선형 또는 비선형 경계 (커널에 따라)
- **신경망**: 복잡한 비선형 경계

## 11. 상호작용과 한계 (심화)

### XOR 문제
**문제 상황**: X와 Y 각각의 엔트로피가 높게 나와 개별 분할 기준으로는 유의미하지 않지만, 함께 고려하면 구분 가능한 경우

**예시**:
```
XOR 진리표:                XOR 데이터 분포:
┌───┬───┬─────────┐        
│ X │ Y │ 클래스  │         Y ↑
├───┼───┼─────────┤           │ B │ A
│ 0 │ 0 │   A     │         1 ├───┼───
│ 0 │ 1 │   B     │           │ A │ B  
│ 1 │ 0 │   B     │         0 └───┼───► X
│ 1 │ 1 │   A     │             0   1
└───┴───┴─────────┘

개별 특성 분석:
X=0: A(1개), B(1개) → 엔트로피 = 1.0 (최대)
X=1: A(1개), B(1개) → 엔트로피 = 1.0 (최대)
Y=0: A(1개), B(1개) → 엔트로피 = 1.0 (최대)  
Y=1: A(1개), B(1개) → 엔트로피 = 1.0 (최대)

→ 개별적으로는 전혀 유용하지 않음!
→ 하지만 X AND Y 조합으로는 완벽 분류 가능
```

**문제점**: 
- X 단독으로는 엔트로피 ≈ 0.99 (높음)
- Y 단독으로는 엔트로피 ≈ 0.99 (높음)
- 탐욕적 기준으로는 둘 다 좋지 않은 분할로 판단

### 무관 속성의 영향
**상황**: 무관(잡음) 속성 Z가 추가된 경우
- Z의 엔트로피가 약간 낮다면 (예: 0.98)
- 탐욕적 기준이 Z를 먼저 선택하는 잘못된 분할로 이어질 수 있음

### 비축정렬 경계의 한계
**문제**: 서로 다른 비스듬한(비직교) 경계가 필요한 분포
- 예: (8,8) vs (12,12) 중심의 비대칭 가우시안 분포
- 의사결정나무는 많은 축정렬 분할을 필요로 하며 비효율적

```
이상적 경계 vs 의사결정나무 경계:

이상적 (대각선 경계):        의사결정나무 (계단식 경계):
Y ↑                         Y ↑
  │    ○○○                   │    ○○○
  │  ○○○○                    │  ○○○○  ┌─┐
  │○○○○ ╱                    │○○○○   │ │
  │○○ ╱ ●●                   │○○ ┌───┘ │●●
  │ ╱ ●●●●                   │  │     │●●●●
  │╱●●●●●                    │  │     └─●●●●
  └────────► X               └──┴───────► X

한 번의 대각선으로 완벽 분리    여러 번의 수직/수평선 필요
→ 간단하고 효율적             → 복잡하고 비효율적
```

**해결 방안**: 
- 앙상블 방법 (랜덤포레스트, 그래디언트 부스팅)
- 특성 엔지니어링으로 상호작용 특성 생성
- 다른 알고리즘과의 하이브리드 접근

## 12. 주요 알고리즘 변형

### 대표 알고리즘
- **Hunt**: 초기 기본 알고리즘
- **CART**: Classification and Regression Trees
- **ID3**: Iterative Dichotomiser 3 (엔트로피 기반)
- **C4.5**: ID3의 개선 버전 (이득 비율 사용)
- **SLIQ, SPRINT**: 대용량 데이터 처리용

### 알고리즘별 특징
```
의사결정나무 알고리즘 계보:

Hunt (1960년대)
    │
    ├─ ID3 (1986) ─── C4.5 (1993)
    │   • 엔트로피      • 이득비율
    │   • 정보이득      • 연속속성 처리
    │   • 범주형만      • 결측값 처리
    │
    └─ CART (1984)
        • 지니 불순도
        • 이진 분할만
        • 회귀 지원
        • 가지치기 내장

현대 구현체:
├─ scikit-learn: CART 기반
├─ R의 tree: CART 기반  
├─ C50: C4.5 상업 버전
└─ SLIQ/SPRINT: 대용량 데이터용
```

**주요 차이점**:
- **ID3/C4.5**: 정보이득/이득비율 사용, 범주형 속성 중심
- **CART**: 지니 불순도 사용, 이진 분할만 수행, 회귀도 지원
- **SLIQ/SPRINT**: 메모리 효율적인 대용량 데이터 처리
