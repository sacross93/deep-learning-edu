# 로지스틱 회귀 완전 이론 가이드

## 1. 개요 및 핵심 개념

### 로지스틱 회귀란?
로지스틱 회귀(Logistic Regression)는 **범주형 변수를 예측**하기 위한 지도학습 알고리즘입니다. 선형 회귀와 달리 연속적인 값이 아닌 **확률**을 출력하여 분류 문제를 해결합니다.

### 핵심 특징
- **이진 분류**: 주로 0 또는 1의 이진 결과 예측
- **확률 기반**: 특정 클래스에 속할 확률을 0~1 사이 값으로 출력
- **선형 결정 경계**: 특성들의 선형 결합으로 결정 경계 형성
- **해석 가능성**: 각 특성의 영향도를 계수로 해석 가능

### 주요 용도
- 의료 진단 (질병 유무 예측)
- 마케팅 (구매 의향 예측)
- 금융 (신용 위험 평가)
- 스팸 메일 분류

## 2. 동작 원리

### 2.1 시그모이드 함수 (Sigmoid Function)

로지스틱 회귀의 핵심은 **시그모이드 함수**입니다:

```
σ(z) = 1 / (1 + e^(-z))
```

여기서 z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ (선형 결합)

**시그모이드 함수의 특성:**
- 입력값이 -∞에서 +∞까지 가능
- 출력값은 항상 0과 1 사이
- S자 모양의 곡선
- z=0일 때 σ(z) = 0.5
- 미분 가능하여 최적화에 적합

### 2.2 확률 해석

로지스틱 회귀는 다음과 같이 확률을 모델링합니다:

```
P(Y=1|X) = σ(β₀ + β₁x₁ + ... + βₙxₙ)
P(Y=0|X) = 1 - P(Y=1|X)
```

### 2.3 로그 오즈 (Log Odds)

**오즈(Odds)**: 성공 확률 / 실패 확률
```
Odds = P(Y=1) / P(Y=0) = P(Y=1) / (1-P(Y=1))
```

**로그 오즈(Log Odds)**: 오즈의 자연로그
```
log(Odds) = log(P(Y=1) / (1-P(Y=1))) = β₀ + β₁x₁ + ... + βₙxₙ
```

로그 오즈는 선형 함수이므로 해석이 용이합니다.

### 2.4 결정 경계

일반적으로 확률이 0.5 이상이면 클래스 1, 미만이면 클래스 0으로 분류:
```
P(Y=1|X) ≥ 0.5 → 클래스 1
P(Y=1|X) < 0.5 → 클래스 0
```

## 3. 파라미터 구성

### 3.1 주요 파라미터

**회귀 계수 (β)**:
- β₀: 절편 (intercept)
- β₁, β₂, ..., βₙ: 각 특성의 가중치

**계수 해석**:
- βᵢ > 0: 해당 특성이 증가하면 클래스 1일 확률 증가
- βᵢ < 0: 해당 특성이 증가하면 클래스 1일 확률 감소
- |βᵢ|가 클수록 해당 특성의 영향력이 큼

### 3.2 하이퍼파라미터

**정규화 (Regularization)**:
- **C (역정규화 강도)**: 작을수록 강한 정규화
- **penalty**: 'l1' (Lasso), 'l2' (Ridge), 'elasticnet'

**최적화 관련**:
- **solver**: 'liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'
- **max_iter**: 최대 반복 횟수
- **tol**: 수렴 허용 오차

**기타**:
- **class_weight**: 클래스 불균형 처리
- **random_state**: 재현 가능한 결과를 위한 시드

## 4. 성능 평가 방법

### 4.1 분류 성능 지표

**정확도 (Accuracy)**:
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**정밀도 (Precision)**:
```
Precision = TP / (TP + FP)
```

**재현율 (Recall)**:
```
Recall = TP / (TP + FN)
```

**F1-Score**:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

### 4.2 확률 기반 평가

**ROC 곡선**: 다양한 임계값에서 TPR vs FPR
**AUC**: ROC 곡선 아래 면적 (0.5~1.0, 높을수록 좋음)
**로그 손실 (Log Loss)**: 확률 예측의 정확성 측정

### 4.3 혼동 행렬 (Confusion Matrix)

```
              예측
실제    0    1
  0    TN   FP
  1    FN   TP
```

## 5. 다른 알고리즘과의 비교

### 5.1 선형 회귀와의 차이점

| 특성 | 선형 회귀 | 로지스틱 회귀 |
|------|-----------|---------------|
| 출력 | 연속값 | 확률 (0~1) |
| 활성화 함수 | 없음 | 시그모이드 |
| 손실 함수 | MSE | 로그 손실 |
| 용도 | 회귀 | 분류 |
| 결정 경계 | 없음 | 선형 |

### 5.2 다른 분류 알고리즘과의 비교

**vs 의사결정나무**:
- 유사점: 해석 가능성
- 차이점: 선형 vs 비선형 결정 경계

**vs SVM**:
- 유사점: 선형 결정 경계 (선형 SVM)
- 차이점: 확률 출력 vs 거리 기반 분류

**vs 나이브 베이즈**:
- 유사점: 확률 기반 분류
- 차이점: 특성 독립 가정 없음 vs 있음

### 5.3 장단점 분석

**장점**:
- 확률 해석 가능
- 계산 효율성
- 정규화 지원
- 다중 클래스 확장 가능
- 특성 중요도 해석 용이

**단점**:
- 선형 관계만 모델링
- 이상치에 민감
- 특성 스케일링 필요
- 완전 분리 시 수렴 문제

## 6. 적용 사례 및 한계

### 6.1 실제 적용 분야

**의료 분야**:
- 질병 진단 (암 여부, 심장병 위험)
- 치료 효과 예측
- 약물 반응 예측

**비즈니스**:
- 고객 이탈 예측
- 구매 확률 예측
- 신용 승인/거부

**마케팅**:
- 이메일 응답률 예측
- 광고 클릭률 예측
- 고객 세분화

### 6.2 알고리즘의 한계

**선형성 가정**:
- 특성 간 비선형 관계 모델링 불가
- 복잡한 패턴 학습 제한

**이상치 민감성**:
- 극값이 모델에 큰 영향
- 로버스트하지 않음

**완전 분리 문제**:
- 데이터가 완전히 분리 가능할 때 수렴하지 않음
- 정규화로 해결 가능

### 6.3 사용 시 주의사항

**데이터 전처리**:
- 특성 스케일링 필수
- 범주형 변수 인코딩
- 결측치 처리

**모델 검증**:
- 교차 검증 사용
- 과적합 방지 (정규화)
- 클래스 불균형 고려

## 7. 최대우도추정 (Maximum Likelihood Estimation)

### 7.1 우도 함수

로지스틱 회귀는 최대우도추정을 통해 파라미터를 학습합니다.

**우도 함수**:
```
L(β) = ∏ᵢ P(yᵢ|xᵢ, β)
```

**로그 우도**:
```
ℓ(β) = Σᵢ [yᵢ log(pᵢ) + (1-yᵢ) log(1-pᵢ)]
```

여기서 pᵢ = σ(β₀ + β₁xᵢ₁ + ... + βₙxᵢₙ)

### 7.2 최적화

**목표**: 로그 우도를 최대화하는 β 찾기
**방법**: 경사 상승법 (Gradient Ascent) 또는 뉴턴-랩슨 방법

**경사 벡터**:
```
∂ℓ/∂β = Σᵢ (yᵢ - pᵢ) xᵢ
```

## 8. 용어 사전

**시그모이드 함수**: S자 모양의 활성화 함수, 출력을 0~1 사이로 제한
**오즈**: 성공 확률과 실패 확률의 비율
**로그 오즈**: 오즈의 자연로그, 선형 관계 표현
**최대우도추정**: 관측 데이터의 우도를 최대화하는 파라미터 추정 방법
**정규화**: 과적합 방지를 위한 패널티 항 추가
**로그 손실**: 확률 예측의 정확성을 측정하는 손실 함수
**완전 분리**: 클래스가 완전히 분리되어 무한대 계수를 가지는 문제
**다중공선성**: 독립변수 간 높은 상관관계로 인한 불안정성

## 9. 수식 기호 설명

- **σ(z)**: 시그모이드 함수
- **β**: 회귀 계수 벡터
- **P(Y=1|X)**: 주어진 X에서 Y=1일 조건부 확률
- **ℓ(β)**: 로그 우도 함수
- **C**: 정규화 강도의 역수
- **TP, TN, FP, FN**: 참양성, 참음성, 거짓양성, 거짓음성

## 10. 개념 간 연관성

```
선형 결합 → 시그모이드 함수 → 확률 출력
     ↓              ↓              ↓
  로그 오즈    →   오즈 비율   →   분류 결정
     ↓              ↓              ↓
최대우도추정  →   파라미터 학습 →  모델 완성
```

로지스틱 회귀는 선형 회귀의 확장으로, 시그모이드 함수를 통해 분류 문제에 적용할 수 있게 만든 강력하면서도 해석 가능한 알고리즘입니다.