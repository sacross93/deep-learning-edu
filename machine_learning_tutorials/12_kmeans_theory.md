# K-평균 클러스터링 완전 이론 가이드

## 1. 개요 및 핵심 개념

### K-평균 클러스터링이란?
K-평균(K-means) 클러스터링은 가장 널리 사용되는 비지도 학습 알고리즘 중 하나로, 주어진 데이터를 K개의 클러스터로 분할하는 방법입니다. 각 클러스터는 중심점(centroid)으로 대표되며, 데이터 포인트들은 가장 가까운 중심점에 할당됩니다.

### 핵심 개념
- **클러스터(Cluster)**: 유사한 특성을 가진 데이터 포인트들의 그룹
- **중심점(Centroid)**: 각 클러스터의 중심을 나타내는 점
- **클러스터 할당**: 각 데이터 포인트를 가장 가까운 중심점의 클러스터에 배정
- **목적 함수**: 클러스터 내 제곱합(Within-Cluster Sum of Squares, WCSS) 최소화

### 기본 가정
- 클러스터는 구형(spherical) 모양을 가짐
- 클러스터의 크기가 비슷함
- 클러스터의 밀도가 유사함
- 특성들이 비슷한 스케일을 가짐

## 2. 동작 원리

### K-평균 알고리즘 단계

#### 1단계: 초기화
- K개의 중심점을 임의로 선택하거나 특정 방법으로 초기화
- 일반적으로 데이터 범위 내에서 무작위로 선택

#### 2단계: 할당 (Assignment)
- 각 데이터 포인트를 가장 가까운 중심점의 클러스터에 할당
- 거리 측정은 주로 유클리드 거리 사용:
  ```
  d(x, c) = √Σ(xi - ci)²
  ```

#### 3단계: 업데이트 (Update)
- 각 클러스터의 새로운 중심점을 해당 클러스터에 속한 모든 점들의 평균으로 계산:
  ```
  ci = (1/|Si|) Σ(x∈Si) x
  ```
  여기서 Si는 클러스터 i에 속한 점들의 집합

#### 4단계: 수렴 확인
- 중심점이 더 이상 변하지 않거나 변화량이 임계값 이하일 때 종료
- 또는 최대 반복 횟수에 도달했을 때 종료

### 수렴 조건
1. **중심점 변화량**: 이전 반복과 현재 반복의 중심점 차이가 임계값(ε) 이하
2. **할당 변화**: 데이터 포인트의 클러스터 할당이 변하지 않음
3. **목적 함수 변화**: WCSS의 감소량이 임계값 이하
4. **최대 반복 횟수**: 미리 정한 최대 반복 횟수 도달

## 3. 파라미터 구성

### 주요 하이퍼파라미터

#### K (클러스터 개수)
- **의미**: 데이터를 분할할 클러스터의 개수
- **선택 방법**: 엘보우 방법, 실루엣 분석, Gap 통계량
- **영향**: K가 너무 작으면 과소분할, 너무 크면 과분할

#### 초기화 방법 (init)
- **random**: 무작위 초기화
- **k-means++**: 중심점들이 서로 멀리 떨어지도록 초기화
- **사용자 정의**: 도메인 지식을 활용한 초기화

#### 최대 반복 횟수 (max_iter)
- **기본값**: 보통 300
- **조정**: 복잡한 데이터의 경우 증가 필요

#### 허용 오차 (tol)
- **의미**: 수렴 판정을 위한 임계값
- **기본값**: 1e-4
- **조정**: 정밀도와 계산 시간의 트레이드오프

#### 실행 횟수 (n_init)
- **의미**: 다른 초기화로 알고리즘을 실행하는 횟수
- **목적**: 지역 최적해 문제 완화
- **기본값**: 10

## 4. 성능 평가 방법

### 내부 평가 지표 (레이블 없음)

#### 1. 엘보우 방법 (Elbow Method)
- WCSS를 K에 대해 플롯
- WCSS 감소율이 급격히 줄어드는 지점(엘보우)을 최적 K로 선택
- 수식: WCSS = ΣΣ ||x - ci||²

#### 2. 실루엣 분석 (Silhouette Analysis)
- 각 점의 실루엣 계수 계산:
  ```
  s(i) = (b(i) - a(i)) / max(a(i), b(i))
  ```
- a(i): 같은 클러스터 내 다른 점들과의 평균 거리
- b(i): 가장 가까운 다른 클러스터 점들과의 평균 거리
- 범위: [-1, 1], 1에 가까울수록 좋음

#### 3. Calinski-Harabasz 지수
- 클러스터 간 분산과 클러스터 내 분산의 비율
- 높을수록 좋은 클러스터링

#### 4. Davies-Bouldin 지수
- 클러스터 내 거리와 클러스터 간 거리의 비율
- 낮을수록 좋은 클러스터링

### 외부 평가 지표 (레이블 있음)

#### 1. 조정된 랜드 지수 (Adjusted Rand Index, ARI)
- 실제 레이블과 클러스터링 결과의 일치도
- 범위: [-1, 1], 1에 가까울수록 좋음

#### 2. 정규화된 상호정보량 (Normalized Mutual Information, NMI)
- 실제 레이블과 클러스터링 결과 간의 정보 공유 정도
- 범위: [0, 1], 1에 가까울수록 좋음

## 5. 다른 알고리즘과의 비교

### 계층적 클러스터링과의 비교

#### 유사점
- 둘 다 거리 기반 클러스터링
- 클러스터 개수 결정 필요
- 구형 클러스터에 적합

#### 차이점
| 특성 | K-평균 | 계층적 클러스터링 |
|------|--------|------------------|
| 계산 복잡도 | O(nkt) | O(n³) |
| 클러스터 개수 | 사전 지정 필요 | 덴드로그램으로 결정 |
| 결과 | 분할형 | 계층형 |
| 대용량 데이터 | 적합 | 부적합 |
| 클러스터 모양 | 구형 | 다양한 모양 |

### DBSCAN과의 비교

#### 유사점
- 비지도 학습 클러스터링
- 데이터 그룹화 목적

#### 차이점
| 특성 | K-평균 | DBSCAN |
|------|--------|---------|
| 클러스터 개수 | 사전 지정 | 자동 결정 |
| 클러스터 모양 | 구형 | 임의 모양 |
| 이상치 처리 | 모든 점 할당 | 이상치 탐지 |
| 밀도 | 균등 밀도 가정 | 밀도 기반 |
| 파라미터 | K | eps, min_samples |

## 6. 적용 사례 및 한계

### 실제 적용 분야

#### 1. 고객 세분화 (Customer Segmentation)
- 구매 패턴, 인구통계학적 특성 기반 고객 그룹화
- 마케팅 전략 수립, 개인화 서비스 제공

#### 2. 이미지 분할 (Image Segmentation)
- 픽셀 색상 정보를 이용한 이미지 영역 분할
- 의료 영상 분석, 컴퓨터 비전

#### 3. 시장 세분화 (Market Segmentation)
- 제품 특성, 가격대별 시장 구분
- 경쟁 분석, 포지셔닝 전략

#### 4. 유전자 분석
- 유전자 발현 패턴 기반 그룹화
- 질병 분류, 약물 반응 예측

#### 5. 추천 시스템
- 사용자 선호도 패턴 분석
- 협업 필터링의 기초 단계

### 알고리즘의 한계

#### 1. 클러스터 개수 사전 지정
- K 값을 미리 알아야 함
- 최적 K 찾기가 어려움

#### 2. 초기화 민감성
- 초기 중심점 위치에 따라 결과 달라짐
- 지역 최적해에 빠질 가능성

#### 3. 클러스터 모양 제한
- 구형 클러스터만 잘 찾음
- 길쭉하거나 복잡한 모양 클러스터 부적합

#### 4. 이상치 민감성
- 이상치가 중심점 위치에 큰 영향
- 클러스터 품질 저하

#### 5. 스케일 민감성
- 특성들의 스케일 차이에 민감
- 정규화 전처리 필수

#### 6. 밀도 차이 처리 어려움
- 클러스터 크기가 다를 때 성능 저하
- 균등한 밀도 가정

### 사용 시 주의사항

#### 1. 데이터 전처리
- 특성 정규화/표준화 필수
- 이상치 제거 고려
- 범주형 변수 처리 방법 결정

#### 2. K 값 선택
- 여러 방법으로 최적 K 탐색
- 도메인 지식 활용
- 비즈니스 목적 고려

#### 3. 초기화 전략
- k-means++ 사용 권장
- 여러 번 실행 후 최적 결과 선택
- 안정성 확인

#### 4. 결과 해석
- 클러스터 특성 분석
- 비즈니스 의미 부여
- 시각화를 통한 검증

## 7. 용어 사전

### 핵심 용어

- **중심점(Centroid)**: 클러스터의 중심을 나타내는 점, 해당 클러스터 모든 점들의 평균
- **클러스터 내 제곱합(WCSS)**: Within-Cluster Sum of Squares, 각 점과 중심점 간 거리의 제곱합
- **엘보우 방법**: WCSS 감소율이 급격히 줄어드는 지점을 찾아 최적 K를 결정하는 방법
- **실루엣 계수**: 각 점이 자신의 클러스터에 얼마나 잘 속해있는지 측정하는 지표
- **k-means++**: 초기 중심점들이 서로 멀리 떨어지도록 선택하는 초기화 방법

### 수학 기호

- **K**: 클러스터 개수
- **n**: 데이터 포인트 개수
- **d**: 특성(차원) 개수
- **ci**: i번째 클러스터의 중심점
- **Si**: i번째 클러스터에 속한 점들의 집합
- **||·||**: 유클리드 거리 (L2 norm)
- **ε**: 수렴 판정을 위한 허용 오차

### 평가 지표

- **ARI**: Adjusted Rand Index, 조정된 랜드 지수
- **NMI**: Normalized Mutual Information, 정규화된 상호정보량
- **CH 지수**: Calinski-Harabasz 지수
- **DB 지수**: Davies-Bouldin 지수
- **Gap 통계량**: 클러스터링 품질을 평가하는 통계적 방법

### 개념 간 연관성

```
K-평균 클러스터링
├── 목적 함수 (WCSS 최소화)
├── 알고리즘 단계
│   ├── 초기화 → 할당 → 업데이트 → 수렴 확인
│   └── 반복적 최적화 과정
├── 파라미터 선택
│   ├── K 값 결정 (엘보우, 실루엣)
│   └── 초기화 방법 (k-means++)
├── 성능 평가
│   ├── 내부 지표 (실루엣, CH, DB)
│   └── 외부 지표 (ARI, NMI)
└── 한계 및 개선
    ├── 초기화 민감성 → k-means++
    ├── 지역 최적해 → 다중 실행
    └── 클러스터 모양 제한 → 다른 알고리즘 고려
```

## 8. 고급 주제

### K-평균 변형 알고리즘

#### 1. K-평균++ (K-means++)
- 개선된 초기화 방법
- 중심점들이 서로 멀리 떨어지도록 선택
- 수렴 속도 향상 및 안정성 증대

#### 2. Mini-batch K-평균
- 전체 데이터 대신 미니배치 사용
- 대용량 데이터에 적합
- 계산 속도 향상, 메모리 효율성

#### 3. K-평균++
- 구형이 아닌 클러스터 처리 개선
- 가우시안 혼합 모델과의 연관성
- 확률적 접근 방법

### 최적화 관점

#### 목적 함수
K-평균은 다음 목적 함수를 최소화:
```
J = ΣΣ ||xi - cj||²
```

#### 수렴 보장
- 목적 함수가 단조 감소
- 유한한 가능한 할당 조합
- 수렴 보장 (지역 최적해)

#### 계산 복잡도
- 시간 복잡도: O(n·k·d·t)
  - n: 데이터 포인트 수
  - k: 클러스터 수
  - d: 차원 수
  - t: 반복 횟수
- 공간 복잡도: O(n·d + k·d)

이 이론 가이드를 통해 K-평균 클러스터링의 핵심 개념부터 실제 적용까지 체계적으로 이해할 수 있습니다. 다음 실습에서는 이론을 바탕으로 실제 고객 세분화 데이터를 활용한 클러스터링을 수행해보겠습니다.