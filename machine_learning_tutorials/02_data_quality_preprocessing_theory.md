# 데이터 품질 및 전처리 완전 이론 가이드

## 1. 개요 및 중요성

### 데이터 품질의 정의
데이터 품질은 **데이터가 의도된 용도에 얼마나 적합한지를 나타내는 척도**입니다. 고품질 데이터는 정확하고, 완전하며, 일관성 있고, 시의적절하며, 관련성이 높은 특성을 가집니다.

### 전처리의 필요성
실제 데이터는 다음과 같은 문제들을 포함하고 있어 전처리가 필수적입니다:

- **불완전성**: 결측치, 누락된 속성값
- **잡음**: 측정 오류, 데이터 입력 실수
- **불일치성**: 서로 다른 데이터 소스 간 형식 차이
- **중복성**: 동일한 정보의 반복 저장
- **이상치**: 정상 범위를 벗어난 극값

### 전처리의 영향
- **모델 성능**: 전처리 품질이 최종 모델 성능의 80% 결정
- **학습 효율성**: 깨끗한 데이터로 더 빠른 수렴
- **해석 가능성**: 일관된 데이터로 명확한 패턴 발견
- **신뢰성**: 품질 높은 데이터로 안정적인 예측

## 2. 데이터 품질 문제 유형

### 2.1 결측치 (Missing Values)

#### 결측치 유형
1. **완전 무작위 결측 (MCAR: Missing Completely At Random)**
   - 결측이 다른 변수와 무관하게 발생
   - 예: 설문 응답자가 무작위로 일부 문항 미응답

2. **무작위 결측 (MAR: Missing At Random)**
   - 결측이 관측된 다른 변수와 관련
   - 예: 고소득자가 소득 문항을 더 자주 미응답

3. **비무작위 결측 (MNAR: Missing Not At Random)**
   - 결측이 결측값 자체와 관련
   - 예: 우울증 환자가 우울증 관련 문항 미응답

#### 결측치 탐지 방법
```python
# 결측치 패턴 분석
- 결측치 비율 계산
- 결측치 패턴 시각화 (히트맵)
- 결측치 간 상관관계 분석
```

### 2.2 잡음 (Noise)

#### 잡음의 원인
- **측정 오류**: 센서 오작동, 측정 도구 부정확성
- **데이터 입력 오류**: 수동 입력 시 오타, 형식 오류
- **전송 오류**: 네트워크 전송 중 데이터 손상
- **처리 오류**: 데이터 변환 과정에서 발생하는 오류

#### 잡음 유형
1. **가우시안 잡음**: 정규분포를 따르는 무작위 오류
2. **임펄스 잡음**: 극값으로 나타나는 산발적 오류
3. **균등 잡음**: 균등분포를 따르는 무작위 오류

### 2.3 이상치 (Outliers)

#### 이상치 유형
1. **전역 이상치**: 전체 데이터셋에서 비정상적인 값
2. **맥락적 이상치**: 특정 맥락에서만 비정상적인 값
3. **집단 이상치**: 개별적으로는 정상이지만 집단적으로 비정상

#### 이상치 원인
- **측정 오류**: 잘못된 측정이나 기록
- **자연적 변이**: 실제로 존재하는 극값
- **데이터 입력 오류**: 잘못된 데이터 입력
- **실험적 오류**: 실험 설계나 수행 과정의 문제

### 2.4 불일치성 (Inconsistency)

#### 불일치성 유형
- **형식 불일치**: 날짜 형식, 단위 차이
- **명명 불일치**: 동일 개체의 서로 다른 표현
- **스케일 불일치**: 서로 다른 측정 단위
- **인코딩 불일치**: 문자 인코딩, 범주 코딩 차이

## 3. 데이터 정제 (Data Cleaning)

### 3.1 결측치 처리 방법

#### 삭제 기법 (Deletion Methods)
1. **완전 사례 분석 (Complete Case Analysis)**
   - 결측치가 있는 모든 행 삭제
   - 장점: 간단하고 편향 없음 (MCAR인 경우)
   - 단점: 데이터 손실, 표본 크기 감소

2. **가용 사례 분석 (Available Case Analysis)**
   - 분석별로 필요한 변수만 고려
   - 장점: 더 많은 데이터 활용
   - 단점: 분석 간 표본 크기 차이

#### 대체 기법 (Imputation Methods)
1. **단순 대체 (Simple Imputation)**
   - 평균/중앙값/최빈값 대체
   - 상수값 대체 (0, -999 등)
   - 장점: 구현 간단, 빠른 처리
   - 단점: 분산 축소, 관계 왜곡

2. **고급 대체 (Advanced Imputation)**
   - **회귀 대체**: 다른 변수들로 결측값 예측
   - **핫덱 대체**: 유사한 케이스의 값으로 대체
   - **다중 대체**: 여러 번 대체하여 불확실성 반영
   - **KNN 대체**: 가장 유사한 K개 이웃의 평균값 사용

### 3.2 잡음 제거 방법

#### 통계적 방법
1. **이동 평균 (Moving Average)**
   - 시계열 데이터의 단기 변동 제거
   - 윈도우 크기에 따라 평활화 정도 조절

2. **중앙값 필터 (Median Filter)**
   - 임펄스 잡음 제거에 효과적
   - 엣지 정보 보존 가능

3. **가우시안 필터 (Gaussian Filter)**
   - 가우시안 잡음 제거에 효과적
   - 전체적인 평활화 효과

#### 기계학습 방법
1. **회귀 기반 평활화**
   - 다항식 회귀로 추세 추출
   - 잔차를 잡음으로 간주

2. **클러스터링 기반**
   - 정상 데이터 클러스터 식별
   - 클러스터 중심으로 값 보정

### 3.3 이상치 탐지 및 처리

#### 통계적 방법
1. **Z-점수 (Z-Score)**
   - 표준화된 값으로 이상치 탐지
   - 임계값: |z| > 2 또는 3

2. **IQR 방법 (Interquartile Range)**
   - Q1 - 1.5×IQR 미만 또는 Q3 + 1.5×IQR 초과
   - 비모수적 방법으로 분포 가정 불필요

3. **수정된 Z-점수 (Modified Z-Score)**
   - 중앙값과 MAD 사용
   - 이상치에 더 강건

#### 기계학습 방법
1. **Isolation Forest**
   - 이상치를 더 쉽게 분리 가능하다는 원리
   - 고차원 데이터에 효과적

2. **One-Class SVM**
   - 정상 데이터만으로 학습
   - 복잡한 경계 학습 가능

3. **Local Outlier Factor (LOF)**
   - 지역적 밀도 기반 이상치 탐지
   - 맥락적 이상치 탐지 가능

#### 이상치 처리 방법
1. **제거**: 명확한 오류인 경우
2. **변환**: 로그 변환 등으로 영향 감소
3. **윈저화**: 극값을 특정 백분위수로 대체
4. **별도 분석**: 이상치만 따로 분석

## 4. 데이터 변환 (Data Transformation)

### 4.1 정규화 (Normalization)

#### Min-Max 정규화
- 값을 [0, 1] 범위로 변환
- 공식: (x - min) / (max - min)
- 장점: 해석 용이, 범위 고정
- 단점: 이상치에 민감

#### Z-점수 정규화 (표준화)
- 평균 0, 표준편차 1로 변환
- 공식: (x - μ) / σ
- 장점: 이상치에 상대적으로 강건
- 단점: 범위가 고정되지 않음

#### 로버스트 스케일링
- 중앙값과 IQR 사용
- 공식: (x - median) / IQR
- 장점: 이상치에 매우 강건
- 단점: 정규분포 가정 불필요

### 4.2 이산화 (Discretization)

#### 등폭 이산화 (Equal-Width)
- 값의 범위를 동일한 폭으로 분할
- 장점: 구현 간단
- 단점: 데이터 분포 무시

#### 등빈도 이산화 (Equal-Frequency)
- 각 구간에 동일한 개수의 데이터 포함
- 장점: 균등한 분포
- 단점: 구간 경계가 불규칙

#### 엔트로피 기반 이산화
- 정보 이득을 최대화하는 분할점 선택
- 장점: 목표 변수 고려
- 단점: 계산 복잡도 높음

### 4.3 속성 생성 (Feature Engineering)

#### 파생 속성 생성
1. **수학적 변환**
   - 로그, 제곱근, 거듭제곱 변환
   - 삼각함수, 지수함수 적용

2. **조합 속성**
   - 기존 속성들의 합, 차, 곱, 나눗셈
   - 비율, 백분율 계산

3. **시간 기반 속성**
   - 날짜에서 연도, 월, 요일 추출
   - 시간 간격, 주기성 계산

#### 범주형 변수 처리
1. **원-핫 인코딩 (One-Hot Encoding)**
   - 각 범주를 이진 변수로 변환
   - 장점: 순서 가정 없음
   - 단점: 차원 증가

2. **라벨 인코딩 (Label Encoding)**
   - 범주를 정수로 변환
   - 장점: 차원 증가 없음
   - 단점: 순서 관계 암시

3. **타겟 인코딩 (Target Encoding)**
   - 범주별 목표 변수 평균으로 인코딩
   - 장점: 예측력 높음
   - 단점: 과적합 위험

## 5. 데이터 축소 (Data Reduction)

### 5.1 차원 축소 (Dimensionality Reduction)

#### 특성 선택 (Feature Selection)
1. **필터 방법 (Filter Methods)**
   - 통계적 측도로 특성 평가
   - 상관계수, 카이제곱 검정, 정보 이득

2. **래퍼 방법 (Wrapper Methods)**
   - 모델 성능으로 특성 부분집합 평가
   - 전진 선택, 후진 제거, 단계적 선택

3. **임베디드 방법 (Embedded Methods)**
   - 모델 학습 과정에서 특성 선택
   - LASSO, Ridge, 의사결정나무

#### 특성 추출 (Feature Extraction)
1. **주성분 분석 (PCA)**
   - 분산을 최대화하는 주성분 추출
   - 선형 변환, 해석 어려움

2. **선형 판별 분석 (LDA)**
   - 클래스 분리를 최대화하는 축 찾기
   - 지도학습 방식

3. **독립 성분 분석 (ICA)**
   - 통계적으로 독립인 성분 추출
   - 신호 분리에 효과적

### 5.2 수치 축소 (Numerosity Reduction)

#### 샘플링 (Sampling)
1. **단순 무작위 샘플링**
   - 모든 데이터가 동일한 선택 확률
   - 장점: 편향 없음
   - 단점: 희소 클래스 누락 가능

2. **층화 샘플링 (Stratified Sampling)**
   - 각 층에서 비례적으로 샘플링
   - 장점: 클래스 비율 유지
   - 단점: 층 정보 필요

3. **클러스터 샘플링**
   - 클러스터 단위로 샘플링
   - 장점: 지리적 분산 고려
   - 단점: 클러스터 내 유사성

#### 집계 (Aggregation)
1. **시간 집계**
   - 일별 → 월별, 시간별 → 일별
   - 계절성, 추세 패턴 보존

2. **공간 집계**
   - 세부 지역 → 광역 지역
   - 지리적 패턴 단순화

3. **범주 집계**
   - 세부 범주 → 상위 범주
   - 희소 범주 문제 해결

## 6. 데이터 통합 (Data Integration)

### 6.1 스키마 통합

#### 엔티티 식별 문제
- **동일 엔티티의 다른 이름**: Customer vs Client
- **다른 엔티티의 동일 이름**: Date (주문일 vs 배송일)
- **동의어 처리**: 메타데이터, 도메인 지식 활용

#### 속성 통합
- **단위 통합**: 미터 vs 피트, 섭씨 vs 화씨
- **정밀도 통합**: 소수점 자릿수 차이
- **형식 통합**: 날짜 형식, 전화번호 형식

### 6.2 데이터 값 충돌 해결

#### 충돌 유형
1. **표현 차이**: "M/F" vs "Male/Female"
2. **스케일 차이**: 1-5점 vs 1-10점 척도
3. **시점 차이**: 서로 다른 시점의 데이터

#### 해결 방법
1. **수동 해결**: 도메인 전문가 판단
2. **자동 해결**: 규칙 기반, 통계적 방법
3. **메타데이터 활용**: 데이터 출처, 품질 정보

## 7. 전처리 파이프라인 설계

### 7.1 파이프라인 구성 요소

#### 순서 고려사항
1. **데이터 탐색** → 문제 파악
2. **결측치 처리** → 분석 가능한 형태로
3. **이상치 처리** → 모델 성능 향상
4. **변환/정규화** → 알고리즘 요구사항 충족
5. **특성 선택/추출** → 차원 축소
6. **검증** → 처리 결과 확인

#### 파이프라인 설계 원칙
- **재현 가능성**: 동일한 입력에 동일한 출력
- **확장 가능성**: 새로운 데이터에 적용 가능
- **모듈화**: 각 단계를 독립적으로 수정 가능
- **문서화**: 각 단계의 목적과 방법 기록

### 7.2 교차 검증과 전처리

#### 데이터 누출 방지
- **잘못된 방법**: 전체 데이터 전처리 → 분할
- **올바른 방법**: 분할 → 훈련 데이터만 전처리 → 테스트 적용

#### 파이프라인 검증
```python
# 올바른 교차 검증 파이프라인
for train_idx, val_idx in cv_folds:
    X_train, X_val = X[train_idx], X[val_idx]
    
    # 훈련 데이터로만 전처리 파라미터 학습
    scaler = StandardScaler().fit(X_train)
    
    # 훈련/검증 데이터에 동일한 변환 적용
    X_train_scaled = scaler.transform(X_train)
    X_val_scaled = scaler.transform(X_val)
```

## 8. 전처리 품질 평가

### 8.1 정량적 평가 지표

#### 완전성 (Completeness)
- 결측치 비율: (전체 값 - 결측값) / 전체 값
- 목표: 95% 이상 완전성 확보

#### 정확성 (Accuracy)
- 참조 데이터와의 일치도
- 도메인 규칙 위반 비율

#### 일관성 (Consistency)
- 중복 레코드 비율
- 형식 표준화 준수율

#### 유효성 (Validity)
- 데이터 타입 준수율
- 범위 제약 조건 만족률

### 8.2 정성적 평가 방법

#### 도메인 전문가 검토
- 비즈니스 규칙 준수 여부
- 실제 상황과의 일치성
- 예상 패턴과의 부합성

#### 시각적 검증
- 분포 변화 확인
- 이상치 탐지 결과 검토
- 변환 전후 비교

## 9. 도구 및 라이브러리

### 9.1 Python 라이브러리

#### 기본 라이브러리
- **pandas**: 데이터 조작 및 분석
- **numpy**: 수치 연산
- **scipy**: 통계 함수

#### 전처리 전용
- **scikit-learn**: 전처리 파이프라인
- **feature-engine**: 특성 엔지니어링
- **missingno**: 결측치 시각화

#### 시각화
- **matplotlib**: 기본 시각화
- **seaborn**: 통계 시각화
- **plotly**: 인터랙티브 시각화

### 9.2 전처리 자동화 도구

#### AutoML 플랫폼
- **H2O.ai**: 자동 전처리 및 모델링
- **DataRobot**: 엔터프라이즈 AutoML
- **Google AutoML**: 클라우드 기반 AutoML

#### 데이터 품질 도구
- **Great Expectations**: 데이터 품질 검증
- **Deequ**: 대규모 데이터 품질 측정
- **pandas-profiling**: 자동 EDA 리포트

## 10. 실무 적용 가이드라인

### 10.1 프로젝트 단계별 체크리스트

#### 초기 탐색 단계
- [ ] 데이터 구조 파악 (행/열 수, 데이터 타입)
- [ ] 결측치 패턴 분석
- [ ] 기본 통계량 확인
- [ ] 분포 시각화
- [ ] 이상치 탐지

#### 전처리 계획 단계
- [ ] 비즈니스 요구사항 확인
- [ ] 전처리 우선순위 결정
- [ ] 처리 방법 선택 및 근거 문서화
- [ ] 파이프라인 설계

#### 실행 및 검증 단계
- [ ] 단계별 전처리 실행
- [ ] 중간 결과 검증
- [ ] 최종 품질 평가
- [ ] 파이프라인 테스트

### 10.2 일반적인 실수와 해결책

#### 흔한 실수들
1. **데이터 누출**: 테스트 데이터 정보 사용
2. **과도한 전처리**: 원본 정보 손실
3. **일관성 부족**: 훈련/테스트 다른 처리
4. **도메인 지식 무시**: 맥락 없는 기계적 처리

#### 해결 방안
1. **엄격한 분할**: 전처리 전 데이터 분할
2. **점진적 접근**: 단계별 효과 확인
3. **파이프라인 사용**: 일관된 처리 보장
4. **도메인 전문가 협업**: 의미 있는 전처리

## 11. 고급 주제

### 11.1 스트리밍 데이터 전처리

#### 온라인 학습 전처리
- 점진적 통계량 업데이트
- 슬라이딩 윈도우 기법
- 개념 드리프트 탐지

#### 실시간 이상치 탐지
- 스트리밍 알고리즘 사용
- 적응적 임계값 조정
- 메모리 효율적 구현

### 11.2 대용량 데이터 전처리

#### 분산 처리 전략
- 청크 단위 처리
- 병렬 처리 활용
- 메모리 효율적 알고리즘

#### 클라우드 기반 전처리
- Apache Spark 활용
- 클라우드 서비스 이용
- 비용 최적화 전략

## 12. 다음 단계 학습 가이드

이 전처리 이론을 바탕으로 다음과 같은 순서로 학습을 진행하는 것을 권장합니다:

1. **실습을 통한 기법 습득**: 각 전처리 기법의 실제 구현
2. **유사도와 거리**: 전처리된 데이터의 유사성 측정
3. **지도학습 알고리즘**: 전처리된 데이터로 예측 모델 구축
4. **비지도학습 알고리즘**: 패턴 발견 및 구조 탐색

각 단계에서 전처리의 영향을 관찰하고, 도메인 지식을 활용한 의미 있는 전처리 방법을 개발하시기 바랍니다.