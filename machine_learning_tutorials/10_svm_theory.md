# SVM (Support Vector Machine) 완전 이론 가이드

## 1. 개요 및 핵심 개념

### SVM의 정의
Support Vector Machine(서포트 벡터 머신)은 **최적의 분류 경계(결정 경계)**를 찾는 지도학습 알고리즘입니다. SVM의 핵심 아이디어는 클래스 간의 **마진(margin)**을 최대화하는 초평면(hyperplane)을 찾는 것입니다.

### 핵심 개념들

#### 최대 마진 (Maximum Margin)
- **마진**: 결정 경계와 가장 가까운 데이터 포인트들 사이의 거리
- **목표**: 마진을 최대화하여 일반화 성능을 향상시킴
- **직관**: 마진이 클수록 새로운 데이터에 대한 예측이 더 안정적

#### 서포트 벡터 (Support Vectors)
- **정의**: 결정 경계에 가장 가까이 위치한 데이터 포인트들
- **특징**: 이 점들만이 결정 경계의 위치를 결정함
- **중요성**: 다른 데이터 포인트들을 제거해도 결정 경계는 변하지 않음

#### 초평면 (Hyperplane)
- **2차원**: 직선 (ax + by + c = 0)
- **3차원**: 평면 (ax + by + cz + d = 0)
- **n차원**: (w₁x₁ + w₂x₂ + ... + wₙxₙ + b = 0)

### 데이터 마이닝에서의 위치
SVM은 **지도학습의 분류** 알고리즘으로, 특히 **고차원 데이터**에서 뛰어난 성능을 보입니다. 통계학적 학습 이론에 기반하여 **구조적 위험 최소화(Structural Risk Minimization)** 원리를 따릅니다.

## 2. 동작 원리

### 선형 SVM의 수학적 원리

#### 목적 함수 (Objective Function)
SVM은 다음 최적화 문제를 해결합니다:

```
최소화: (1/2)||w||² + C∑ξᵢ
제약조건: yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0
```

여기서:
- **w**: 가중치 벡터 (결정 경계의 방향)
- **b**: 편향 (결정 경계의 위치)
- **C**: 정규화 매개변수
- **ξᵢ**: 슬랙 변수 (오분류 허용)

#### 결정 함수 (Decision Function)
```
f(x) = sign(w·x + b)
```

#### 마진 계산
- **기하학적 마진**: 2/||w||
- **마진 최대화** = **||w|| 최소화**

### 알고리즘 단계

1. **데이터 준비**: 훈련 데이터 (xᵢ, yᵢ) 준비
2. **최적화 문제 설정**: 라그랑주 승수법 적용
3. **쌍대 문제 해결**: QP(Quadratic Programming) 문제로 변환
4. **서포트 벡터 식별**: αᵢ > 0인 데이터 포인트들
5. **결정 경계 구성**: w와 b 계산
6. **예측**: 새로운 데이터에 대해 결정 함수 적용

### 시각적 이해

```
클래스 +1    |    클래스 -1
    ×        |        ○
    ×    마진|마진    ○
    ×   ←----|----→   ○
    ×        |        ○
         결정경계
```

서포트 벡터들이 마진의 경계에 위치하며, 이들만이 결정 경계를 결정합니다.

## 3. 커널 트릭과 비선형 SVM

### 선형 분리 불가능한 문제
실제 데이터는 대부분 선형으로 분리되지 않습니다. 이를 해결하기 위해 **커널 트릭(Kernel Trick)**을 사용합니다.

### 커널 트릭의 원리
1. **특징 공간 매핑**: 원본 데이터를 고차원 공간으로 변환
2. **내적 계산**: 고차원에서의 내적을 원본 공간에서 계산
3. **차원의 저주 회피**: 명시적 변환 없이 고차원 효과 획득

### 주요 커널 함수들

#### 1. 선형 커널 (Linear Kernel)
```
K(xᵢ, xⱼ) = xᵢ · xⱼ
```
- **용도**: 선형 분리 가능한 데이터
- **특징**: 가장 단순하고 빠름

#### 2. 다항식 커널 (Polynomial Kernel)
```
K(xᵢ, xⱼ) = (γxᵢ · xⱼ + r)^d
```
- **매개변수**: γ (스케일), r (계수), d (차수)
- **용도**: 중간 정도의 비선형성

#### 3. RBF 커널 (Radial Basis Function)
```
K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²)
```
- **매개변수**: γ (대역폭)
- **용도**: 복잡한 비선형 패턴
- **특징**: 가장 널리 사용됨

#### 4. 시그모이드 커널 (Sigmoid Kernel)
```
K(xᵢ, xⱼ) = tanh(γxᵢ · xⱼ + r)
```
- **용도**: 신경망과 유사한 효과
- **주의**: 모든 매개변수 조합에서 유효하지 않음

### 커널 선택 가이드라인
- **선형 데이터**: Linear 커널
- **중간 복잡도**: Polynomial 커널 (d=2,3)
- **복잡한 패턴**: RBF 커널
- **대용량 데이터**: Linear 커널 (속도 우선)

## 4. 파라미터 구성

### 주요 하이퍼파라미터

#### C (정규화 매개변수)
- **의미**: 오분류에 대한 페널티 강도
- **높은 C**: 
  - 오분류 최소화 우선
  - 복잡한 결정 경계
  - 과적합 위험 증가
- **낮은 C**:
  - 마진 최대화 우선
  - 단순한 결정 경계
  - 과소적합 위험 증가

#### γ (RBF 커널의 대역폭)
- **의미**: 개별 훈련 샘플의 영향 범위
- **높은 γ**:
  - 좁은 영향 범위
  - 복잡한 결정 경계
  - 과적합 위험
- **낮은 γ**:
  - 넓은 영향 범위
  - 부드러운 결정 경계
  - 과소적합 위험

### 파라미터 튜닝 전략

#### 1. 그리드 서치 (Grid Search)
```python
# 예시 매개변수 범위
C_range = [0.1, 1, 10, 100]
gamma_range = [0.001, 0.01, 0.1, 1]
```

#### 2. 교차 검증
- **5-fold 또는 10-fold CV** 사용
- **층화 샘플링**으로 클래스 비율 유지

#### 3. 학습 곡선 분석
- **검증 곡선**: 단일 매개변수 효과 분석
- **학습 곡선**: 훈련 데이터 크기 효과 분석

## 5. 성능 평가 방법

### 분류 성능 지표

#### 기본 지표
- **정확도 (Accuracy)**: 전체 예측 중 정확한 예측 비율
- **정밀도 (Precision)**: 양성 예측 중 실제 양성 비율
- **재현율 (Recall)**: 실제 양성 중 올바르게 예측한 비율
- **F1-Score**: 정밀도와 재현율의 조화평균

#### ROC 곡선과 AUC
- **ROC 곡선**: TPR vs FPR 곡선
- **AUC**: ROC 곡선 아래 면적
- **해석**: AUC가 높을수록 좋은 분류기

#### 혼동 행렬 (Confusion Matrix)
```
실제\예측   양성    음성
양성       TP     FN
음성       FP     TN
```

### SVM 특화 평가

#### 마진 분석
- **서포트 벡터 비율**: 전체 데이터 중 서포트 벡터 비율
- **마진 너비**: 결정 경계의 안정성 지표

#### 결정 함수 값 분석
- **결정 함수 값의 분포**: 예측 신뢰도 분석
- **마진 내부 점들**: 불확실한 예측 식별

## 6. 다른 알고리즘과의 비교

### 로지스틱 회귀와의 비교

#### 유사점
- 둘 다 **선형 분류기** (기본형)
- **이진 분류**에 특화
- **확률적 해석** 가능

#### 차이점
| 특성 | SVM | 로지스틱 회귀 |
|------|-----|---------------|
| 목적 | 마진 최대화 | 우도 최대화 |
| 출력 | 결정 함수 값 | 확률 |
| 이상치 민감도 | 낮음 | 높음 |
| 커널 트릭 | 자연스러움 | 복잡함 |
| 해석성 | 낮음 | 높음 |

### 의사결정나무와의 비교

#### 유사점
- **비선형 패턴** 학습 가능
- **다중 클래스** 분류 지원

#### 차이점
| 특성 | SVM | 의사결정나무 |
|------|-----|---------------|
| 해석성 | 낮음 | 높음 |
| 과적합 | 제어 가능 | 쉽게 발생 |
| 고차원 데이터 | 우수 | 어려움 |
| 훈련 속도 | 느림 | 빠름 |
| 결측치 처리 | 전처리 필요 | 자동 처리 |

### 신경망과의 비교

#### 유사점
- **비선형 패턴** 학습
- **복잡한 결정 경계** 형성

#### 차이점
| 특성 | SVM | 신경망 |
|------|-----|--------|
| 이론적 기반 | 강함 | 경험적 |
| 지역 최적해 | 전역 최적해 | 지역 최적해 |
| 매개변수 수 | 적음 | 많음 |
| 해석성 | 중간 | 낮음 |

## 7. 적용 사례 및 한계

### 적용 분야

#### 1. 텍스트 분류
- **스팸 메일 필터링**: 고차원 텍스트 특징
- **문서 분류**: TF-IDF 벡터화
- **감정 분석**: 텍스트의 긍정/부정 분류

#### 2. 이미지 인식
- **얼굴 인식**: 픽셀 기반 특징
- **필기 숫자 인식**: MNIST 데이터
- **의료 영상 분석**: X-ray, MRI 이미지

#### 3. 생물정보학
- **유전자 분류**: 고차원 유전자 발현 데이터
- **단백질 구조 예측**: 서열 기반 분류
- **질병 진단**: 바이오마커 기반 분류

#### 4. 금융 분야
- **신용 평가**: 고객 신용도 분류
- **사기 탐지**: 거래 패턴 분석
- **주가 예측**: 기술적 지표 기반 분류

### SVM의 장점

1. **고차원 데이터 처리**: 차원이 샘플 수보다 많아도 효과적
2. **메모리 효율성**: 서포트 벡터만 저장하면 됨
3. **다양한 커널**: 다양한 비선형 패턴 처리 가능
4. **과적합 제어**: C 매개변수로 복잡도 조절
5. **이론적 기반**: 통계학습 이론에 기반한 견고함

### SVM의 한계

1. **대용량 데이터**: O(n²) 또는 O(n³) 시간 복잡도
2. **확률 출력 없음**: 직접적인 확률 추정 어려움
3. **매개변수 민감성**: C, γ 등의 튜닝 필요
4. **해석성 부족**: 결정 과정 이해 어려움
5. **전처리 의존성**: 스케일링, 정규화 필수

### 사용 시 주의사항

1. **데이터 전처리**
   - 특징 스케일링 필수 (StandardScaler 사용)
   - 결측치 처리 필요
   - 이상치 탐지 및 처리

2. **커널 선택**
   - 데이터 특성에 맞는 커널 선택
   - 선형부터 시작하여 점진적 복잡화
   - 교차 검증으로 성능 비교

3. **매개변수 튜닝**
   - 그리드 서치 또는 랜덤 서치 사용
   - 교차 검증으로 일반화 성능 평가
   - 학습 곡선으로 과적합/과소적합 확인

## 8. 용어 사전

### 핵심 용어

- **서포트 벡터 (Support Vector)**: 결정 경계에 가장 가까운 훈련 샘플들
- **마진 (Margin)**: 결정 경계와 서포트 벡터 사이의 거리
- **초평면 (Hyperplane)**: n차원 공간에서 (n-1)차원의 결정 경계
- **커널 트릭 (Kernel Trick)**: 고차원 매핑 없이 내적 계산하는 기법
- **슬랙 변수 (Slack Variable)**: 오분류를 허용하는 변수 (ξ)

### 수학 기호

- **w**: 가중치 벡터 (weight vector)
- **b**: 편향 (bias)
- **C**: 정규화 매개변수 (regularization parameter)
- **γ**: RBF 커널의 대역폭 매개변수
- **α**: 라그랑주 승수 (Lagrange multiplier)
- **ξ**: 슬랙 변수 (slack variable)

### 개념 간 연관성

```
SVM 최적화 문제
    ↓
라그랑주 승수법
    ↓
쌍대 문제 (Dual Problem)
    ↓
커널 트릭 적용
    ↓
서포트 벡터 식별
    ↓
결정 함수 구성
```

이러한 단계적 과정을 통해 SVM은 최적의 분류 경계를 찾아 높은 일반화 성능을 달성합니다.